---------------------------------------------------------------------------
                          Pengelaan Deep Learning
---------------------------------------------------------------------------

Neural Network adalah fondasi dari deep learning. Bagian ini adalah sebuah model matematis yang terinspirasi dari cara kerja jaringan saraf manusia, di mana komputer dapat belajar mengenali pola kompleks dari data

NN adalah representasi sederhana dari neural network. Di dalamnya, ada neuron-neuron buatan yang terhubung dalam lapisan-lapisan. Setiap neuron menerima input, melakukan perhitungan matematis, dan menghasilkan output. Dengan banyaknya neuron dan lapisan yang saling terhubung, neural network dapat mempelajari pola-pola yang rumit dari data

Taksonomi AI:
1. Artificial Intelligence (AI)
Artificial intelligence adalah konsep yang mendasari seluruh bidang kecerdasan buatan. Pada tingkat paling dasar, AI mencakup penggunaan komputer atau mesin untuk melakukan tugas yang membutuhkan kecerdasan manusia, seperti pengambilan keputusan, pengenalan pola, dan pemecahan masalah.

2. Machine Learning (ML)
Machine learning adalah cabang dari AI, ketika komputer dapat belajar dari data tanpa perlu diprogram secara eksplisit. Teknik-teknik ML memungkinkan komputer untuk mengenali pola dalam data, membuat prediksi, dan mengambil keputusan berdasarkan informasi yang dipelajari dari pengalaman atau data latihan.

3. Neural Network (NN)
Neural network adalah model matematis yang terinspirasi dari struktur jaringan saraf manusia. Dalam konteks ML, NN digunakan untuk memproses informasi dan belajar dari data. Model ini terdiri atas neuron-neuron buatan yang saling terhubung dalam lapisan-lapisan dan mampu mempelajari representasi yang semakin abstrak dari data.

4. Deep Learning (DL)
Deep learning adalah sub-bidang dari ML yang menggunakan NN dengan banyak lapisan atau deep neural network (DNN) untuk memahami representasi data yang abstrak dan kompleks. DL telah menghasilkan kemajuan besar dalam bidang pengenalan gambar, pemrosesan bahasa alami, dan berbagai aplikasi AI lainnya.

5. Gen AI
Generative AI adalah cabang dari AI yang berfokus pada penciptaan konten baru dan original. Berbeda dengan AI tradisional yang biasanya beroperasi berdasarkan aturan yang telah ditetapkan dan data yang ada, generative AI memiliki kemampuan untuk menghasilkan teks, gambar, musik, video, dan bentuk konten lainnya yang belum pernah ada sebelumnya.



Konsep Dasar Neural Network:
Neural network (NN) atau jaringan saraf tiruan adalah model matematis yang terinspirasi dari struktur jaringan saraf manusia. NN dirancang untuk meniru cara otak manusia bekerja dengan menggunakan unit-unit pemrosesan sederhana yang disebut neuron. Jaringan saraf tiruan terdiri dari beberapa lapisan neuron yang saling terhubung. Informasi yang dibawa akan mengalir melalui jaringan ini untuk melakukan tugas-tugas tertentu, seperti pengenalan pola, klasifikasi, atau prediksi berdasarkan data yang diberikan.


Saraf Biologis vs Saraf Tiruan:
Sebelum kita belajar mengenai saraf tiruan, kita akan mengenal lebih dahulu saraf biologis (neuron). National Institute of Neurological Disorders and Stroke dalam tulisannya yang berjudul “Brain Basics: The Life and Death of a Neuron” menyatakan bahwa neuron atau saraf adalah pembawa pesan atau informasi. Mereka menggunakan impuls listrik dan sinyal kimiawi untuk mengirimkan informasi di antara area otak yang berbeda, serta antara otak dan seluruh sistem saraf.

Sebuah saraf terdiri dari tiga bagian utama, yaitu akson, dendrit, dan badan sel yang di dalamnya terdapat nukleus. Nukleus berisi materi genetik dan bertugas mengontrol seluruh aktivitas sel. Akson adalah cabang yang terlihat seperti ekor panjang dan bertugas mengirimkan pesan dari sel. Panjang akson berkisar antara beberapa kali lebih panjang dari badan sel, sampai 10 ribu kali. Berikutnya, dendrit adalah cabang-cabang pendek yang terlihat seperti cabang pohon dan bertugas menerima pesan untuk sel. 

Setiap ujung akson dari sebuah neuron terhubung dengan dendrit dari neuron lainnya. Neuron berkomunikasi satu sama lain dengan mengirimkan senyawa kimia yang disebut neurotransmitter, melintasi ruang kecil (synapse) antara akson dan dendrit neuron yang berdekatan. 

Tahukah Anda? Konsep kerja dan struktur NN ternyata terinspirasi dari saraf biologis kita, lo!  Pada NN, ada unit-unit pemrosesan disebut neuron buatan dan saling terhubung dalam struktur yang mirip dengan jaringan saraf biologis. Setiap neuron buatan menerima sejumlah input, menghitung hasil berdasarkan bobot yang ditetapkan, dan mengirimkan output ke neuron-neuron lainnya.  

Pengaturan bobot ini adalah kunci dalam NN. Selama proses pelatihan, NN belajar untuk mengubah bobot-bobot ini berdasarkan data latihan yang diberikan. Jadi, NN mampu mengenali pola-pola kompleks dan membuat prediksi atau keputusan berdasarkan informasi yang diterima. 



Struktur Artificial Neural Network:
Neural network yang paling sederhana atau minimal terdiri dari satu unit perceptron tunggal, juga dikenal sebagai single perceptron. Perceptron ini memproses input dengan mengalikan nilai input (x) dengan bobot (w), kemudian menjumlahkannya dengan bias (b). Hasil dari operasi ini disebut net input (z) dan kemudian akan diproses melalui fungsi aktivasi (f) untuk menghasilkan output (y) dari perceptron tersebut. 

Rumus Single Perceptron:

     m
Z = EEE   Wi Xi + b
    i = 1

- x  : x1, x2, …, xn; xi adalah nilai input ke-i.
- w : w1, w2, …, wn; wi adalah bobot terkait dengan input xi.
- b : nilai bias (konstanta tambahan) yang dimiliki oleh perceptron.
- z : fungsi linear.
- f : fungsi aktivasi untuk menghasilkan output y.
- y : output.

Jadi, secara matematis, single perceptron dapat dijelaskan sebagai sebuah fungsi matematis yang mengambil input, mengalikannya dengan bobot, menambahkan bias, dan kemudian menerapkan fungsi aktivasi pada hasilnya untuk menghasilkan output. Prosedur ini adalah dasar dari komputasi dalam NN yang lebih kompleks.


Cara Kerja Artificial Neural Network:
Cara kerja artificial neural network (ANN) mengambil inspirasi dari struktur jaringan saraf manusia. Pada sebuah ANN, ada jutaan atau bahkan miliaran "neuron" buatan yang terorganisir dalam lapisan-lapisan. Setiap neuron menerima input dari neuron-neuron dalam lapisan sebelumnya, melakukan operasi matematika pada input ini, dan menghasilkan output yang akan diteruskan ke neuron-neuron dalam lapisan berikutnya.

Mirip dengan kerja jaringan saraf manusia, setiap koneksi antar neuron dalam ANN memiliki bobot atau nilai yang memengaruhi pentingnya informasi dari neuron sebelumnya. Selama proses pembelajaran, bobot-bobot ini diatur ulang agar ANN dapat belajar memahami pola-pola yang ada dalam data.

Saat ANN diberikan masukan (misalnya gambar, teks, atau data numerik), sinyal-sinyal ini bergerak melalui jaringan neuron, diolah, dan menghasilkan keluaran. Proses ini memungkinkan ANN untuk mengenali pola-pola kompleks pada data, seperti pengenalan wajah dalam gambar, klasifikasi teks, atau prediksi harga saham berdasarkan data historis.


Perceptron:
Perceptron adalah komponen dasar pembangun jaringan saraf tiruan. Frank Rosenblatt dari Cornell Aeronautical Library adalah ilmuwan yang pertama kali menemukan perceptron pada tahun 1957. Perceptron terinspirasi dari neuron pada jaringan saraf dalam otak manusia. Dalam jaringan saraf tiruan, perceptron dan neuron merujuk pada hal yang sama. 

Lantas, bagaimana perceptron bekerja pada jaringan saraf tiruan? 
Sebuah perceptron menerima masukan berupa bilangan numerik. Perceptron kemudian memproses masukan tersebut untuk menghasilkan sebuah keluaran. Agar lebih memahami cara kerja perceptron, kita akan menggunakan diagram di bawah.

Sebuah perceptron terdiri dari 5 komponen, yaitu:
- input (xi),
- bobot atau weights (wi) dan bias (w0),
- penjumlahan atau sum (sigma),
- fungsi aktivasi atau non linearity function, dan
- output (y).


Sebuah perceptron tunggal saat berdiri sendiri mungkin tidak memberikan hasil yang signifikan dalam konteks pemrosesan data kompleks. Namun, ketika perceptron ini dihubungkan dengan ratusan atau ribuan perceptron lain dalam sebuah jaringan yang lebih besar, kemampuannya untuk memproses informasi dan memberikan hasil akan jauh lebih kuat. 

Banyak perceptron yang saling terhubung dalam neural network. Setiap perceptron melakukan komputasi terhadap inputnya sendiri dengan bobot dan bias yang spesifik. Kemudian, informasi hasil komputasi dari satu perceptron akan mengalir ke perceptron-perceptron lainnya melalui koneksi yang terbentuk di antara mereka. Proses ini memungkinkan jaringan untuk mempelajari pola kompleks dalam data dan membuat prediksi atau keputusan yang akurat.

Dengan koneksi yang kuat antar-perceptron serta kemampuan untuk menyesuaikan bobot dan bias selama proses pelatihan, neural network dapat mengungguli banyak metode machine learning lainnya dalam hal keakuratan, begitu pun kemampuan adaptasi terhadap masalah yang rumit. 

Seiring dengan meningkatnya ukuran dan kompleksitas jaringan, performa neural network dapat semakin ditingkatkan. Ini menjadikannya alat yang sangat efektif dalam berbagai aplikasi, termasuk pengenalan pola, klasifikasi, prediksi, dan banyak lagi.

Pertama, input menerima masukan berupa angka-angka. Setiap input memiliki bobot masing-masing. Bobot adalah parameter yang akan dipelajari oleh sebuah perceptron dan menunjukkan kekuatan node tertentu. 

Selanjutnya adalah tahap penjumlahan input. Pada tahap ini, setiap input akan dikalikan dengan bobotnya masing-masing, lalu hasilnya ditambahkan dengan bias yang berupa sebuah konstanta atau angka. Nilai bias memungkinkan Anda untuk mengubah kurva fungsi aktivasi ke atas atau bawah sehingga bisa lebih fleksibel dalam meminimalkan error. Hasil penjumlahan pada tahap ini biasanya disebut weighted sum.

Langkah berikutnya, aplikasikan weighted sum pada fungsi aktivasi atau disebut juga non-linearity function. Fungsi aktivasi digunakan untuk memetakan nilai hasil menjadi nilai yang diperlukan, misalnya antara (0, 1) atau (-1, 1). Fungsi ini memungkinkan perceptron dapat menyesuaikan pola untuk data yang non-linier. Penjelasan lebih lanjut tentang fungsi aktivasi akan diulas pada paragraf di bawah.


      (     m        )
y =g ( W0 + EEE Wi Xi )
      (   i = 1      )

- y = output
- g = non-linear activation function

Fungsi matematis dari perceptron dapat kita lihat di bawah. Rumus di bawah adalah notasi matematis yang menjelaskan proses sebelumnya. Keluaran (ŷ) dari perceptron adalah bias (w0) ditambah dengan jumlah setiap input (xi) yang dikali dengan bobot masing-masing (wi) sehingga menghasilkan weighted sum, kemudian dimasukkan dalam fungsi aktivasi (g). 



Multilayer Perceptron (MLP):
Multilayer perceptron (MLP) atau feedforward neural network adalah jenis neural network yang terdiri dari banyak perceptron (neuron) yang saling terhubung dalam beberapa lapisan. MLP memiliki struktur yang terdiri dari tiga jenis layer utama.

1. Input Layer
Layer pertama dari MLP adalah input layer. Ini berfungsi untuk menerima data atau input dari luar. Setiap neuron dalam input layer mewakili satu fitur atau variabel dari data yang masuk ke jaringan. Misalnya, dalam aplikasi pengenalan gambar, setiap neuron dapat mewakili nilai intensitas piksel dari gambar.

2. Hidden Layer
Setelah menerima input, data akan diteruskan ke hidden layer (lapisan tersembunyi) dalam MLP. Hidden layer terdiri dari satu atau lebih lapisan antara input layer dan output layer. Neuron-neuron dalam hidden layer memproses input yang diterima dari layer sebelumnya dengan melakukan operasi matematika menggunakan bobot (weights) dan fungsi aktivasi tertentu. Hidden layer berfungsi sebagai penyaring atau extractor fitur yang membantu jaringan dalam mempelajari pola-pola kompleks pada data.

3. Output Layer
Output layer adalah layer terakhir dalam MLP yang menghasilkan output berdasarkan hasil pemrosesan oleh hidden layer. Jumlah neuron dalam output layer bergantung pada tipe tugas yang ingin diselesaikan oleh jaringan. Misalnya, untuk tugas klasifikasi biner, output layer dapat memiliki satu neuron yang menghasilkan nilai antara 0 dan 1 (mengindikasikan probabilitas kelas), sedangkan untuk klasifikasi multiclass, setiap neuron mungkin mewakili probabilitas dalam kelas tertentu.



Terms pada Neural Network:
1. Activation Function (Fungsi Aktivasi)
Fungsi aktivasi adalah sebuah fungsi matematika yang menentukan bahwa neuron dalam jaringan saraf tiruan akan menghasilkan output atau tidak berdasarkan inputnya. Analoginya, fungsi ini meniru cara neuron biologis "aktif" atau "tidak aktif" berdasarkan sinyal masukan yang diterima.

Fungsi aktivasi pada perceptron bertugas untuk membuat jaringan saraf mampu menyesuaikan pola dengan data non linier. Sebagaimana yang sudah pernah dibahas sebelumnya, mayoritas data di dunia nyata adalah data non linier seperti di bawah.

Fungsi aktivasilah yang memungkinkan jaringan saraf dapat mengenali pola non-linier seperti di bawah. Tanpa fungsi aktivasi, jaringan saraf hanya bisa mengenali pola linier layaknya garis pada regresi linier.


Secara umum, ada dua jenis activation function, linear, dan non-linear activation function. Ada beberapa jenis fungsi aktivasi yang umum digunakan dalam jaringan saraf sebagai berikut.

a. Linear [f(x) = x]
Linear activation function (fungsi aktivasi linear) dalam konteks jaringan saraf tiruan adalah sebuah fungsi dengan keluaran yang proporsional secara linear terhadap input. Dengan kata lain, fungsi ini hanya melakukan transformasi linear sederhana dari input ke output tanpa memperkenalkan non-linearitas.

Pada rumus linear, x adalah input ke neuron atau lapisan jaringan. Dalam hal ini, keluaran dari neuron atau lapisan tersebut sama dengan inputnya sendiri. Oleh karena itu, tidak ada transformasi non-linear yang terjadi.

Sebagai tambahan informasi, fungsi aktivasi linear jarang digunakan dalam lapisan tersembunyi (hidden layers) karena tidak mampu memodelkan hubungan yang kompleks antara variabel input dan output. Namun, fungsi ini dapat digunakan pada lapisan output untuk masalah regresi.


b. ReLU (Rectified Linear Activation) [f(x) = max(0, x)]
ReLU (rectified linear activation) adalah jenis fungsi aktivasi yang umum digunakan dalam jaringan saraf tiruan. Fungsi ReLU didefinisikan sebagai f(x) = max(0, x). Ini berarti output dari ReLU adalah nilai inputnya jika nilai input tersebut lebih besar dari atau sama dengan nol, dan output-nya adalah nol jika nilai inputnya kurang dari nol.

Keuntungan utama dari ReLU adalah sederhana pada komputasi dan memperkenalkan non-linearitas ke dalam jaringan. ReLU umumnya digunakan sebagai fungsi aktivasi pada lapisan tersembunyi karena kemampuannya untuk mempercepat konvergensi pembelajaran dan mengurangi risiko overfitting.

ReLU adalah pilihan populer dan efektif untuk fungsi aktivasi dalam jaringan saraf modern karena sederhana, efisien, serta mampu membantu jaringan mempelajari representasi yang kompleks dari data dengan baik.


c. Leaky ReLU [f(x) = max(0.1 x, x)
Kekurangan pada ReLU adalah beberapa gradien dapat menjadi sangat rapuh selama proses pelatihan, yang dapat menyebabkan fenomena "neuron mati". Ini berarti bahwa selama pelatihan, beberapa neuron dapat terkunci dalam keadaan mereka tidak akan pernah diaktifkan lagi pada sebagian besar atau semua data yang diberikan. Hal itu terjadi karena gradien (turunan) dari fungsi ReLU menjadi nol pada bagian negatifnya.

Untuk mengatasi masalah ini, modifikasi lain dari fungsi ReLU diperkenalkan, dikenal sebagai leaky ReLU. Ia memperkenalkan kemiringan kecil (biasanya nilai tetap yang sangat kecil, seperti 0.01) pada bagian negatif dari fungsi ReLU.

Hal ini membantu menjaga agar neuron tetap aktif selama pelatihan, bahkan jika gradien pada bagian negatifnya mendekati nol. Dengan demikian, leaky ReLU dapat mencegah "kematian" neuron dan membantu meningkatkan stabilitas serta kecepatan konvergensi dalam pelatihan jaringan saraf.


d. Sigmoid
Sigmoid akan menerima angka tunggal dan mengubah x menjadi sebuah nilai yang memiliki rentang mulai dari 0 sampai 1. Fungsi ini biasanya dapat diinterpretasikan sebagai probabilitas dalam konteks klasifikasi biner.

Keuntungan utama dari sigmoid adalah kemampuannya dalam menghasilkan keluaran dengan batas antara 0 dan 1, yang berguna untuk tugas klasifikasi ketika kita ingin memprediksi probabilitas keanggotaan pada kelas tertentu.

Namun, ada beberapa perhatian terkait penggunaan sigmoid.
a. Gradien yang Menghilang
Ketika nilai input sangat besar (positif atau negatif), gradien sigmoid cenderung mendekati nol, yang dapat menyebabkan masalah lambatnya konvergensi selama pelatihan jaringan. ?

b. Output yang Tidak Seimbang
Sigmoid memiliki kecenderungan untuk menghasilkan output yang condong ke nilai 0 atau 1 dengan cepat; ini dapat menghambat pembelajaran dalam beberapa kasus.


e. Tanh [f(x) = tanh(x)]
Tanh (hyperbolic tangent) adalah jenis fungsi aktivasi lain yang umum digunakan dalam jaringan saraf tiruan. Tanh akan mengubah nilai input x-nya menjadi sebuah nilai yang memiliki rentang mulai dari -1 hingga 1.

Namun, serupa halnya dengan sigmoid, tanh juga memiliki beberapa masalah, seperti rentang output yang terbatas (-1 sampai 1). Ini juga bisa menyebabkan gradien menghilang pada jaringan. Meskipun demikian, tanh masih menjadi pilihan populer untuk fungsi aktivasi dalam beberapa kasus, terutama ketika rentang output yang simetris di sekitar nol diinginkan atau saat sigmoid tidak memberikan hasil yang memuaskan.

Beberapa karakteristik dari fungsi tanh adalah berikut.
a. Rentang Output
Fungsi tanh menghasilkan output antara -1 dan 1. Ini membuatnya lebih simetris di sekitar titik nol dibandingkan sigmoid yang memiliki rentang antara 0 dan 1.

b. Non-linearitas
Layaknya sigmoid, tanh juga memperkenalkan non-linearitas ke jaringan. Ini memungkinkan jaringan untuk mempelajari hubungan yang lebih kompleks antara input dan output.

c. Penggunaan di Lapisan Tersembunyi
Tanh sering digunakan sebagai alternatif sigmoid pada lapisan tersembunyi karena rentangnya simetris dan kemampuannya untuk menangani gradien yang menghilang lebih baik daripada sigmoid.


f. Softmax
Softmax adalah jenis fungsi aktivasi yang umum digunakan pada lapisan output dari jaringan saraf, terutama untuk tugas klasifikasi multiclass. Fungsi softmax mengubah nilai input menjadi distribusi probabilitas yang memetakan output pada rentang (0, 1) sehingga total probabilitas output menjadi 1.

Berikut adalah beberapa karakteristik dari fungsi softmax.
a. Interpretasi Probabilitas
Output dari softmax dapat diinterpretasikan sebagai probabilitas bahwa input termasuk dalam setiap kelas yang mungkin karena nilai-nilainya berada pada rentang (0, 1) dan total probabilitasnya adalah 1.

b. Penanganan Masalah Multikelas
Softmax sangat berguna dalam tugas klasifikasi, yakni ketika ada lebih dari dua kelas yang mungkin. Ini memungkinkan model dalam menghasilkan prediksi probabilitas untuk setiap kelas.

c. Loss Function / Cost Function
Softmax sering digunakan bersama dengan cross-entropy loss function sebagai fungsi kerugian (loss function) dalam jaringan saraf untuk tugas klasifikasi multiclass. Ini karena softmax menghasilkan distribusi probabilitas dan cross-entropy dapat digunakan untuk mengukur kesalahan antara distribusi prediksi dan distribusi target.

Penggunaan softmax biasa ditemukan dalam lapisan output jaringan saraf, sedangkan pada lapisan tersembunyi, fungsi aktivasi lainnya, seperti ReLU atau tanh, lebih umum digunakan. Kombinasi softmax dengan fungsi aktivasi yang sesuai pada lapisan tersembunyi membentuk arsitektur jaringan saraf yang efektif untuk tugas klasifikasi multi kelas.


2. Loss Function

Loss function (fungsi kerugian) dalam konteks jaringan saraf adalah algoritma matematis yang digunakan untuk mengukur seberapa baik atau buruk kinerja model neural network pada data pelatihan. Fungsi kerugian memberikan representasi numerik tentang seberapa besar kesalahan atau perbedaan antara prediksi model dan nilai yang sebenarnya dalam data pelatihan.

Tujuan utama dari loss function untuk membimbing proses pelatihan jaringan saraf agar dapat meminimalkan kesalahan prediksi. Saat model melakukan prediksi pada data pelatihan, loss function menghitung seberapa jauh hasil prediksi dari nilai yang sebenarnya. Semakin kecil nilai loss function, semakin baik kinerja model.
 
Contoh loss function yang umum digunakan adalah mean square error (MSE). MSE menghitung rata-rata dari kuadrat selisih antara prediksi model dan nilai yang sebenarnya pada setiap titik data. Nilai MSE yang lebih kecil menunjukkan bahwa model memiliki kesalahan prediksi lebih rendah. 

Selain MSE, ada juga berbagai jenis loss function lainnya, seperti cross entropy loss yang umum digunakan untuk masalah klasifikasi. Setiap jenis loss function memiliki karakteristik dan aplikasi berbeda tergantung pada tipe masalah yang dihadapi dalam pembelajaran mesin. 

Pada dasarnya, loss function berperan penting dalam membantu model jaringan saraf untuk belajar dari data pelatihan. Dengan mengukur kesalahan prediksi secara objektif, loss function memungkinkan kita untuk menyesuaikan parameter model. Jadi, model dapat menghasilkan prediksi lebih akurat dan sesuai dengan data yang diberikan.

3. Optimizer

Optimizer (pengoptimal) adalah komponen kunci dalam pelatihan jaringan saraf tiruan yang bertanggung jawab untuk mengoptimalkan atau menyesuaikan bobot dan bias pada jaringan agar dapat mengurangi kesalahan prediksi. Tujuan utama dari pengoptimal adalah untuk menemukan nilai bobot dan bias dengan hasil prediksi paling akurat dan generalisasi yang baik terhadap data baru.

Beberapa fungsi utama dari optimizer dalam konteks jaringan saraf adalah berikut.
a. Menghitung Gradien
Optimizer menghitung gradien dari loss function terhadap parameter (weights dan biases) dalam jaringan. Gradien ini memberikan informasi tentang arah dan seberapa besar perubahan yang harus dilakukan pada parameter untuk mengurangi nilai fungsi kerugian.

b. Memperbarui Parameter
Berdasarkan gradien yang dihitung, optimizer akan memperbarui nilai parameter (weights dan biases) jaringan. Update ini dilakukan dengan cara menggerakkan nilai parameter ke arah yang mengurangi nilai fungsi kerugian. Cara update ini biasanya menggunakan metode gradient descent atau varian-varian lainnya, yakni momentum, RMSprop, Adam, dan lainnya.

c. Menangani Masalah Optimalisasi
Optimizer berusaha menangani masalah, seperti lambatnya konvergensi, kemungkinan terjebak dalam optimum lokal, ataupun masalah gradien yang meledak atau menghilang. Beberapa optimizer menggunakan berbagai teknik, seperti momentum, learning rate scheduling, atau adaptive learning rate untuk mengatasi masalah-masalah ini.

Beberapa contoh optimizer yang umum digunakan dalam pelatihan jaringan saraf seperti berikut.

a. Stochastic Gradient Descent (SGD): Optimizer klasik yang menggunakan gradien dari subset data (batch) untuk memperbarui parameter.

b. RMSprop: Optimizer yang menyesuaikan learning rate untuk setiap parameter berdasarkan perbedaan antara gradien saat ini dan sejarah gradien.

c. Adam: Optimizer adaptif yang menggabungkan konsep dari momentum dan RMSprop untuk mengatur learning rate secara adaptif berdasarkan estimasi momen gradien.

Pemilihan optimizer sangat penting dalam pelatihan jaringan saraf karena dapat memengaruhi kecepatan konvergensi, kualitas model yang dihasilkan, dan kemampuan jaringan untuk menghindari masalah-masalah optimisasi. Berbagai faktor, seperti jenis tugas, ukuran dataset, dan arsitektur jaringan, dapat memengaruhi pemilihan optimizer yang paling sesuai untuk suatu proyek.


Konsep Metode Forward Propagation dan Backpropagation:
1. Forward Propagation
Forward propagation adalah langkah pertama dalam proses penggunaan neural network untuk membuat prediksi berdasarkan masukan data.

a. Input Data
Langkah pertama dalam penggunaan neural network adalah memberikan data sebagai input pada jaringan. Data ini bisa berupa berbagai jenis informasi, seperti gambar (dalam bentuk piksel), teks (dalam bentuk token atau urutan kata), atau nilai numerik (seperti atribut atau fitur yang menggambarkan suatu objek atau fenomena).

Misalnya, jika kita ingin menggunakan neural network untuk klasifikasi gambar, data input dapat berupa gambar-gambar digital yang direpresentasikan dalam bentuk matriks piksel. Setiap piksel memiliki nilai intensitas yang mewakili warna pada posisi tertentu dalam gambar. Gambar-gambar ini kemudian dijadikan input pada neural network untuk memungkinkan model mempelajari pola dan fitur yang terkandung dalam gambar.

Contoh lainnya, jika kita ingin melakukan analisis sentimen terhadap teks, data input bisa berupa urutan kata atau kalimat. Setiap kata dalam teks direpresentasikan dengan token atau angka yang menunjukkan kata tersebut. Data tersebut kemudian diproses dan dimasukkan ke neural network untuk memprediksi sentimen atau makna dari teks tersebut.

Selain itu, data numerik, seperti atribut dari suatu objek, juga dapat digunakan sebagai input. Misalnya, jika kita ingin memprediksi harga rumah berdasarkan fitur-fitur, seperti luas tanah, jumlah kamar, lokasi, dan sebagainya, atribut-atribut ini dapat dimasukkan sebagai input pada neural network.

Dengan pemberian data sebagai input, neural network dapat memproses informasi ini melalui serangkaian langkah komputasi yang kompleks. Itu bertujuan untuk mempelajari pola, menerapkan transformasi, dan menghasilkan output yang berguna atau prediksi sesuai dengan keinginan berdasarkan tugas atau tujuan.


b. Neuron dan Bobot
Setelah data dimasukkan dalam neural network, data tersebut melewati serangkaian neuron pada berbagai lapisan jaringan tersebut. Setiap neuron dalam jaringan menerima input data dari neuron-neuron pada lapisan sebelumnya atau data asli, seperti gambar atau teks.

Setiap neuron memiliki parameter "bobot" (weights) yang digunakan untuk mengalikan input tersebut. Bobot ini mewakili kekuatan atau pentingnya setiap input terhadap aktivasi neuron. Misalnya, jika kita memiliki neuron pada lapisan pertama yang menerima gambar sebagai input, setiap piksel dalam gambar akan dikalikan dengan bobot yang sesuai. Bobot ini adalah angka-angka yang dipelajari oleh model selama proses pelatihan untuk mengoptimalkan kinerja jaringan.

Selain bobot, setiap neuron juga memiliki bias sebagai nilai tambahan yang ditambahkan ke hasil perkalian input dengan bobot. Bias membantu neuron untuk belajar membedakan pola yang kompleks dalam data. Misalnya, jika semua input bernilai nol, bias dapat memungkinkan neuron untuk tetap aktif dan belajar dari situasi-situasi yang tidak biasa.

Proses perkalian input dengan bobot dan penambahan bias pada setiap neuron menghasilkan nilai yang diteruskan ke fungsi aktivasi. **Fungsi aktivasi bertujuan untuk** memutuskan bahwa neuron tersebut harus "aktif" atau "tidak aktif" berdasarkan hasil perhitungan tersebut. Fungsi aktivasi yang umum digunakan adalah ReLU untuk lapisan tersembunyi dan softmax dalam lapisan output pada masalah klasifikasi.

Selanjutnya, hasil aktivasi dari setiap neuron akan menjadi input bagi neuron-neuron pada lapisan berikutnya dalam jaringan. Proses ini berulang terus menerus melalui seluruh jaringan dan setiap lapisan akan menghasilkan representasi yang semakin abstrak dari data asli. Hasil akhir dari neural network adalah prediksi atau output berdasarkan input data yang dipelajari oleh model selama proses pelatihan.


c. Perhitungan di Neuron
Pada sebuah neural network, setiap neuron memiliki peran penting dalam menghitung output berdasarkan input yang diterimanya. Proses perhitungan dalam neuron dimulai dengan menerima input, yang bisa berasal dari neuron-neuron pada lapisan sebelumnya atau data asli, seperti gambar, teks, atau nilai numerik. Tiap input ini kemudian dikalikan dengan bobot yang telah dipelajari oleh jaringan selama proses pelatihan. **Bobot ini menentukan seberapa penting setiap input terhadap aktivasi neuron.**

Selain itu, neuron memiliki bias yang ditambahkan ke hasil perkalian input dan bobot sebelum memasuki fungsi aktivasi. Bias ini membantu neuron mempelajari pola-pola yang kompleks dalam data. Setelah itu, hasil penjumlahan input dan bobot dengan bias dimasukkan ke fungsi aktivasi. Fungsi aktivasi, seperti ReLU atau sigmoid, bertujuan untuk menentukan output neuron berdasarkan ambang batas tertentu.

Output ini kemudian menjadi input bagi neuron-neuron pada lapisan berikutnya dalam jaringan. Proses ini terjadi secara berulang melalui seluruh jaringan, yakni setiap neuron belajar mengoptimalkan bobot dan biasnya untuk menghasilkan representasi yang semakin abstrak dari data.

Hasil akhir dari neural network adalah prediksi atau output sesuai dengan keinginan, yang digunakan untuk menyelesaikan tugas spesifik, seperti klasifikasi, regresi, atau pengenalan pola.


2. Backpropagation
Backpropagation adalah langkah kedua dalam proses penggunaan neural network untuk menghitung nilai error dari output hasil prediksi.

a. Perhitungan Error
Setelah mendapatkan output dari neural network, kita bandingkan output tersebut dengan nilai target yang sebenarnya (ground truth) dari data latih. Perhitungan error dilakukan dengan menggunakan loss function, seperti mean squared error (MSE) atau cross-entropy loss. Tujuan kita adalah untuk mengukur seberapa jauh prediksi neural network dari target sebenarnya.

b. Backward Pass
Setelah menghitung error, langkah berikutnya adalah melakukan backward pass. Ide dasar dari **backpropagation adalah menghitung seberapa besar setiap bobot harus diubah agar mengurangi kesalahan total neural network.**

Pertama, kita hitung gradien (turunan) dari loss function terhadap setiap bobot dalam neural network menggunakan aturan rantai (chain rule) dari kalkulus. Tujuannya untuk memberi tahu kita seberapa besar setiap bobot berkontribusi terhadap kesalahan total.

Gradien ini kemudian digunakan untuk mengubah setiap bobot dalam arah yang mengurangi kesalahan. Dalam hal ini, kita menggunakan algoritma optimasi, seperti gradient descent, ketika bobot diperbarui dengan menggerakkannya melawan arah gradien dengan suatu laju pembelajaran (learning rate).

c. Perubahan Bobot
Bobot-bobot dalam neural network diperbarui menggunakan hasil gradien yang telah dihitung sebelumnya. Proses ini bertujuan untuk menggerakkan bobot ke arah yang mengurangi nilai fungsi kerugian sehingga prediksi neural network menjadi lebih akurat.

d. Iterasi
Langkah-langkah forward propagation, perhitungan error, backward propagation, dan optimisasi bobot diulang untuk setiap batch data latihan (mini-batch) hingga bobot-bobot dalam neural network konvergen bernilai optimal. Proses ini dapat melibatkan banyak iterasi (epochs) tergantung pada kompleksitas model dan jumlah data latihan.

Pengantar Deep Learning:


Sejarah Perkembangan Deep Learning:

Deep learning memiliki akar yang dapat ditelusuri kembali ke konsep dasar jaringan saraf buatan, mulai pada tahun 1943 ketika Warren McCulloch dan Walter Pitts memperkenalkan model awal jaringan saraf. Konsep ini menginspirasi pengembangan selanjutnya dalam bidang kecerdasan buatan.

Pada tahun 1950-an dan 1960-an, ilmuwan seperti Frank Rosenblatt mengembangkan perceptron, bentuk awal dari jaringan saraf buatan yang mampu mengenali pola sederhana. Meskipun demikian, kemajuan signifikan dalam deep learning baru terjadi pada tahun 1980-an dan 1990-an, ketika muncul metode pembelajaran yang lebih efisien dan algoritma pelatihan yang lebih baik.

Pada tahun 2006, Geoffrey Hinton, Yoshua Bengio, dan Yann LeCun berhasil mengembangkan algoritma backpropagation; itu memungkinkan pelatihan jaringan saraf yang lebih dalam secara efisien. Backpropagation memungkinkan penyesuaian bobot jaringan berdasarkan kesalahan prediksi, memungkinkan pembelajaran yang lebih baik dari data.

Perkembangan lebih lanjut terjadi dengan penggunaan unit pemrosesan grafis (GPU) yang kuat untuk mempercepat pelatihan jaringan saraf kompleks. Selain itu, perkembangan perangkat keras yang lebih kuat dan tersedia secara luas mendukung kemajuan deep learning.

Titik balik terjadi pada tahun 2012 ketika jaringan saraf konvensional yang dalam, dikenal sebagai AlexNet, memenangkan kompetisi ImageNet dengan hasil mengesankan. Kemenangan ini menunjukkan kemampuan deep learning dalam pengenalan gambar.

Sejak itu, deep learning telah mencapai kemajuan signifikan dalam berbagai bidang. Dalam pengenalan gambar, deep learning digunakan untuk mendeteksi objek, mengklasifikasikan citra, dan menghasilkan deskripsi otomatis.

Pada pemrosesan bahasa alami, deep learning mempercepat kemajuan dalam pemahaman teks, terjemahan mesin, dan pengenalan suara.

Perkembangan deep learning didorong oleh hadirnya kerangka kerja (framework), seperti TensorFlow dan PyTorch, yang mempermudah pengembangan dan implementasi jaringan saraf yang dalam.


Definisi Deep Learning:

Deep learning adalah bagian dari bidang kecerdasan buatan dengan menggunakan algoritma yang terinspirasi dari cara otak manusia bekerja. Metode ini diimplementasikan melalui jaringan saraf tiruan yang disebut artificial neural networks atau disingkat sebagai ANN. 

ANN adalah model matematis yang terdiri dari tiga atau lebih lapisan neuron yang saling terhubung. Dengan struktur ini, ANN dapat memproses dan mempelajari pola kompleks dari data sehingga mampu mengatasi berbagai masalah yang sulit diselesaikan dengan algoritma machine learning konvensional.

Deep learning menggunakan konsep ANN dengan banyak lapisan (deep neural networks) untuk melakukan beberapa tugas, seperti pengenalan gambar, pengenalan suara, atau bahkan penerjemahan bahasa. Keunggulan utama deep learning terletak pada kemampuannya untuk belajar secara mandiri dari data yang besar dan kompleks. Proses pembelajaran ini melibatkan penyesuaian bobot dan parameter jaringan secara iteratif melalui proses yang disebut pelatihan (training), ketika model diuji dan diperbaiki berdasarkan umpan balik dari data.

Pada konteks machine learning, deep learning telah membuka kemungkinan baru dalam pemrosesan data, terutama perihal analisis gambar dan data berurutan. Metode-metode, seperti convolutional neural networks (CNNs) dan recurrent neural networks (RNNs), adalah contoh dari deep learning yang sangat sukses dalam domain ini. Meskipun demikian, deep learning juga menghadapi tantangan, termasuk sulitnya interpretasi dari keputusan model, kebutuhan data yang besar untuk pelatihan, serta kompleksitas dalam pengaturan parameter model.

Jadi, deep learning bisa dianggap seperti "membuat komputer belajar layaknya manusia", ketika model-modelnya bisa belajar sendiri dari data besar untuk membuat prediksi yang lebih cerdas dan akurat. Hal ini telah membawa terobosan besar dalam berbagai bidang, yaitu pengenalan wajah, mobil otonom, penelitian medis, dan banyak lagi, lo




Arsitektur Deep Learning:

Arsitektur deep learning adalah struktur atau tata letak jaringan saraf buatan yang kompleks. Arsitektur ini terdiri dari berbagai lapisan yang bertugas untuk memproses dan mentransformasikan data masukan menjadi hasil keluaran sesuai dengan keinginan.

Setiap lapisan dalam arsitektur memiliki fungsi khusus serta bertanggung jawab untuk mengekstrak fitur-fitur yang semakin abstrak dan kompleks seiring dengan meningkatnya kedalaman jaringan. Dengan kedalaman lebih besar, arsitektur deep learning dapat mempelajari pola yang lebih kompleks dari data dan menunjukkan hasil lebih baik.


Input Layer:
Input layer adalah bagian pertama dari jaringan neural yang bertanggung jawab untuk menerima data masukan, yaitu gambar, teks, atau data numerik lainnya. Fungsi utama dari input layer adalah meneruskan data masukan ke lapisan-lapisan selanjutnya dalam jaringan, yang disebut sebagai hidden layers.

Karakteristik:

- Input layer tidak melakukan komputasi yang kompleks, seperti aktivasi atau transformasi data. Tugasnya hanyalah untuk menerima data masukan dan meneruskannya ke hidden layers.
- **Jumlah neuron atau unit dalam input layer ditentukan oleh dimensi atau jumlah fitur pada data masukan**. Misalnya, jika data masukan berupa gambar berwarna dengan resolusi 32 × 32 piksel dan tiga saluran warna RGB, input layer akan memiliki 32 × 32 × 3 neuron.

Input layer berperan penting dalam proses pembelajaran jaringan neural. Ini karena data masukan yang disampaikan ke jaringan akan diproses dan dipelajari oleh hidden layers. Fungsinya untuk menghasilkan output yang diinginkan, seperti klasifikasi gambar, prediksi teks, atau regresi numerik.


Hidden Layer:
Hidden layer adalah lapisan-lapisan di antara input layer dan output layer dalam jaringan neural. Tugas utama dari hidden layers adalah untuk mengekstrak fitur-fitur yang semakin abstrak dan kompleks dari data masukan yang telah diteruskan oleh input layer.

Jenis jenis hidden layer:
1. Fully Connected Layer (Dense Layer)
Deskripsi: Setiap neuron pada lapisan ini terhubung dengan setiap neuron dalam lapisan sebelumnya dan sesudahnya.
Penggunaan Umum: Fully connected layer paling umum digunakan dalam jaringan saraf multi-layer perceptron.
Fungsi: Layer ini membantu dalam mempelajari hubungan yang kompleks antara fitur-fitur input.

2. Convolutional Layer (Conv Layer)
Deskripsi: Ini digunakan khusus untuk memproses data spasial, seperti gambar.
Karakteristik: Convolutional layer menggunakan filter atau kernel yang bergerak pada seluruh gambar untuk mengekstrak fitur lokal, seperti tepi, sudut, atau tekstur.
Penggunaan Umum: Lapisan ini digunakan dalam convolutional neural networks untuk tugas-tugas tertentu, seperti pengenalan gambar atau segmentasi objek

3. Batch Normalization Layer
Deskripsi: Batch normalization adalah teknik yang digunakan untuk mempercepat dan stabilisasi pelatihan jaringan neural dengan normalisasi batch. Umumnya ditempatkan setelah lapisan aktivasi (misalnya setelah convolutional layer atau fully connected layer) sebelum lapisan selanjutnya

karakteristik :
- Menghitung mean dan variance dari setiap batch data input.
- Normalisasi input dengan menggunakan mean dan variance untuk mengurangi internal covariate shift.
- Menerapkan transformasi linier pada setiap mini-batch untuk memperbaiki distribusi input ke setiap lapisan.

Penggunaan Umum: Lapisan ini digunakan untuk mempercepat konvergensi pelatihan dengan memungkinkan penggunaan learning rate yang lebih tinggi.


4. Recurrent Layer (RNN, LSTM, GRU)
Deskripsi: Ini digunakan untuk memproses data berurutan, seperti teks, audio, atau video.
Karakteristik: Lapisan ini mempertahankan informasi state (keadaan) dari waktu ke waktu sehingga mampu mengatasi masalah dependensi jarak jauh.
Penggunaan Umum: Recurrent layers, termasuk long short-term memory (LSTM) dan gated recurrent units (GRU), digunakan dalam aplikasi, seperti pemrosesan bahasa alami dan pengenalan suara.

5. Dropout Layer:
Deskripsi: Dropout layer adalah teknik regularisasi yang digunakan untuk mencegah overfitting dalam jaringan neural dengan secara acak "menonaktifkan" sebagian neuron pada setiap iterasi pelatihan.
Karakteristik: Ini menonaktifkan secara acak sebagian neuron dengan probabilitas tertentu selama proses pelatihan dan mencegah neuron menjadi terlalu bergantung pada subset tertentu dari neuron lainnya.
Penggunaan Umum: Ini digunakan di antara lapisan-lapisan tersembunyi pada jaringan dan membuatnya lebih robust dan umumnya efektif dalam mengatasi overfitting

6. Pooling Layer:
Deskripsi: Lapisan ini digunakan untuk mengurangi dimensi spasial dari feature map.
Karakteristik: Pooling layer menggabungkan informasi dari beberapa neuron tetangga untuk mengurangi ukuran representasi data.
Penggunaan Umum: Umumnya, ini digunakan setelah convolutional layers dalam CNN untuk mengurangi overfitting dan menghemat komputasi

7. Flatten Layer:
Deskripsi: Ini adalah lapisan yang mengubah tensor multidimensi (seperti hasil dari convolutional layers) menjadi vektor satu dimensi agar dapat diproses oleh lapisan fully connected layer.
Karakteristik: Flatten layer mengubah representasi data spasial menjadi representasi linear.
Penggunaan Umum: 
Biasanya ditempatkan sebelum masuk ke lapisan fully connected layer (dense layer) di akhir arsitektur.
Ini memungkinkan hasil dari feature extraction (misalnya convolutional layer) dapat dijadikan input untuk proses klasifikasi atau regresi pada lapisan fully connected.


Output Layer:
Output layer adalah layer terakhir dalam deep learning yang menghasilkan output berdasarkan hasil pemrosesan oleh hidden layer. Jumlah neuron dalam output layer bergantung pada tipe tugas yang ingin diselesaikan oleh jaringan. Misalnya, untuk tugas klasifikasi biner, output layer dapat memiliki satu neuron yang menghasilkan nilai antara 0 dan 1 (mengindikasikan probabilitas kelas), sedangkan untuk klasifikasi multiclass, setiap neuron mungkin mewakili probabilitas dalam kelas tertentu. 




Pengenalan Arsitektur Deep Learning yang Populer:


Convolutional Neural Network:
Convolutional Neural Network (CNN) adalah jenis arsitektur deep learning yang dirancang khusus untuk memproses data gambar dan visual dengan efisien. CNN terdiri atas serangkaian lapisan yang dapat mengekstrak fitur-fitur hierarkis dari gambar secara bertahap.

Karakteristik CNN:
- Lapisan konvolusi (convolutional layer): Lapisan ini menggunakan filter konvolusi untuk mengekstrak fitur-fitur lokal dari gambar, seperti tepi, sudut, atau tekstur. Filter ini bergerak di seluruh gambar untuk menghasilkan feature map.
- Lapisan pooling (pooling layer): Lapisan ini mengurangi dimensi spasial dari feature map dengan memilih nilai maksimum (max pooling) atau rata-rata (average pooling) dalam suatu area.
- Lapisan aktivasi (activation layer): Umumnya menggunakan ReLU (Rectified Linear Unit) sebagai fungsi aktivasi untuk memperkenalkan non-linearitas ke dalam jaringan.
- Lapisan fully connected (dense layer): Ini digunakan untuk klasifikasi akhir berdasarkan fitur-fitur yang diekstraksi.


Recurrent Neural Network (RNN):
Recurrent neural network (RNN) adalah arsitektur deep learning yang dirancang untuk memproses data berurutan, seperti teks, audio, atau time series data. RNN memiliki kemampuan untuk "mengingat" informasi dari iterasi sebelumnya melalui loop rekursif

Karakteristik RNN:
- Recurrent loop (loop rekursif): Ini memungkinkan RNN untuk mengolah data berurutan dan mempertahankan konteks dari waktu ke waktu

- Memory cell (sel memori): Pada LSTM dan GRU (variasi RNN yang lebih canggih), ini memungkinkan jaringan untuk mengingat informasi dalam jangka panjang.

Implementasi:
- Pemrosesan bahasa alami, seperti penerjemahan mesin, pembangkitan teks, dan analisis sentimen.
- Pengenalan suara dan pengolahan sinyal audio.
- Prediksi time series, misalnya peramalan harga saham atau cuaca.


Long Short-Term Memory (LSTM): 
Long short-term memory (LSTM) adalah jenis RNN yang ditingkatkan dengan mekanisme gate untuk mengatasi masalah hilangnya informasi jangka panjang dalam pembelajaran berurutan. 

Karakteristik LSTM:
- Forget gate: LSTM dapat "melupakan" informasi yang tidak relevan atau usang dengan menggunakan gate ini 

- Input gate: Ini mengontrol aliran informasi baru yang akan disimpan dalam memori jangka panjang.

Implementasi :
- Pemrosesan bahasa alami yang memerlukan pemahaman konteks lebih dalam, seperti generasi teks yang alami dan jelas
- Pengenalan wicara dan pemrosesan sinyal audio yang membutuhkan pengertian konteks temporal


Generative Adversarial Network:
Generative adversarial network (GAN) adalah arsitektur deep learning terdiri dari dua jaringan neural berlawanan yang saling bersaing, yaitu generator dan diskriminator.

Karakteristik GAN:

- Generator: Ini menghasilkan data sintetis, seperti gambar, dari distribusi laten.
- Diskriminator: Ini mencoba membedakan antara data asli dan data sintetis yang dihasilkan oleh generator.

Implementasi:
- Menghasilkan gambar sintetis untuk aplikasi kreatif, seperti pembuatan gambar wajah palsu atau pembingkaian ulang data.
- Pemberdayaan data dan augmentasi dataset untuk melatih model yang lebih baik 


Transformer:
Transformer adalah arsitektur deep learning berbasis atensi (attention-based) yang digunakan, terutama dalam pemrosesan bahasa alami untuk mengatasi masalah ketergantungan jarak panjang 

Karakteristik Transformer:
- Mekanisme self-attention: Ini memungkinkan model untuk memberikan bobot yang tepat pada kata-kata penting dalam teks 
- Encoder block dan decoder block: Ini digunakan untuk tugas seperti penerjemahan mesin dan pemodelan bahasa

Implementasi:
- Penerjemahan mesin lintas bahasa
- Pemodelan bahasa untuk tugas NLP, seperti analisis teks atau penghasilan teks yang alami.


Autoencoder:
Autoencoder adalah arsitektur deep learning yang digunakan untuk melakukan reduksi dimensi atau generasi data sintetis 

Karakteristik:
- Encoder: Ini menghasilkan representasi tersembunyi dari data input 
- Decoder: Ini memulihkan data asli dari representasi tersembunyi yang dihasilkan oleh encoder 


Implementasi:
- Reduksi dimensi data untuk mempercepat proses pelatihan
- Denoising dan menghasilkan data sintetis untuk penggunaan tertentu


Pengenalan Convolutional Neural Network:
Convolutional neural networks (CNN) pertama kali dikenalkan oleh Yann LeCun dkk., pada tahun 1998 dalam makalahnya “Gradient-Based Learning Applied to Document Recognition”. LeCun mengenalkan versi awal CNN, yaitu LeNet (berasal dari nama LeCun), yang berhasil mengenali karakter tulisan tangan. Saat itu, LeNet hanya mampu bekerja dengan baik pada gambar dengan resolusi rendah.

Database yang digunakan dalam LeCun adalah MNIST Database of Handwritten Digits, terdiri dari pasangan angka 0 hingga 9 dengan labelnya. Dataset MNIST dikenal luas hingga saat ini dan banyak digunakan terutama oleh para pemula untuk melatih model machine learning.

Sejak ditemukannya LeNet, para peneliti terus melakukan riset untuk mengembangkan model CNN. Hingga pada tahun 2012, Alex Krizhevsky memperkenalkan AlexNet, versi lebih canggih dari CNN yang memenangkan perlombaan terkenal: ImageNet. AlexNet ini adalah cikal bakal deep learning, salah satu cabang AI yang menggunakan multi-layer neural networks 

Selain deep learning, salah satu bidang menarik yang muncul dari perkembangan machine learning adalah computer vision. Itu adalah bidang yang memberi komputer kemampuan untuk ‘melihat’ seperti manusia.

Convolutional neural network (CNN) adalah tipe jaringan saraf yang umum digunakan untuk memproses data gambar. CNN digunakan untuk mendeteksi dan mengenali objek dalam gambar, terinspirasi dari cara manusia memproses informasi visual. 

Secara umum, CNN memiliki struktur mirip seperti jaringan saraf biasa dengan neuron yang memiliki bobot, bias, dan fungsi aktivasi. Namun, pembeda CNN dengan lainnya adalah lapisan konvolusi, terdiri dari neuron-neuron yang tersusun dalam filter dengan dimensi panjang dan tinggi (piksel).


Bagaimana CNN Bekerja:
Secara garis besar, CNN memanfaatkan proses konvolusi untuk mengolah gambar. Proses konvolusi melibatkan penggunaan sebuah kernel konvolusi (filter) berukuran tertentu yang digerakkan melintasi seluruh gambar. Pada setiap lokasi dalam gambar, filter ini berinteraksi dengan piksel-piksel di sekitarnya.

Langkah 1: Memecah gambar menjadi bagian-bagian lebih kecil atau jendela-jendela gambar yang disebut "patches”.

Dari gambar seorang anak kecil yang menaiki kuda mainan, hasil dari proses konvolusi dapat diilustrasikan dengan membagi gambar tersebut menjadi bagian-bagian kecil atau "patches".

Dengan menggunakan konvolusi, gambar asli seorang anak kecil di atas dibagi menjadi 77 gambar yang lebih kecil.

Langkah 2: Memasukkan setiap gambar yang lebih kecil ke jaringan saraf yang lebih sederhana (small neural network).

Setiap gambar kecil hasil konvolusi digunakan sebagai input untuk menghasilkan representasi fitur melalui sebuah proses yang memberikan kemampuan kepada convolutional neural network (CNN) dalam mengenali objek, tidak peduli bahwa posisi objek tersebut muncul pada gambar. Proses ini diulang sebanyak 77 kali untuk setiap gambar kecil dengan menggunakan filter yang sama untuk setiap iterasi. 

Dengan demikian, setiap bagian dari gambar kecil akan mengalami transformasi yang sama menggunakan faktor pengali yang sama. Dalam konteks jaringan saraf, hal itu disebut sebagai weights sharing. Jika ada hal menarik atau penting dalam setiap gambar kecil, fitur tersebut akan ditemukan dan diidentifikasi sebagai objek yang relevan (object of interest). 

Proses ini memungkinkan CNN untuk memahami berbagai aspek visual pada gambar secara komprehensif dan memperoleh pemahaman yang lebih dalam tentang objek-objek pada gambar.

Langkah 3: Menyimpan hasil dari masing-masing gambar kecil yang telah melewati proses konvolusi pada sebuah array baru. 

Langkah ketiga dalam pengolahan gambar menggunakan convolutional neural network (CNN) adalah menyimpan hasil dari masing-masing gambar kecil yang telah melewati proses konvolusi pada sebuah array baru.

Setelah setiap gambar kecil melewati lapisan konvolusi dan lapisan-lapisan jaringan saraf (neural network) lainnya, gambar-gambar tersebut akan menghasilkan representasi fitur yang terdiri dari nilai-nilai numerik. Representasi fitur ini mencerminkan informasi penting yang diekstrak dari setiap bagian gambar.

Proses penyimpanan dilakukan dengan mengumpulkan semua representasi fitur dari gambar-gambar kecil dan mengaturnya pada sebuah array data. Array ini akan memiliki dimensi sesuai dengan jumlah gambar kecil yang dihasilkan oleh konvolusi.

Array baru yang berisi representasi fitur dari gambar-gambar kecil tersebut akan digunakan sebagai input untuk langkah-langkah selanjutnya dalam pengolahan data, seperti klasifikasi, deteksi objek, atau segmentasi. Representasi fitur ini menjadi landasan penting pada pengambilan keputusan oleh CNN terkait dengan informasi dalam gambar asli.

Langkah 4: Downsampling.

Langkah keempat dalam convolutional neural network (CNN) setelah representasi fitur dari gambar-gambar kecil disimpan pada array baru adalah melakukan downsampling. Ini adalah proses pengurangan dimensi atau resolusi dari representasi fitur yang telah diperoleh.

Proses downsampling umumnya dilakukan menggunakan lapisan pooling, seperti max pooling atau average pooling. Lapisan pooling mengambil bagian tertentu dari representasi fitur (misalnya, area 2 × 2 atau 3 × 3) dan melakukan operasi statistik, seperti mengambil nilai maksimum atau max pooling maupun rata-rata atau average pooling dari bagian tersebut.

Tujuan dari downsampling adalah mengurangi jumlah parameter dalam jaringan, mengurangi overfitting, dan meningkatkan invariansi terhadap pergeseran spasial. Dengan mengurangi resolusi representasi fitur, CNN dapat mempertahankan informasi penting sambil mempercepat komputasi dan mengurangi kompleksitas model.

Setelah proses downsampling selesai, representasi fitur yang sudah di-downsample akan menjadi input untuk lapisan-lapisan selanjutnya dalam CNN, seperti lapisan konvolusi tambahan, lapisan aktivasi, atau lapisan fully connected. Lapisan-lapisan itu akan terus memproses dan mempelajari fitur-fitur dari gambar secara hierarkis untuk tujuan tertentu, seperti klasifikasi atau deteksi objek. 

Langkah kelima dalam CNN setelah melakukan proses downsampling adalah membuat prediksi berdasarkan representasi fitur yang telah diproses oleh jaringan. Proses ini terjadi di bagian akhir dari arsitektur CNN, setelah representasi fitur melewati beberapa lapisan, yakni konvolusi, aktivasi, dan pooling.

Untuk membuat prediksi, representasi fitur yang telah dihasilkan dari gambar-gambar masukan akan dimasukkan ke lapisan fully connected di akhir jaringan. Lapisan ini akan mengubah representasi fitur menjadi vektor satu dimensi yang kemudian diumpankan ke jaringan saraf terakhir (biasanya berupa softmax) untuk menghasilkan output klasifikas.

Dalam lapisan softmax, vektor fitur akan diubah menjadi distribusi probabilitas yang menunjukkan kemungkinan kelas-kelas berbeda berdasarkan representasi fitur yang diterima. Kelas dengan probabilitas tertinggi akan menjadi prediksi akhir dari CNN untuk gambar yang diberikan.

Proses pembuatan prediksi ini adalah langkah terakhir dalam CNN dan biasanya dilakukan ketika jaringan telah menjalani proses pelatihan untuk mempelajari hubungan antara representasi fitur dengan label atau kelas yang benar dari gambar-gambar pelatihan. Dengan menggunakan prediksi yang dihasilkan oleh CNN, kita dapat mengidentifikasi atau mengklasifikasikan objek atau pola dalam gambar. Tingkat akurasinya pun tinggi berdasarkan pembelajaran dari data pelatihan.



Pengenalan Recurrent Neural Network:
Recurrent neural network (RNN) adalah jenis arsitektur jaringan saraf yang dirancang untuk memproses data berurutan, yakni ketika hubungan antar elemen dalam urutan memiliki arti atau konteks temporal. RNN biasanya digunakan pada data teks (sekuens kata), audio (gelombang suara), deret waktu (data berurutan terkait waktu), dan lainnya. Keunggulan utama RNN adalah kemampuannya untuk "mengingat" atau mempertahankan informasi tentang sejarah (konteks) dari urutan data yang diproses.

Anda dapat menemukan RNN digunakan dalam aplikasi populer, seperti Siri, pencarian suara, dan Google Translate. Sebagaimana halnya jaringan saraf feedforward dan convolutional neural networks (CNNs), RNN juga belajar dari data pelatihan. **RNN mengambil informasi dari input dan output saat ini yang memungkinkannya untuk "mengingat" konteks sebelumnya.**

Tujuan utama RNN adalah memahami dan memproses data yang memiliki struktur urutan atau jangka waktu. Hal ini membuat RNN sangat berguna dalam aplikasi-aplikasi yang melibatkan data berurutan, yaitu pemrosesan bahasa alami atau natural language processing (NLP), pemodelan bahasa, prediksi deret waktu (time series), pengenalan suara, dan lainnya. RNN memungkinkan jaringan saraf untuk memahami konteks temporal dan menghasilkan prediksi atau output berdasarkan urutan masukan yang diberikan.


Tipe-tipe RNN:
Ada empat tipe RNN berdasarkan panjang input dan output yang berbeda.

1. One-to-one adalah jaringan neural sederhana yang umum digunakan untuk masalah pembelajaran mesin dengan satu input dan satu output.

2. One-to-many memiliki satu input dan banyak output. Ini biasanya digunakan untuk menghasilkan deskripsi gambar.

3. Many-to-one mengambil urutan multiple inputs dan memprediksi satu output. Populer dalam klasifikasi sentimen, yaitu ketika inputnya berupa teks dan output-nya adalah kategori.

4. Many-to-many menggunakan multiple inputs dan outputs. Aplikasi paling umumnya adalah terjemahan mesin.


Jenis-jenis RNN:
RNN merupakan salah satu arsitektur jaringan saraf yang sangat berguna dalam memproses data berurutan. Namun, terkadang satu model RNN saja tidak cukup untuk menangani semua tugas yang berbeda. Karena itu, RNN telah berkembang menjadi beberapa jenis yang dibedakan berdasarkan bentuk dasar dan cara kerja masing-masing. Setiap jenis RNN ini dirancang untuk menangani situasi atau masalah tertentu dengan lebih baik. 

Sebagai contoh, ada jenis RNN sederhana yang disebut Vanilla RNN, yang memiliki keterbatasan dalam mengingat informasi dari waktu ke waktu. Kemudian, ada jenis RNN yang lebih canggih, seperti long short-term memory (LSTM) dan gated recurrent unit (GRU), yang dirancang khusus untuk mengatasi masalah hilangnya informasi jangka panjang. Dengan memahami perbedaan dan kelebihan masing-masing jenis RNN, pengembang dan peneliti dapat memilih model yang paling sesuai untuk tugas mereka.

1. RNN Sederhana (Vanila RNN)
RNN sederhana, atau sering disebut sebagai vanilla RNN, adalah bentuk dasar dari arsitektur recurrent neural network. Pada RNN sederhana, setiap neuron memiliki sambungan kembali ke dirinya sendiri. Ini memungkinkan RNN sederhana untuk menggunakan output sebelumnya sebagai input pada langkah waktu berikutnya dalam urutan data

Meskipun RNN sederhana tidaklah rumit dan mudah diimplementasikan, masalah vanishing gradient menjadi kendala utama dalam aplikasi pada data berurutan yang panjang. Inovasi seperti long short-term memory dan gated recurrent unit dikembangkan untuk mengatasi masalah ini dan meningkatkan kemampuan RNN dalam memodelkan hubungan

2. Long Short-Term Memory (LSTM)
LSTM adalah varian RNN yang dikembangkan untuk mengatasi masalah vanishing gradient. LSTM memiliki struktur yang lebih kompleks dengan mekanisme gerbang (gate mechanism) yang memungkinkan model untuk memilih dan melupakan informasi secara selektif. Hal ini membuat LSTM efektif dalam memahami konteks jangka panjang pada data berurutan, seperti pengenalan teks atau prediksi deret waktu

Dengan menggunakan mekanisme gerbang ini, LSTM dapat mengontrol aliran informasi lebih baik dan mempertahankan informasi yang relevan dalam cell state. Hal ini memungkinkan LSTM untuk efektif mengatasi masalah vanishing gradient dan memodelkan konteks jangka panjang pada data berurutan, seperti dalam pengenalan teks atau prediksi deret waktu


3. Gated Recurrent Unit (GRU)
Gated recurrent unit (GRU) adalah variasi dari LSTM karena keduanya memiliki kemiripan dalam desain. Dalam beberapa kasus, keduanya pun membuat hasil yang serupa.

GRU menggunakan gerbang pembaruan (update gate) dan gerbang reset (reset gate) untuk mengatasi masalah vanishing gradient. Gerbang-gerbang ini memutuskan informasi yang penting dan meneruskannya ke output. Gerbang-gerbang ini dapat dilatih untuk menyimpan informasi dari waktu yang lama tanpa menghilang seiring berjalannya waktu atau menghapus informasi tidak relevan.

Berbeda dengan LSTM, GRU tidak memiliki cell state. GRU hanya memiliki state tersembunyi (hidden state). Lalu, sebab arsitekturnya yang sederhana, GRU memiliki waktu pelatihan lebih singkat dibandingkan model LSTM. Arsitektur GRU mudah dipahami karena mengambil input xt dan state tersembunyi dari timestamp sebelumnya ht-1, lalu menghasilkan state tersembunyi baru ht.


Cara Kerja RNN:

Langkah 1: Pengolahan Input

RNN menerima input berurutan, seperti kata-kata dalam sebuah kalimat atau frame pada video. Setiap elemen input direpresentasikan dengan vektor fitur numerik. Misalnya, dalam pemrosesan teks, setiap kata dapat diubah menjadi vektor berdasarkan representasi tertentu, seperti word embedding. Input ini diberikan satu per satu ke jaringan, dimulai sejak elemen pertama hingga elemen terakhir dari urutan data.

Langkah 2: Perhitungan Aktivasi

Setiap unit (neuron) dalam RNN menghitung aktivasi berdasarkan input saat ini dan status internal (state) dari unit pada waktu sebelumnya. Aktivasi ini mencerminkan informasi yang telah dipelajari dari konteks sebelumnya dalam urutan data. Perhitungan aktivasi dilakukan menggunakan fungsi aktivasi, seperti tangen hiperbolik (tanh) atau fungsi sigmoid

Langkah 3: Pembaruan Status Internal

Setiap unit RNN memiliki status internal yang menyimpan informasi dari elemen-elemen sebelumnya dalam urutan data. Status internal tersebut diperbarui pada setiap langkah waktu dengan mempertimbangkan aktivasi saat ini dan status internal sebelumnya. Pembaruan status internal dapat dijelaskan dengan rumus matematis yang melibatkan operasi matematika, seperti perkalian matriks antara vektor input dan bobot serta penambahan bias.

Langkah 4: Output

Pada setiap langkah waktu, RNN menghasilkan output berdasarkan aktivasi saat ini atau status internal terakhir. Output ini dapat digunakan sebagai prediksi berikutnya dalam urutan (misalnya, kata berikutnya pada kalimat) atau sebagai hasil akhir dari proses pemrosesan urutan data. Output RNN dapat digunakan dalam berbagai tugas, seperti klasifikasi, regresi, atau generasi urutan.

Proses tersebut diulang untuk setiap elemen dalam urutan data. Hal itu memungkinkan RNN untuk memahami konteks temporal dan memproses data berurutan secara dinamis. Dengan memanfaatkan hubungan temporal antar elemen data, RNN dapat menghasilkan prediksi atau output yang relevan untuk berbagai tugas pemrosesan data. Secara keseluruhan, RNN adalah alat yang kuat untuk memodelkan informasi berurutan. RNN pun telah digunakan dalam berbagai aplikasi yang memerlukan pemahaman dan pengolahan data temporal. 

1. Bagaimana komputasi yang dilakukan oleh sebuah neuron dalam Artificial Neural Network? 
b. Sebuah neuron menghitung fungsi linear (z = Wx + b) diikuti oleh fungsi aktivasi.


2. Bagaimana lapisan konvolusi dalam CNN menghasilkan fitur-fitur yang semakin kompleks?
b. Dengan menambahkan lebih banyak lapisan konvolusi.

3. Mengapa tantangan utama RNN disebut "vanishing gradient problem"?
b. Vanishing gradient problem terjadi ketika RNN tidak mampu menghasilkan output untuk urutan input yang panjang
d. Vanishing gradient problem terjadi ketika RNN hanya dapat menghasilkan output untuk input dengan dimensi rendah.

4. Apa keuntungan menggunakan Convolutional Neural Network (CNN) dibandingkan dengan jaringan saraf feedforward tradisional untuk tugas pengolahan gambar?
a. CNN dapat memproses gambar secara lokal dengan mempertimbangkan relasi spasial antar piksel.




---------------------------------------------------------------------------
                Neural Network Dengan Tensorflow dan Keras
---------------------------------------------------------------------------
 Ryan memberikan sebuah saran kepada Diana, “Na, kamu pernah dengar tidak istilah ‘garbage in garbage out’? Istilah tersebut sangat ramai ketika aku mempelajari deep learning, coba deh pelajari itu juga.” Gigo mengacu pada prinsip bahwa kualitas keluaran dari sebuah sistem komputer (termasuk model machine learning) sangat tergantung pada kualitas masukan yang diterimanya. Jika data yang digunakan sebagai masukan berkualitas buruk atau tidak relevan, maka hasil yang diperoleh dari model juga akan berkualitas buruk atau tidak akurat 

Setelah menyadari hal itu, Diana melanjutkan pembelajarannya dengan metode slow but sure. Ia tidak mau melewatkan lagi aspek kecil yang dapat menjadi penghalang di kemudian hari


Pengenalan tensorflow dan Keras untuk Deep Learning:

Selain itu, kita juga akan belajar menggunakan TensorFlow Datasets untuk memudahkan pengaturan data loading, mempermudah pelatihan model sehingga Anda semakin dekat menjadi seorang praktisi machine learning.

“Bagaimana membuat berbagai macam layer yang sebelumnya dipelajari?” Sekarang saatnya kita mengenal sebuah kekuatan yang lebih besar untuk membangun model deep learning dan berbagai macam layer dengan lebih mudah, perkenalkan TensorFlow dan Keras! 

Seperti yang dapat Anda lihat pada gambar di atas, TensorFlow dan Keras merupakan framework yang sangat terkenal di kalangan praktisi machine learning, tetapi hal ini bukan berarti framework lainnya kurang bagus, ya. Hasil ini merupakan riset yang dilakukan oleh Jeff Hale pada tahun 2018 mengenai deep learning. Riset tersebut menggunakan 11 sumber data pada 7 kategori yang berbeda untuk mengukur popularitas framework berdasarkan ketertarikan pengguna. Hasilnya TensorFlow menang telak, bukan? Hal ini terjadi karena kedua framework ini memiliki beberapa kelebihan, seperti Open-source, mudah untuk debugging, compatibility yang baik, scalability yang fleksibel, komunitas yang luas, dan masih banyak lagi. 


TensorFlow (TF)

TensorFlow (TF) adalah platform open-source end-to-end yang dikembangkan oleh Google Brain dan sangat populer untuk pengembangan machine learning berskala besar. TensorFlow memiliki ekosistem tools, library, dan sumber daya komunitas yang komprehensif dan fleksibel, sehingga memungkinkan para peneliti dan pengembang membangun serta menerapkan aplikasi machine learning dengan mudah

Perjalanan TensorFlow sangatlah panjang, semuanya berawal pada perilisan TensorFlow pada tahun 2015 TensorFlow dibuat open-source di bawah lisensi Apache. Perkembangan yang signifikan dan cepat terus diperlihatkan oleh TensorFlow, hingga pada tahun 2019 sebuah update dikeluarkan oleh Google dengan nama baru yaitu TensorFlow 2.0. 

Mengapa update ini sangat berpengaruh? Ada beberapa hal yang perlu kita highlight dari perkembangan TensorFlow ini mulai dari hadirnya TensorBoard, fitur kolom, kompatibilitas hardware, hingga kemunculan TensorFlow Datasets. 

Secara umum, TensorFlow memiliki banyak kelebihan seperti yang sudah disebutkan di atas. Tentunya Anda penasaran terkait kelebihan tersebut, bukan? Mari kita bahas kelebihan TensorFlow satu per satu agar Anda lebih terpacu untuk menyelesaikan misi menjadi praktisi machine learning yang andal.

1. Open Source
Sebagai sebuah frameworkopen-source tentunya penggunaan TensorFlow tidak mengeluarkan biaya dan bisa digunakan oleh semua orang mulai dari pelajar, guru, dosen, peneliti, data scientist, machine learning engineer, dan lainnya. Selain itu, hal ini juga memungkinkan untuk menggunakan TensorFlow kapan saja serta di mana saja sesuai kebutuhan pengguna tanpa harus memikirkan pengeluaran sedikitpun.

2. Scalability
Hingga saat ini, TensorFlow tidak terbatas hanya pada satu platform saja. Kelebihan ini tentu menguntungkan bagi pengguna karena kita dapat membangun sebuah sistem AI menggunakan TensorFlow di berbagai platform dengan komputasi atau performa yang sama. Selain itu, model yang dihasilkan TensorFlow juga dapat menyesuaikan komputasi yang digunakan berdasarkan jumlah requests. Sangat menarik, bukan? 

3. Compatibility
Saat ini, kompatibilitas/kecocokan merupakan salah satu faktor yang paling penting. Nah, pada kasus ini, TensorFlow kompatibel dengan berbagai macam bahasa pemrograman, seperti Python, C++, JavaScript, Kotlin, dan lain sebagainya. Selain itu TensorFlow juga dapat diimplementasikan pada berbagai platform menggunakan TensorFlow Lite, TensorFlow Js dan TensorFlow Serving.

Selain itu, TensorFlow juga menyediakan beberapa package yang dapat disandingkan dengan bahasa pemrograman yang lebih banyak, seperti C#, Haskell, Julia, MATLAB, R, Pascal, Rust, OCaml, Crystal, dan masih banyak lagi.

Sebagai praktisi machine learning yang umumnya menggunakan Python, Anda tidak perlu khawatir lagi dengan requests dari stakeholder yang menggunakan bahasa pemrograman lain.

4. Parallelism
Hal krusial lainnya yang mampu ditangani TensorFlow adalah hardware acceleration Mungkin sebagian dari Anda bertanya, “Apa itu hardware acceleration?” Ini merupakan sebuah library yang memberikan TensorFlow kemampuan untuk membagikan ketersediaan hardware seperti CPU atau GPU pada proses pelatihan maupun prediksi.

TensorFlow hadir dengan tf.distribute() sebagai salah satu modul dalam TensorFlow yang menyediakan tools dan API untuk melatih model deep learning secara terdistribusi. Ini sangat berguna untuk mempercepat pelatihan dengan menggunakan beberapa perangkat keras seperti GPU atau TPU dan untuk mengelola pelatihan di lingkungan yang besar.

5. Graph
Untuk meningkatkan pengalaman developer ketika membangun sebuah machine learning, tentunya kita juga memerlukan pengukuran yang lebih mudah dengan menggunakan visualisasi. Hal itu akan meringankan beban kognitif yang dibebankan kepada otak kita. Setelah memikirkan banyak hal pada setiap tahapan pembangunan machine learning, tentunya kita juga memerlukan sebuah alat yang dapat mempermudah perhitungan performa model.

Oleh karena itu, TensorFlow menyediakan sebuah alat bernama TensorBoard yang dapat membantu kita untuk menghitung performa model yang disajikan dalam bentuk visualisasi. TensorBoard memungkinkan pelacakan metrik percobaan, seperti loss dan akurasi, memvisualisasikan grafik model, memproyeksikan embedding ke dimensi yang lebih kecil, dan banyak lagi. Tentunya alat ini akan sangat membantu, bukan?

6. Architectural Support
TensorFlow didukung oleh beberapa hardware yang dapat melakukan proses pelatihan, mulai dari CPU, GPU, hingga TPU. Pada tahap eksplorasi, CPU dapat dikatakan sangat cukup untuk melakukan pelatihan machine learning maupun deep learning. Namun, untuk beberapa kasus seperti dataset yang sangat besar atau data unstructured (gambar, suara, video, dan lain sebagainya), GPU akan memproses pelatihan dengan lebih cepat. Hal ini dikarenakan GPU memiliki sebuah arsitektur yang dapat melakukan proses yang sangat banyak secara bersamaan.

Arsitektur ini bernama Compute Unified Device Architecture (CUDA) yang sayangnya pada saat ini hanya dimiliki oleh brand NVIDIA. Eitsss, jangan khawatir, Anda juga dapat menggunakan VGA lainnya untuk melakukan proses pelatihan dengan lebih cepat. Anda dapat melihat perbandingannya pada video di sini ya. 

Lalu, bagaimana jika tidak memiliki VGA tambahan atau komputer yang mempunyai spesifikasi rendah? Tentunya ada beberapa cara agar Anda dapat melakukan pelatihan dalam waktu yang lebih singkat dengan hardware yang mumpuni.

Anda dapat menggunakan Google Colab dan Kaggle untuk mendukung proses pelatihan dengan dataset yang besar sehingga dapat meminimalisasi waktu pelatihan. Kedua platform tersebut menyediakan environment yang dapat menjalankan kebutuhan pelatihan dengan sangat baik. Anda juga dapat mengatur penggunaan CPU, GPU, bahkan TPU saat menggunakan platform tersebut. Wow, keren bukan?

7. Library Management
Sebagai salah satu framework yang dikembangkan oleh Google, tentunya TensorFlow memiliki update yang sangat baik. TensorFlow sendiri melakukan pembaruan dalam waktu yang sangat cepat agar dapat memberikan pengalaman yang maksimal kepada seluruh developernya. Hingga saat ini, pembaruan TensorFlow masih terus dikembangkan. Hal ini dibuktikan dengan rilisnya versi 2.16.1 pada 8 Maret 2024. Tentunya dengan pembaruan ini membuat para praktisi machine learning tidak perlu merasa takut dengan framework yang deprecated sehingga kreatifitas developer bisa tetap tersalurkan..

Kelebihan di atas merupakan highlight yang perlu kita perhatikan. Selain itu, masih ada banyak sekali kelebihan TensorFlow yang akan Anda rasakan ketika membangun model machine learning menggunakan framework tersebut. Sampai di sini tentunya Anda sudah paham alasan TensorFlow menjadi sangat populer di kalangan praktisi machine learning, ‘kan?  



Keras: 
Halo para pejuang praktisi machine learning! Sekarang saatnya kita mempelajari Keras sebagai salah satu library open-source yang sangat populer. Keras merupakan sebuah open-source high-level neural network yang dikembangkan oleh François Chollet pada tahun 2015. 

Keras adalah API untuk mengembangkan jaringan saraf tiruan. Dengan Keras, kita akan lebih mudah dalam membuat sebuah multi layer perceptron dan convolutional neural network karena kemampuannya untuk membangun dan mengelola model dengan pendekatan berbasis lapisan (layer modeling). Aplikasi dari Keras sangat luas dan memungkinkan kita untuk membangun jaringan saraf tiruan dengan tujuan klasifikasi gambar, pemrosesan bahasa alami, pengenalan suara, dan prediksi time series.

Komponen inti pembangun sebuah jaringan saraf tiruan dalam Keras adalah layer. Sebuah layer pada Keras, sama dengan sebuah layer pada MLP yang memiliki beberapa perceptron. Terdengar sangat kompleks, bukan? Namun, penggunaan Keras saat ini sangatlah mudah.

Pada pertengahan tahun 2017, Keras diadopsi dan bergabung dengan TensorFlow. Ini menjadi sebuah keuntungan bagi para praktisi machine learning karena integrasi tersebut membuat Keras lebih mudah diakses oleh pengguna cukup dengan memanggil fungsi tf.keras.module. 

Sebelumnya, kita sudah mendengar MLP menggunakan Keras. Jika Anda memiliki pertanyaan, “Apa itu MLP dan bagaimana cara menggunakannya?”, ini merupakan jalur yang tepat! Mari kita bahas bersama-sama untuk mengetahui jawaban pertanyaan tersebut.

Multi Layer Perceptron (MLP) adalah sebuah jaringan saraf yang terdiri dari satu layer input, satu atau lebih hidden layer, dan satu output layer. MLP yang memiliki banyak hidden layer disebut juga Deep Neural Network (DNN).

mnist = tf.keras.datasets.fashion_mnist
 
(x_train, y_train), (x_test, y_test) = mnist.load_data()
X_train, x_test = x_train / 255.0, x_test / 255.0
 
model = tf.keras.models.Sequentials([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax) #ada 10 kelas
])
model.compile(optimizer = tf.optimizers.Adam(),
                              loss = ‘sparse_categorical_crossentropy’,
                              Metrics = [‘accuracy’])
model.fit(x_train, y_train, epochs = 10)


Langkah pertama adalah kita perlu mempersiapkan data kemudian membaginya menjadi data latih dan data uji. Data fashion MNIST bisa kita dapatkan dengan mudah dari library datasets yang disediakan Keras. Nah, untuk kalian yang belum tahu, fashion MNIST ini merupakan dataset yang umum digunakan dalam tugas-tugas machine learning dan computer vision, terutama untuk klasifikasi gambar. Dataset ini memiliki struktur yang mirip dengan dataset MNIST yang terkenal namun berisi gambar-gambar item pakaian daripada angka tulisan tangan dan sudah disediakan oleh TensorFlow Dataset.

Dalam klasifikasi gambar, setiap piksel pada gambar memiliki nilai dari 0 sampai 255. Selanjutnya, perlu melakukan normalisasi dengan membagi setiap piksel pada gambar dengan 255. Dengan normalisasi, model machine learning dapat dilatih dengan lebih cepat, stabil, dan akurat, sehingga meningkatkan performa dan generalisasi model pada data baru

Pada langkah berikutnya, definisikan arsitektur dari jaringan saraf yang akan kita latih. Untuk membuat sebuah MLP, kita hanya perlu mendefinisikan sebuah input layer, hidden layer, dan sebuah output layer. 

Untuk membuat sebuah model MLP di Keras, kita bisa memanggil fungsi tf.keras.models.Sequential([...]) dan menampungnya pada sebuah variabel. Model sequential pada keras adalah tumpukan layer-layer yang sama seperti pada sebuah MLP. Kode tersebut dapat ditulis seperti berikut.

- Input Layer
Layer yang memiliki parameter ‘input_shape’. Input_shape sendiri adalah resolusi dari gambar-gambar pada data latih. Dalam hal ini, sebuah gambar MNIST memiliki resolusi 28x28 piksel sehingga input shape-nya adalah (28, 28). Sebuah layer Flatten pada Keras akan berfungsi untuk meratakan input. Meratakan di sini artinya mengubah gambar yang merupakan matriks 2 dimensi menjadi larik/array 1 dimensi. Pada kasus kita, sebuah gambar MNIST yang merupakan matriks 28x 28 elemen akan diubah menjadi array satu dimensi sebesar 784 elemen

- Hidden Layer
Dense layer pada Keras merupakan layer yang dapat dipakai sebagai hidden layer dan output layer pada sebuah MLP. Parameter unit merupakan jumlah perceptron pada sebuah layer. Kita dapat menggunakan fungsi aktivasi rectified linear unit (Relu) atau fungsi aktivasi lainnya pada layer ini.

- Output layer
Layer ini dapat didefinisikan dengan membuat sebuah Dense layer. Jumlah unit menyesuaikan dengan jumlah label pada dataset. Untuk fungsi aktivasi pada layer output, gunakan fungsi aktivasi Sigmoid jika hanya terdapat 2 kelas/label pada dataset. Untuk dataset yang memiliki 3 kelas atau lebih, gunakan fungsi aktivasi Softmax. Fungsi aktivasi softmax akan memilih kelas mana yang memiliki probabilitas tertinggi. Untuk data fashion MNIST, kita akan menggunakan fungsi aktivasi softmax karena terdapat 10 kelas.

Sampai sini, tentunya Anda sudah dapat membuat arsitektur dari MLP, tetapi model kita belum bisa melakukan tugasnya yaitu memprediksi. Agar model dapat belajar dan memprediksi, kita perlu memanggil fungsi compile pada model tersebut dan menambahkan optimizer dan loss function. 

Untuk optimizer, kita bisa menggunakan Adam seperti yang sudah dijelaskan pada modul sebelumnya. Selanjutnya untuk loss function, kita dapat menggunakan sparse categorical entropy pada kasus klasifikasi 3 kelas atau lebih. Untuk masalah 2 kelas, loss function yang lebih tepat adalah binary cross entropy. Parameter metrics berfungsi untuk menampilkan metrik yang dipilih pada proses pelatihan model.

Setelah membuat arsitektur MLP dan menentukan optimizer serta loss functionnya, kita dapat melatih model kita pada data training. Parameter epoch merupakan jumlah berapa kali sebuah model melakukan propagasi balik (back propagation).



Pra-Pemrosesan Data Untukk Model:
Tahap pemrosesan data merupakan perubahan dari data mentah yang dibersihkan dan diatur untuk tahap pemrosesan berikutnya. Selama tahapan pemrosesan data, data mentah harus diperiksa dengan sangat cermat agar kita dapat memahami karakteristik dataset yang akan dilatih.

Dataset untuk setiap masalah tentunya berbeda. Contohnya pada dataset untuk klasifikasi gambar dan dataset untuk pemrosesan bahasa alami atau NLP. Pada dataset untuk klasifikasi gambar, sampelnya berupa kumpulan gambar atau matriks hasil representasi sebuah gambar seperti gambar berikut. 

Berbeda dengan dataset NLP, sampelnya tentu berupa kalimat-kalimat yang terdiri dari sejumlah kata tertentu seperti gambar di bawah.

Tidak seperti manusia yang bisa mengenali gambar atau memahami kalimat secara langsung, kita perlu melakukan pemrosesan dahulu agar data tersebut siap diterima oleh model. 

Secara umum, ada empat tahapan yang perlu kita lakukan dalam memproses data.

1. Ubah dataset ke dalam bentuk larik/array. Yup, larik berisi angka-angka adalah format data yang dapat diterima oleh model kita. Sama seperti pada kelas Belajar Machine Learning untuk Pemula, model kita menerima gambar sebagai matriks atau larik 2 dimensi.
2. Pisahkan atribut dan label pada data. Model kita akan mempelajari korelasi antara atribut dan label pada dataset.
3. Ubah skala data dalam skala yang seragam. Nama teknik ini adalah normalization. Normalization dilakukan karena NN akan memproses nilai yang berada di antara 0 dan 1  sehingga membuat komputasi lebih optimal. Teknik ini dibutuhkan sehingga komputasi lebih optimal.
4. Terakhir, pisahkan dataset ke dalam data latih dan data uji. Betul, kita memerlukan data uji untuk mengevaluasi kinerja dari model yang telah kita latih.


Pemrosesan Data Gambar:
Kita akan menerapkan data augmentation untuk data latih dan data validasi agar dataset yang Anda miliki dapat diterima dengan baik oleh model sebelum pelatihan dimulai. Pada tahap ini, Anda memiliki dua opsi yang bisa digunakan untuk melakukan augmentasi, kondisi ini menyesuaikan dengan versi TensorFlow yang Anda instal pada komputer atau environment Anda. 

Kasus pertama jika menggunakan TensorFlow versi <= 2.9, Anda dapat melakukan augmentasi menggunakan fungsi ImageDataGenerator untuk data latih dan data validasi. Saat ini, peringatan penghentian penggunaan akan muncul ketika Anda menggunakan fungsi tersebut.

Namun, untuk kasus kedua ketika menggunakan TensorFlow versi > 2.9, Anda dapat menggunakan tf.data dan layer augmentasi yang secara langsung dapat dimasukkan pada jajaran layer sequentials Anda.

Sampai di sini sudah mulai cukup menantang, ‘kan? Jangan risau terlebih dahulu, mari kita bahas bersama-sama implementasi dari kedua kasus di atas agar data yang kita miliki dapat melakukan proses pelatihan dengan baik.


Data Augmentasi dengan Tensorflow <= 2.9
Seperti yang sudah Anda pelajari sebelumnya, pada kasus ini kita dapat menggunakan fungsi ImageDataGenerator. ImageDataGenerator merupakan sebuah fungsi yang sangat berguna untuk mempersiapkan data latih dan data validasi. Beberapa kemudahan yang disediakan ImageDataGenerator antara lain, preprocessing data, pelabelan sampel otomatis, dan augmentasi gambar.

Augmentasi gambar merupakan sebuah teknik yang dapat digunakan untuk memperbanyak data latih dengan cara menduplikasi gambar yang telah ada dengan menambahkan variasi tertentu seperti rescale, rotation, zoom, dan lain sebagainya. Anda juga dapat melihat detail mengenai augmentasi gambar menggunakan ImageDataGenerator pada tautan di sini.

Kode berikut menunjukkan proses augmentasi gambar pada setiap sampel di dataset.

from tensorflow.keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')
 
test_datagen = ImageDataGenerator(
                    rescale=1./255)

Fungsi di atas berguna untuk memperbanyak data latih dengan mengubah kondisi gambar menyesuaikan dengan layer yang digunakan, berikut hasil gambar yang telah dilakukan augmentasi.

Terlihat sangat mudah, bukan? Walaupun terlihat mudah tetapi fungsi ini berguna untuk meningkatkan keberagaman data pelatihan dengan cara membuat variasi dari data yang ada. Tujuan utamanya adalah untuk meningkatkan kinerja model dengan mengurangi overfitting dan meningkatkan generalisasi.


Data Augmentasi dengan TensorFlow > 2.9
Setelah Anda mempelajari penggunaan data augmentasi menggunakan versi lama, tahapan ini mungkin terasa useless karena Anda sudah dapat menjalankan augmentasi dengan lancar dan mudah. Namun, metode kedua ini akan sangat berguna ketika perusahaan atau proyek Anda memiliki kebutuhan untuk menggunakan library terbaru. Selain itu, kasus ini juga akan membantu Anda ketika TensorFlow versi lama sudah deprecated dan tidak bisa digunakan lagi.

Dengan menggunakan TensorFlow versi > 2.9, Anda dapat mengubah ukuran gambar dan piksel secara eksplisit dengan memanfaatkan layers sequential seperti kode berikut.

IMG_SIZE = 180
 
resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMG_SIZE, IMG_SIZE),
  layers.Rescaling(1./255)
])

Selanjutnya, Anda harus melakukan data augmentasi menggunakan berbagai macam fungsi augmentasi, seperti tf.keras.layers.Resizing, tf.keras.layers.Rescaling, tf.keras.layers.RandomFlip, tf.keras.layers.RandomRotation, dan lain sebagainya. Penggunaan fungsi menyesuaikan kebutuhan dan karakteristik dataset yang Anda miliki ketika membangun sebuah model machine learning. Berikut penggunaan beberapa layer augmentasi dengan menggunakan tf.keras.layers.

data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
])

Pada tahap terakhir, Anda perlu memasukkan data augmentasi tersebut pada layer sequentials ketika membangun struktur neural network agar seluruh dataset yang ada dapat diproses dan melakukan pelatihan dengan baik. Berikut contoh kode yang dapat Anda lakukan untuk menerapkan data augmentasi.

model = tf.keras.Sequential([
  # Menambahkan processing image yang telah didefinisikan sebelumnya
  resize_and_rescale,
  data_augmentation,
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  # Sesuaikan sisa layer dengan kasus yang Anda miliki
])

Pada kasus ini, ada tiga hal penting yang perlu Anda perhatikan ketika membangun model neural network menggunakan TensorFlow versi > 2.9.
1. Data augmentasi akan berjalan pada device yang digunakan, baik itu lokal maupun cloud environment seperti Google Colab atau Kaggle Kernel. Proses ini akan berjalan secara bersamaan dengan eksekusi layer lainnya. Hal ini menyebabkan komputasi yang dilakukan akan lebih berat sehingga penggunaan GPU akan lebih menguntungkan.

2. Ketika Anda mengekspor model menggunakan model.save, layer processing ini akan disimpan bersama dengan layer lainnya. Jika Anda nantinya menggunakan model ini, model ini akan secara otomatis menstandarkan gambar (sesuai dengan konfigurasi layer Anda). Hal ini dapat membantu Anda untuk mengimplementasikan ulang logika tersebut di sisi server dengan lebih mudah.

3. Data augmentasi tidak akan aktif pada saat pengujian sehingga gambar input hanya akan ditambah selama pemanggilan Model.fit (bukan Model.evaluate atau Model.predict).


Dengan menggunakan kasus kedua ini, komputasi yang dilakukan akan lebih berat karena berjalan bersamaan dengan layer lainnya ketika proses pelatihan dijalankan. Namun, Anda tidak perlu khawatir karena ini terjadi hanya pada saat proses pelatihan saja. 

Sampai di sini, Anda sudah dapat melakukan augmentasi terhadap data dengan tipe gambar (unstructured data) sehingga data yang Anda miliki sudah dapat diterima dengan baik oleh model yang akan dibangun. Selanjutnya, Anda akan mempelajari pemrosesan data bahasa agar dapat membuat model dengan tipe lainnya. Silakan rehat sejenak dan mengambil secangkir kopi untuk menemani perjalanan indah yang akan Anda jalani pada tahap berikutnya, ya!


Pemrosesan Data Bahasa:
Halo pelopor machine learning masa depan! Apakah Anda sudah menyiapkan secangkir kopi dan semangat yang membara untuk mengarungi perjalanan terakhir kita pada materi ini? Tarik jangkar dan mari kita berlayar!

Pada kasus ini, kita akan belajar pemrosesan bahasa manusia. Berbeda dengan pemrosesan gambar yang telah kita pelajari, pemrosesan teks memiliki tantangan tersendiri, seperti perbedaan panjang teks, bahasa, serta bagaimana merepresentasikan teks ke dalam format yang dapat diterima oleh sebuah model.

Mungkin tebersit di benak Anda sebuah pertanyaan, “Bagaimana komputer memproses sebuah kalimat?” Jika Pertanyaan tersebut muncul, Anda berada pada jalur yang tepat! Berikut garis besar cara komputer untuk memproses sebuah kalimat.

Gambar di atas merepresentasikan langkah umum pemrosesan data yang telah kita bahas sebelumnya. Hal ini karena komputer memiliki sifat tidak seperti manusia yang bisa mengenali gambar atau memahami kalimat secara langsung, kita perlu melakukan pemrosesan dahulu agar data tersebut siap diterima oleh model. Sekarang saatnya kita mempelajari tahapan untuk mengubah kata atau kalimat menjadi angka atau larik sehingga dapat diterima oleh model neural network.

Tanpa tanpa berlama-lama lagi, mari kita langsung bahas bersama pemrosesan data yang perlu dilakukan untuk data bahasa alami pada kode berikut.

sentences = ["I love my cat"]
 
tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentence)
 
print(tokenizer.word_index)
print(sequences)


Output
{'i': 1, 'love': 2, 'my': 3, 'cat': 4}
[[1, 2, 3, 4]]

Dengan menggunakan kode di atas, Anda sudah dapat mengubah kalimat menjadi sebuah larik/array yang dapat digunakan untuk proses pelatihan model neural network. Tokenizer digunakan untuk membangun indeks kata dari korpus teks. Ini membuat kamus dari kata-kata yang ada dalam teks dan memberi indeks unik untuk setiap kata. Sedangkan texts_to_sequences bertugas untuk mengonversi teks menjadi urutan indeks numerik berdasarkan kamus yang dibuat.

Namun, kondisi tersebut masih ringkih dan belum mendapatkan performa yang cukup baik karena bentuk dari data yang Anda miliki masih beragam dan terbatas pada kata-kata yang tidak termasuk dalam dataset. 

Untuk mengatasi permasalahan tersebut, kita dapat menggunakan fungsi oov_token dan padding agar dataset yang Anda miliki lebih baik karena memiliki shape yang serupa. 

Pada kasus ini, oov_token berguna untuk mengatasi kata yang tidak termasuk pada tokenizer sehingga akan diubah menjadi special value yang dapat kita tentukan sendiri. Oleh karena itu, model dapat melakukan perhitungan ketika menemukan kata yang tidak terlihat. Lalu, padding berguna untuk menyesuaikan semua kamus dalam satu urutan yang serupa. Jadi, semua urutan memiliki panjang yang sama. Berikut merupakan contoh implementasi penggunaan oov_token dan padding secara bersamaan.

sentences = ["I love my cat",
             "Do you think my cat is cute?"]
 
 
tokenizer = Tokenizer(num_words = 100, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
sentences = ["I love my cat",
             "Do you think my cat is cute?",
             "Additional cat for you"]
sequences = tokenizer.texts_to_sequences(sentences)
 
padded = pad_sequences(sequences, padding="post", truncating="post", maxlen=10)
 
print("Tokenizer: ",tokenizer.word_index)
print("Sequences: ",sequences)
print("Padded: ",padded)
 

Output
Tokenizer:  {'<OOV>': 1, 'my': 2, 'cat': 3, 'i': 4, 'love': 5, 'do': 6, 'you': 7, 'think': 8, 'is': 9, 'cute': 10}
Sequences:  [[4, 5, 2, 3], [6, 7, 8, 2, 3, 9, 10], [1, 3, 1, 7]]
Padded:  [[ 4  5  2  3  0  0  0  0  0  0]
 [ 6  7  8  2  3  9 10  0  0  0]
 [ 1  3  1  7  0  0  0  0  0  0]]


Seperti yang Anda lihat pada kode di atas, dengan menggunakan oov_token dan padding, kita akan mendapatkan sebuah larik/array yang memiliki ukuran sama sehingga memberikan performa yang lebih baik ketika pelatihan model dijalankan. Hal ini dikarenakan padding merupakan teknik yang digunakan untuk memastikan bahwa semua urutan teks (sequences) memiliki panjang yang sama. Ini penting karena sebagian besar model machine learning, terutama neural networks, mengharuskan input mereka memiliki ukuran yang konsisten.

Sampai pada tahap ini, Anda sudah memiliki sebuah data yang siap untuk melakukan pelatihan dan dapat diterima dengan baik oleh layer embedding. Ngomong-ngomong, layer embedding ini berguna untuk mengubah larik menjadi representasi kata yang memungkinkan kata-kata dengan arti yang sama memiliki representasi yang sama. Sedikit bocoran layer embedding ini akan mengubah larik/array menjadi seperti berikut.



Menggunakan Model Untuk Melakukan Prediksi:
Sama seperti model ML yang disediakan pada library Scikit Learn, mekanisme kerja dari model library TensorFlow dan Keras API juga sama. Yup, untuk melatih model, kita hanya perlu memanggil fungsi fit() dan mengisi parameter atribut dan label pada dataset serta jumlah epoch yang harus dilakukan. 

Nantinya, Anda akan menjadi seorang praktisi machine learning andal yang dapat membangun segala macam model neural network di berbagai industri atau perusahaan kalian kelak. Namun, sekarang kita akan mempelajari pembuatan model neural network atau jaringan saraf tiruan dasar sehingga Anda dapat melakukan perubahan atau penyesuaian dengan lebih andal pada kasus yang Anda hadapi kelak.

Bayangkan ketika Anda mempelajari fungsi matematika pada bangku sekolah sekitar kelas 8 atau 9. Tentunya Anda akan sangat mudah ketika menyelesaikan sebuah fungsi(x) = x + 9


x = -4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0
f(x) = 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0


Sebuah model dari API Keras dapat menerima masukan dengan tipe data numpy array sehingga kita bisa membuat 2 buah objek bertipe numpy array, satu untuk atribut (x) dan satu lagi sebagai labelnya (f(x)). Untuk jenis tipe data lain yang dapat diterima sebagai masukan sebuah model dari API Keras, Anda dapat mengunjungi tautan berikut. 

Mari kita mulai dengan melakukan impor library yang akan digunakan seperti kode di bawah ini.

import tensorflow as tf
import numpy as np
from tensorflow import keras

Selanjutnya, kita harus mengubah angka-angka di atas menjadi sebuah larik/array agar dapat diterima oleh Keras sehingga proses pelatihan akan berjalan dengan baik. Kita dapat menggunakan kode berikut untuk mengubahnya menjadi sebuah larik/array.

X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)
Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0], dtype=float)

Kemudian, kita buat model neural network atau jaringan saraf tiruan (JST) dengan memanggil fungsi tf.keras.Sequential(). Sequential adalah model JST yang paling sederhana dan telah kita pelajari sebelumnya. Pada model sequential, setiap layer pada jaringan saraf tiruan terhubung secara sekuensial. 

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1, input_shape=[1])
    ])

Pada model sequential di atas, kita akan membuat sebuah model dengan jumlah layer yang dapat diatur menyesuaikan dengan kebutuhan dan data yang digunakan. Untuk membuat sebuah layer, kita dapat menggunakan fungsi keras.layers.Dense(). Sekadar informasi, jumlah layer yang Anda buat nantinya perlu melewati tahapan trial and error karena tidak ada petunjuk baku mengenai penggunaan layer beserta parameternya.

Loh, apa itu parameter? Pada kode di atas, terdapat parameter units dan input_shape, mari kita bahas kedua parameter tersebut.

- Parameter units dari fungsi keras.layers.Dense() adalah jumlah perceptron yang dimiliki oleh layer tersebut. Yang perlu diperhatikan pada model sequential adalah layer pertama dari model tersebut haruslah memiliki parameter input_shape agar model bisa mengenali bentuk input yang akan diprosesnya.
- Parameter input_shape menunjukkan bentuk dari setiap elemen input yang akan diterima oleh model. Pada kasus yang kita alami, setiap elemen dari data adalah sebuah bilangan numerik 1 digit sehingga input_shape dapat diisi dengan angka 1. Jika sebuah elemen dari dataset kita berupa gambar yang memiliki dimensi 32*32 piksel, input_shape yang sesuai adalah [32,32].


Kemudian, hal yang paling penting selanjutnya adalah menentukan optimizer dan loss dari model agar model kita bisa melakukan pelatihan. Untuk menentukan optimizer dan loss, kita menggunakan fungsi compile, sedangkan untuk regresi yang sederhana, kita dapat menggunakan stochastic gradient descent sebagai optimizer, dan mean squared error sebagai loss function model.

Jadi rumusnya compile:

1. Klasifikasi
- Adam
- accuracy, cross entropy loss

2. Regresi
- stochastic gradient descent
- mse,

Istilah "stochastic" dalam SGD merujuk pada sifat acak dari algoritma tersebut. Kehadiran ketidakpastian ini disebabkan oleh penggunaan mini-batch. Dengan melatih subset data yang berbeda pada setiap iterasi, langkah-langkah SGD melalui perhitungan komputasi menjadi tidak terduga, dengan menambahkan unsur keacakan sehingga membantu model keluar dari loss minimum lokal. Karakteristik ini membuat SGD sangat efektif dalam mengoptimalkan fungsi yang tidak cembung dan memiliki dimensi tinggi yang sering ditemui dalam domain pembelajaran mesin. Fungsi akhir dari optimizers ini untuk menemukan nilai loss terkecil secara keseluruhan dari proses pelatihan yang dijalankan sebanyak x iterasi.

Setelah kita mengetahui salah satu optimizers SGD, tidak afdal jika kita tidak mempelajari mean square error pada loss function agar semakin lihai dalam pengembangan jaringan saraf tiruan.

Fungsi kerugian (loss function) adalah sebuah metrik yang digunakan dalam machine learning untuk mengevaluasi seberapa baik model memprediksi target pada data pelatihan. Fungsi ini mengukur seberapa jauh prediksi model dari nilai sebenarnya yang diharapkan. Tujuan utama dari fungsi kerugian adalah untuk memberikan umpan balik kepada model tentang seberapa baik atau buruk performanya sehingga model dapat disesuaikan selama proses pelatihan untuk meminimalkan kesalahan prediksi.

Fungsi kerugian umumnya disesuaikan berdasarkan jenis masalah yang sedang dihadapi. Beberapa contoh fungsi kerugian termasuk mean squared error (MSE) untuk masalah regresi, Cross-Entropy Loss (atau Log Loss) untuk klasifikasi biner, dan Categorical Cross-Entropy untuk klasifikasi multi kelas. Pilihan fungsi kerugian yang tepat sangat penting karena dapat memengaruhi performa dan konvergensi model.

Pada kasus regresi yang sedang dihadapi, kita dapat menggunakan fungsi mean square error sebagai loss functionnya. Mean squared error (MSE) adalah salah satu metrik yang umum digunakan dalam kasus regresi untuk mengevaluasi seberapa baik model memprediksi nilai kontinu. MSE mengukur rata-rata dari kuadrat selisih antara prediksi model dengan nilai sebenarnya yang diharapkan. Semakin kecil nilai MSE, semakin baik model dalam memprediksi data. Secara matematis, MSE dapat dihitung dengan rumus berikut.

Untuk mengimplementasikan optimizers dan loss function menggunakan TensorFlow, Anda hanya memerlukan sebuah fungsi sederhana yaitu model.compile(). Selain itu, Anda juga perlu memasukkan metode yang akan digunakan, seperti contoh pada kasus ini adalah SGD dan MSE

model.compile(optimizer='sgd', loss='mean_squared_error')

Last but not least, setelah perjalanan panjang, akhirnya kita perlu memanggil sebuah fungsi yang paling terkenal dari machine learning yaitu fit(). Fungsi fit() memungkinkan kita menyuruh model untuk mempelajari hubungan antara atribut dan label pada dataset. 

Selain atribut dan label, parameter lain yang diperlukan sebuah model Keras pada fit adalah epochs. Epochs adalah berapa kali sebuah model neural network harus belajar memperbaiki akurasinya. Kita dapat menggunakan fungsi tersebut dengan kode di bawah ini.

model.fit(X, y, epochs=520)

Jumlah epochs di atas dapat disesuaikan dengan kebutuhan dan kasus yang kita miliki. Semakin banyak epochs, belum tentu menghasilkan loss lebih kecil. Pada beberapa kasus, jumlah epochs yang besar bisa menyebabkan loss tidak mengalami perubahan (stagnan) atau bahkan menyebabkan loss naik kembali. Tentunya, Anda perlu melakukan trial and error untuk memperhatikan loss yang didapatkan ketika melakukan pelatihan. 

Setelah menjalankan fungsi tersebut, dapat Anda lihat bahwa pada setiap epoch yang baru akan memiliki nilai error yang semakin menurun pada kasus regresi ini.

Ketika model telah dilatih, selanjutnya kita dapat menggunakan model tersebut untuk memprediksi data yang belum pernah dilihatnya menggunakan fungsi predict. Hal ini merupakan salah satu tahap paling penting ketika kita membangun sebuah model machine learning. Karena sejatinya, machine learning dibangun untuk melakukan tugas yang telah kita tentukan sebelumnya.

Seperti pada gambar di atas, kita akan melakukan prediksi untuk menghasilkan sebuah nilai sehingga dapat menghitung kualitas model yang telah kita bangun. Anda dapat melakukan prediksi dengan menggunakan kode berikut.

model.predict([4, 5])
Ketika kita menjalankan kode di atas, hasil yang kita dapat seharusnya adalah 13 dan 14 di mana f(4) = 4 + 9 dan f(5) = 5 + 9. Namun, ketika kita menjalankan kode tersebut, output yang dihasilkan adalah sebagai berikut.

1/1 [==============================] - 0s 40ms/step
array([[12.999903],
       [13.999887]], dtype=float32)
Jika Anda amati secara saksama, nilai yang dihasilkan ketika menjalankan fungsi prediksi hanya mendekati angka 13 dan 14 tidak 100% presisi. Kenapa demikian? Karena neural  network tidak memprediksi kepastian melainkan menghitung probabilitas yang terjadi. Pada kasus kita, neural network mempelajari bahwa pola yang terdapat pada dataset kemungkinan adalah x + 9 sehingga prediksi yang dihasilkan adalah probabilitas yang mendekati angka 13 dan 14. Semakin banyak data yang dilatih maka tingkat error training juga semakin rendah sehingga prediksi dari neural network akan semakin mendekati 13 dan 14.

Karena kita memiliki nilai loss yang sangat kecil, Anda dapat menggunakan fungsi .round() agar mengembalikan nilai dengan lebih presisi. Namun, tentunya ini akan menjadi resiko ketika kita memiliki loss function yang besar karena akan menjadi noise pada hasil prediksi yang kita lakukan sehingga hasilnya akan mengembalikan nilai sesuai dengan input yang sebelumnya kita latih.


model.predict([4, 5]).round()

Output
1/1 [==============================] - 0s 41ms/step
array([[13.],
       [14.]], dtype=float32)




Model Sekuensial dengan Beberapa Layer:
Dengan menghadapi kasus yang ada di industri, sangat minim kemungkinannya kita dapat menyelesaikan masalah tersebut hanya dengan satu layer. Sebetulnya, tidak ada panduan khusus untuk menentukan jumlah layer beserta parameter yang digunakan ketika membangun sebuah model neural network. Sekali lagi, sebagai seorang praktisi machine learning, Anda perlu melakukan trial and error untuk meningkatkan intuisi sehingga kelak tidak perlu lagi melakukan ribuan percobaan pada kasus serupa.

Sekarang mari kita bayangkan jika Anda bekerja di sebuah instansi kesehatan dan perlu membangun sebuah model yang dapat melakukan prediksi tumor otak. Pada kasus ini, kita memiliki 15.000 data gambar yang perlu kita latih agar dapat melakukan klasifikasi kondisi kesehatan otak. Tentunya, semakin banyak data yang kita miliki, semakin baik model yang akan dibangun

Jika model yang kita buat berdasarkan data kompleks seperti di atas hanya memiliki satu layer, alih-alih akurasi mendekati 99%, justru kesalahan yang kita dapatkan yang akan mendekati 99%. Di sinilah kita harus paham cara menggunakan multiple layer ketika mengembangkan model neural network. Dengan menggunakan jumlah layer yang lebih banyak, komputasi yang dilakukan oleh komputer akan lebih kompleks, tetapi sebanding dengan hasil yang didapatkan. Secara teori, semakin banyak layer yang digunakan, semakin detail juga perhitungan yang dilakukan oleh komputer.

Agar lebih memahami perbedaan jumlah layer tersebut, mari kita lihat visualisasi yang dapat merepresentasikan komputasi yang harus dilakukan oleh komputer dengan jumlah layer lebih dari satu.

Seperti yang Anda lihat pada gambar di atas, tentunya perhitungan matematis yang dilakukan oleh komputer akan lebih banyak. Hal ini menyebabkan proses pelatihan lebih lama, tetapi model yang kita bangun akan memiliki performa yang lebih baik. Ada satu hal yang perlu Anda ketahui, jumlah hidden layer pada pembangunan model neural network di atas bukanlah acuan pasti yang harus Anda ikuti. Anda dapat mengurangi dan menambahkan layer sesuai dengan kebutuhan dan performa dari model neural network yang dibangun.

Untungnya, sangatlah mudah untuk menambahkan layer pada model karena kita cukup menambahkan fungsi Dense sesuai jumlah layer yang diinginkan. Selanjutnya, mari kita tarik kembali contoh pembuatan model neural network pada materi sebelumnya. Kita akan memodifikasi layer tersebut dengan jumlah yang lebih banyak. 

Pada kode di bawah, kita memanggil fungsi dense sebanyak empat kali yang menunjukkan bahwa model kita memiliki empat buah layer. Layer pertama hanya memiliki 20 buah perceptron, layer kedua memiliki 15 buah perceptron, layer ketiga memiliki tiga buah perceptron, dan layer terakhir memiliki satu buah perceptron. Ingat, layer pertama dari model sequential harus memiliki parameter input_shape agar model bisa mengenali bentuk input yang akan diterimanya.

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=20, input_shape=[1]),
    tf.keras.layers.Dense(units=15),
    tf.keras.layers.Dense(units=10),
    tf.keras.layers.Dense(units=1)
    ])



Membuat Model Klasifikasi Dua Kelas:
Namun, rasanya masih ada satu hal yang kurang agar pengalaman belajar Anda lebih maksimal, yakni membuat model neural network dari awal hingga dapat melakukan inference.

Pada modul sebelumnya, kita telah membahas proses pengembangan model neural network untuk mengatasi permasalahan regresi sederhana. Selanjutnya, sudah waktunya kita akan lanjut mengembangkan model neural network untuk melakukan klasifikasi biner.

Mari kita mulai dengan sebuah permasalahan sederhana, bagaimana cara komputer mengklasifikasikan buah jeruk dan anggur? Tentunya komputer tidak memiliki kemampuan untuk melakukan tugas tersebut tanpa adanya proses pelatihan. Oleh karena itu, yuk, kita buat sebuah sistem yang dapat melakukan tugas tersebut.

Bayangkan ketika Anda memiliki jumlah label yang sangat banyak mungkin 30 label, 50 label, dan seterusnya. Hal tersebut akan memakan banyak tenaga dan waktu hanya untuk mengubah label menjadi data numerik, bukan? Oleh karena itu, kita dapat menggunakan opsi lainnya yaitu menggunakan fungsi LabelEncoder yang disediakan oleh sklearn. Fungsi ini memiliki kemampuan yang sama yaitu untuk mengubah data kategori menjadi numerik berdasarkan urutan abjad. 

from sklearn import preprocessing
 
label_encoder = preprocessing.LabelEncoder() 
df['name'] = label_encoder.fit_transform(df['name']) 
print(df)

Kini dataset di atas sudah memiliki enam buah kolom yang bertipe numerik. Seru, ya? Sayangnya sampai pada tahap ini, model kita belum dapat memproses dataset ini karena dataset masih dalam bentuk dataframe. Betul, dataset harus dalam bentuk array agar dapat diproses oleh model. Nah untungnya kita dapat melakukan ini dengan mudah menggunakan atribut values dari dataframe. Values mengembalikan numpy array yang dikonversi dari dataframe.

X = dataset[:,1:6]


artinya X atau atribut independen merupakan semua baris namun pada kolom indeks ke-1 sampai ke-6.

Sampai di sini, sebetulnya Anda dapat melanjutkan ke tahap selanjutnya, yaitu pembagian dataset menjadi data pelatihan dan data testing. Namun, dataset yang kita gunakan masih memiliki rentang yang cukup besar pada masing-masing kolomnya. Dalam hal ini, fitur dengan rentang nilai yang lebih besar dapat mendominasi perhitungan sehingga normalisasi diperlukan untuk menyeimbangkan pengaruh setiap fitur dalam model. 

Salah satu cara yang sangat penting dilakukan agar model neural network dapat mempelajari dataset dengan baik adalah melakukan normalisasi. Kita dapat menggunakan fungsi fit_transform() dari sebuah objek MinMaxScaler dari library preprocessing SKLearn untuk menormalisasi data kita. Hal ini akan mengubah seluruh data yang dipanggil memiliki rentang 0-1 atau -1 sampai 1, Anda dapat melakukan normalisasi dengan menjalankan perintah berikut.


from keras.models import Sequential
from keras.layers import Dense

Masih ingatkan struktur membangun model neural network yang sudah kita pelajari bersama pada modul sebelumnya? Untuk model yang kita kembangkan adalah model sequential yang memiliki 3 buah layer seperti di bawah. Activation function pada 2 layer pertama yang dapat digunakan adalah relu untuk latihan ini. Anda dapat bereksplorasi menggunakan activation function lain. Untuk layer terakhir, isi parameter unit isi dengan 1 di mana output dari model neural network kita merupakan satu buah bilangan numerik. Activation function pada layer terakhir dipilih sigmoid karena sigmoid memetakan probabilitas dari 0 sampai 1. Sigmoid sangat cocok digunakan pada masalah klasifikasi biner.

model = Sequential([    
                    Dense(32, activation='relu', input_shape=(5,)),    
                    Dense(32, activation='relu'),    
                    Dense(1, activation='sigmoid')])

Kemudian setelah arsitektur dari model neural network sudah dibentuk, seperti biasa kita perlu menentukan optimizer dan loss function untuk menyelesaikan model ini. Untuk optimizer, kita akan menggunakan stochastic gradient descent (SGD) yang merupakan optimizer yang sangat umum dan cocok digunakan pada dataset yang berukuran kecil. Lalu, untuk loss yang sesuai adalah ‘binary_crossentropy’ karena masalah pada latihan kita kali ini adalah 2 kelas (biner). Selain itu, jika kita ingin menampilkan akurasi pada setiap proses pelatihan model, kita dapat menambahkan parameter metrics dan mengisinya dengan string ‘accuracy’.

Untuk melihat performa model pada data baru, Anda perlu melakukan benchmark supaya dapat melihat performa model apakah overfit, underfit, atau goodfit. Masih ingat pada tahap pembagian dataset? Kita membagi 30% data menjadi data testing yang berguna untuk melakukan evaluasi. Untuk melihat loss dan akurasi model pada data test, gunakan fungsi evaluate pada model. Fungsi Evaluate mengembalikan 2 nilai. Yang pertama adalah nilai loss, dan yang kedua adalah nilai akurasinya. 


model.evaluate(X_test, Y_test, batch_size=1)




Membuat dan Melatih Model untuk Klasifikasi Banyak Kelas (Multi Kelas):

Iris Classification

Masih ingatkan kalau sebuah model neural network tidak bisa memproses string sebagai kategori? Hal tersebut menyebabkan kita harus mengubah nilai pada kolom Species menjadi numerik terlebih dahulu agar bisa diproses oleh model neural network. Sebelumnya kita sudah mempelajari penggunaan LabelEncoder dari SKLearn dan transformasi secara manual, tentunya sangat membosankan jika menggunakan metode yang sama terus, bukan? 

Karena pada kasus ini label kita merupakan data kategorikal mari kita pelajari salah satu metode transformasi lainnya, yaitu one hot encoding menggunakan fungsi get_dummies(). Fungsi ini memungkinkan mengubah setiap variabel dalam menjadi angka sebanyak nilai yang berbeda. Kolom pada output masing-masing diberi nama sesuai dengan nilai. Jika inputnya berupa DataFrame, nama variabel asli akan ditambahkan ke nilai tersebut sebagai nama kolom. 

pd.get_dummies(df.Species, dtype=int)


Sesudah itu lakukan pembagian data menjadi data latih dan data uji. Tidak ada acuan baku untuk pembagian ukuran dataset ini. Tujuan dari pembagian dataset ini untuk membagi data menjadi dua bagian seperti namanya. Biasanya, data dipisahkan menjadi dua bagian, satu bagian digunakan untuk mengevaluasi atau menguji data dan bagian lainnya untuk melatih model. Namun, terdapat metode lain seperti membagi menjadi tiga bagian menjadi data latih, data uji, dan data validasi. Biasanya teknik ini disebut dengan cross validation. 

Untuk arsitektur model, kita kali ini menggunakan 3 buah layer. Activation function yang digunakan pada layer terakhir dipilih softmax karena activation tersebut umum dipakai untuk klasifikasi multi kelas seperti ini. Jika penasaran terkait softmax lebih detail, silakan membaca kembali pada modul sebelumnya, ya. 

model = Sequential([    
                    Dense(64, activation='relu', input_shape=(4,)),    
                    Dense(64, activation='relu'),    
                    Dense(3, activation='softmax')])

Lanjutkan dengan menentukan optimizer dan loss function dari model. Untuk masalah klasifikasi multi kelas, Anda dapat menggunakan loss ‘categorical_crossentropy’.

model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

categorical_crossentropy adalah fungsi kerugian (loss function) yang sering digunakan dalam tugas klasifikasi multikelas di mana label targetnya adalah dalam bentuk one-hot encoding. Fungsi ini umumnya digunakan dalam kasus di mana output dari model adalah distribusi probabilitas untuk setiap kelas. 

Tujuan dari categorical_crossentropy adalah untuk mengevaluasi seberapa baik model memprediksi distribusi probabilitas kelas yang benar. Dengan mengoptimalkan fungsi ini selama proses pelatihan menggunakan algoritma seperti Adam atau varian-modifikasi lainnya, model berusaha untuk membuat prediksi yang semakin mendekati distribusi probabilitas yang diharapkan.

Selain itu, categorical_crossentropy juga berguna karena memberikan umpan balik kepada model ketika ada perbedaan besar antara probabilitas prediksi dan probabilitas target, membantu model untuk mengoreksi diri dan meningkatkan performa klasifikasi multikelas.

Nah, pada latihan ini ada sedikit perubahan yang perlu kita pelajari lagi. Fungsi fit() sekarang kita tampung ke dalam objek hist (history). Untuk apa kita melakukan hal ini? Temukan jawabannya di materi berikutnya, ya.

hist = model.fit(X_train, Y_train, epochs=100)
Sampai di sini, Anda sudah berhasil membuat model untuk menyelesaikan kasus multi kelas. Seperti pada gambar di atas, akurasi yang kita dapatkan kurang lebih 0.98 (98%), tetapi ini dapat berbeda setiap kali melatih model seperti yang telah kita bahas sebelumnya. Jadi jangan risau, ya.

Terakhir, kita perlu menguji akurasi prediksi dari model yang telah dibangun menggunakan data uji yang belum pernah dilihat.

model.evaluate(X_test, Y_test, batch_size=1)
Hasil evaluasi yang kita lakukan ternyata mendapatkan akurasi yang dapat diandalkan karena mencapai 0.95 (95%) yang berarti kita berhasil memprediksi 43 dari 45 data dengan benar. Wow, angka yang sangat baik, bukan?




Plot Loss dan Akurasi dari Trained Model:
Di materi sebelumnya, Anda pasti penasaran dengan alasan menampung fungsi fit pada sebuah objek history. Jawabannya adalah karena kita dapat membuat plot dari akurasi dan loss model pada saat proses pelatihan. 

Jika Anda ingat pada latihan sebelumnya, kita perlu melihat performa model yang dibangun melalui teks yang memiliki banyak sekali parameter. Hal itu akan membebankan otak kita dengan banyak informasi karena harus membandingkan performa pada setiap epochs. Sangat melelahkan, bukan? Sebagai manusia, pikiran kita selalu mencari hal yang lebih mudah untuk dirangkai. Oleh karena itu, gambar merupakan media visual yang tepat karena sebuah gambar akan memiliki nilai setara dengan ribuan kata. 

Stimulus visual memiliki peran yang cukup penting dalam mengumpulkan informasi dengan lebih baik. Kemampuan yang dapat mempertahankan informasi ini disebut dengan kognisi visual. Dengan ini, pikiran kita akan lebih mudah mendapatkan informasi dari sebuah gambar.

Nah, dengan menggunakan plot akan sangat berguna untuk melihat proses keseluruhan pembelajaran model dengan lebih mudah. Alih-alih melihat setiap iterasi dan membandingkan secara manual, kita dapat membuat sebuah visualisasi dari performa model yang sudah dilatih. 

Untuk melihat bagaimana plot bekerja mari kita kerjakan latihan berikut. Kita akan menggunakan dataset dan model yang sama dengan latihan sebelumnya. Anda dapat menggunakan latihan sebelumnya dan menambahkan kode-kode berikut di bawah latihan sebelumnya.

import matplotlib.pyplot as plt
Matplotlib.pyplot merupakan sebuah modul dalam library Matplotlib yang menyediakan antarmuka untuk membuat visualisasi data dalam bentuk grafik atau plot. Pada latihan ini kita akan membuat sebuah plot dari objek history. Objek history menampung informasi dari akurasi dan loss model pada setiap epoch di proses pelatihan dengan tipe data dictionary sehingga kita bisa membuat plot akurasi atau loss. 

Masih ingatkan cara memanggil key pada dictionary? Untuk mengakses loss kita bisa memanggil fungsi history pada objek history dan menentukan metrik loss yang akan diambil.

plt.plot(hist.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

Dari hasil plot di atas, Anda dapat melihat bahwa akurasi model stagnan pada epoch di sekitar 20 dan juga kembali stagnan pada epoch di atas 80. Dengan menyimpan informasi pada tahap pelatihan model, kita dapat melihat performa model dengan jauh lebih nyaman bukan? Mari bandingkan jika kita tidak menggunakan plot sebagai visualisasi performa model 



Mencegah Overfitting dengan Dropout dan Batch Normalization

Seperti yang telah dipelajari pada materi sebelumnya, masalah umum yang dihadapi model machine learning adalah overfitting. Masih ingat bukan tentang overfitting? Yup, overfitting adalah situasi ketika sebuah model memiliki performa yang bagus saat mengenali data latih, tetapi buruk saat mengenali data-data baru yang belum pernah ditemuinya.

Overfitting pada machine learning dapat terjadi ketika data pelatihan yang kita miliki terlalu sedikit dan tidak merepresentasikan semua kemungkinan dari nilai sesungguhnya. Kondisi lainnya adalah ketika kita memiliki data pelatihan yang sangat banyak, tetapi memiliki informasi yang tidak relevan dengan data di lapangan. Hal ini menyebabkan model harus mempelajari data yang kompleks tetapi tidak memiliki pola yang berguna.

Penyebab di atas juga berlaku pada jaringan saraf tiruan. Terus, apa solusinya dan bagaimana kita mengetahui kualitas dari model neural network? Tenang saja, Anda sudah berada di jalur yang tepat. Pada latihan sebelumnya, kita sudah memisahkan data latih dan data uji sehingga kualitas model melalui evaluasi yang sudah dilakukan dapat terlihat. Sebagai contoh perhatikan tabel berikut.

Model C adalah contoh kasus overfitting yang jelas, yaitu dengan akurasi pelatihan tinggi, tetapi akurasi pengujian rendah. Penting untuk mempertimbangkan pengetahuan tentang domain masalah dan karakteristik data untuk menilai tingkat overfitting yang dapat diterima. Sampai di sini cukup terbayang ‘kan untuk menentukan model kita overfitting atau tidak?

Sekarang mari kita bahas beberapa cara untuk mencegah overfitting pada model neural network. 


Dropout:
Salah satu cara mencegah overfitting adalah dengan menggunakan dropout. Dropout adalah fungsi standar yang umum digunakan industri untuk mencegah overfitting. Seperti yang kita ketahui, semakin kompleks sebuah model machine learning, semakin tinggi kemungkinan model tersebut mengalami overfitting. Dropout bekerja dengan cara mengurangi kompleksitas model neural network tanpa mengubah arsitektur model tersebut. 

Lalu, bagaimana cara dropout bekerja? Nama dropout mengacu pada unit/perceptron yang di-dropout (dibuang) secara temporer pada sebuah layer. Contohnya seperti gambar di bawah ini di mana besaran dropout yang dipilih adalah 0.5 sehingga 50% dari perceptron hidden layer kedua dimatikan secara berkala pada saat pelatihan. Dropout memilih neuron yang akan "dimatikan" secara acak selama setiap iterasi pelatihan.

Seperti yang Anda tahu, weight dari setiap neuron pada sebuah layer itu bersifat statis. Hal ini menyebabkan jaringan saraf terlalu menyesuaikan dengan karakteristik pada data latih sehingga menyebabkan overfitting. Nah, penerapan dropout akan membantu mengatasi permasalahan overfitting yang disebabkan oleh permasalahan tersebut.

Untuk mengimplementasikan dropout sendiri sangatlah mudah jika menggunakan Keras. Anda cukup menambahkan layer dropout pada hidden layer di model yang sedang dibuat. Agar lebih jelas perhatikan contoh berikut.

from keras.layers import Dense, Dropout
model = Sequential([
                    Dense(64, activation='relu', input_shape=(4,)),
                    Dense(64, activation='relu'),
                    Dropout(0,5),
                    Dense(3, activation='softmax')])

Untuk menggunakan dropout, kita cukup menambahkan fungsi layer tf.keras.layers.Dropout() dan mengisi parameter berupa persentase yang kita inginkan seperti di atas. Dropout akan otomatis menghilangkan jumlah neuron yang aktif pada layer sebelumnya. Sangat mudah, bukan


Batch Normalization:

BatchNormalization adalah layer yang digunakan dalam jaringan saraf tiruan (neural network) untuk mempercepat konvergensi pelatihan dan mengurangi sensitivitas terhadap inisialisasi parameter. Layer ini bekerja dengan melakukan normalisasi input dari layer sebelumnya pada setiap batch sehingga memungkinkan pelatihan yang lebih stabil dan cepat. 

Proses normalisasi dalam BatchNormalization dilakukan dengan mengurangi nilai rata-rata dari setiap batch dan mengukurnya nilai varian. Kemudian layer ini akan melakukan normalisasi pada data dan melakukan pergeseran selama pelatihan. Hal ini membantu dalam mengatasi masalah seperti gradien menghilang (vanishing gradient) dan memungkinkan mempercepat konvergensi pada model jaringan saraf.

Penggunaan BatchNormalization umum dalam pembangunan model jaringan saraf modern karena kemampuannya untuk meningkatkan stabilitas dan kinerja pembelajaran.

Untuk mengimplementasikan BatchNormalization sendiri sangatlah dengan menggunakan Keras. Anda cukup menambahkan layer BatchNormalization pada model neural network. Agar lebih jelas lihatlah contoh di bawah.

from keras.layers import Dense, Dropout, BatchNormalization
model = Sequential([
                    Dense(64, activation='relu', input_shape=(4,)),
                    Dense(64, activation='relu'),
                    Dropout(0,5),
                    BatchNormalization(momentum=0.99),
                    Dense(3, activation='softmax')])

Pada contoh di atas, kita menggunakan salah satu parameter yaitu momentum. Momentum pada BatchNormalization mengacu pada parameter yang dapat mengontrol perhitungan nilai rata-rata dan varians yang dihitung pada setiap batch selama proses pelatihan model. Dengan menggunakan momentum, nilai rata-rata dan varians dihitung berdasarkan gabungan dari nilai-nilai rata-rata dan varians dari mini-batch saat ini dan nilai-nilai sebelumnya yang telah disesuaikan.

Ketika melakukan normalisasi, kita tidak hanya menggunakan nilai dari satu batch saja, tetapi juga mempertimbangkan nilai beberapa batch sebelumnya untuk mendapatkan estimasi yang lebih stabil dari rata-rata dan varians. Ini membantu mengurangi fluktuasi yang mungkin terjadi dalam statistik batch dan meningkatkan konsistensi normalisasi.

Nilai momentum ini sering kali disetel di antara 0,1 dan 0,999. Nilai yang lebih tinggi menunjukkan bahwa kita lebih bergantung pada nilai sebelumnya daripada nilai dari batch saat ini. Sebagai contoh, jika momentum diatur ke 0,9, 90% informasi dari batch sebelumnya akan digunakan dan hanya 10% dari statistik batch saat ini yang akan dipertimbangkan.

Penggunaan momentum pada BatchNormalization membantu menghasilkan data yang lebih stabil dan dapat meningkatkan kinerja pelatihan pada model jaringan saraf tiruan. 

Meskipun BatchNormalization dapat membantu mengurangi overfitting, ini bukan jaminan bahwa model tidak akan mengalami overfitting. Overfitting masih dapat terjadi jika model terlalu kompleks untuk jumlah data pelatihan yang sedikit. Selain itu, ada kemungkinan juga terdapat banyak noise pada data, atau jika ada masalah lainnya dengan proses pelatihan. 

Sebenarnya, masih ada banyak parameter yang dapat kita atur pada BatchNormalization, tetapi hal yang paling penting terdapat pada momentum. Jika Anda masih penasaran dengan parameter lainnya, silakan baca pada tautan berikut: BatchNormalization.



Optimasi Pelatihan Menggunakan Callback:
Pada latihan sebelumnya, pelatihan model berlangsung sebanyak jumlah epoch yang sudah ditentukan. Jika kita amati, epoch terakhir memperlihatkan nilai akurasi dari model tidak meningkat lagi. Contoh kasus lainnya, bayangkan kita menentukan epoch sebanyak 100 dan ternyata pada epoch ke-20, performa dari model sudah memenuhi target. Jika itu terjadi, tentunya kita harus menghentikan proses pelatihan agar tidak membuang-buang waktu, bukan?

Untungnya, kita dapat memberi tahu model untuk berhenti ketika telah mencapai metrik tertentu sehingga proses pelatihan model menjadi lebih singkat. Bayangkan ketika waktu untuk eksekusi 1 epoch sebesar 6 detik. Untuk mengeksekusi 100 epoch berarti membutuhkan waktu selama 600 detik. Jika model telah mencapai target akurasi yang kita inginkan misalnya pada epoch ke-30, dan model otomatis berhenti melakukan pelatihan, kita bisa menghemat waktu eksekusi 70 epoch yaitu selama 420 detik. Wow!

Kasus yang lebih ekstrem terjadi jika Anda membangun sebuah model neural network dengan banyak layer dengan waktu eksekusi 1 epochs sebesar 600 detik atau 10 menit tentunya Anda tidak ingin menunggu selama 60.000 detik atau setara dengan 1000 menit, ‘kan? Anda dapat menghemat cukup banyak waktu dengan hanya menggunakan callbacks. Tentu hal ini menjadi sebuah solusi yang sangat canggih, bukan?
Fungsi callbacks membantu kita untuk memberi tahu model agar berhenti melakukan pelatihan ketika sudah mencapai target tertentu serta mencatat dan melakukan utilitas lain seperti menyimpan performa terbaik model. Nah, langsung saja kita praktik menggunakan callbacks. Kita masih menggunakan dataset dan model dari latihan klasifikasi iris pada materi sebelumnya. Kode utuhnya seperti di bawah ini.

Setelah semuanya siap, kita perlu membuat sebuah kelas bernama myCallback(). Perhatikan baik-baik. Parameter pertama pada kelas callbacks di atas harus memiliki sifat inherit tf.keras.callbacks.Callback. Selanjutnya, kita buat fungsi yang paling penting yaitu on_epoch_end(). Fungsi inilah yang akan berperan untuk memberi tahu model agar berhenti melakukan pelatihan ketika telah mencapai target.

Kode if(logs.get(‘accuracy’)>0.9) sangat intuitif menunjukkan kalau kode itu memiliki arti “jika akurasi lebih besar dari 0.9, maka eksekusi perintah berikutnya”. Kita juga dapat menulis kode print(“\nAkurasi telah mencapai >90%!”) untuk ditampilkan ketika callbacks aktif.

Kode self.model.stop_training = True adalah kode yang memberi tahu model untuk menghentikan pelatihan. Setelah kelas ini dibuat, buatlah objek dari kelas tersebut. Berikut kode lengkap untuk membuat callbacks.

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

Setelah membuat sebuah objek callbacks, selanjutnya kita harus menambahkan hyperparameter tersebut pada saat melakukan proses pelatihan. Untuk menggunakan callbacks sangat gampang, kita hanya perlu menambahkan parameter callbacks dan mengisi objek callbacks yang telah kita buat pada parameter tersebut.

hist = model.fit(X_train, Y_train, epochs=100, callbacks = [callbacks])

Dari latihan di atas dapat dilihat model berhasil menghentikan pelatihan ketika telah mencapai akurasi yang kita tentukan pada epoch ke 5 dari 100. Kita telah menghemat waktu eksekusi sebanyak 96 epoch. Keren, bukan?

Dengan waktu pelatihan yang lebih cepat, tentunya Anda dapat melakukan eksplorasi jauh lebih banyak. Selain menggunakan callbacks, sebenarnya masih banyak fungsi yang sangat berguna ketika kita melakukan pelatihan, salah satu contoh lainnya adalah EarlyStopping. 



Menggunakan Dataset dari Tensorflow:
Ada dua cara untuk mengakses dataset yang sudah disediakan TensorFlow yaitu melalui tf.data.Datasets dan juga TensorFlow Datasets. Keduanya memiliki fungsi yang sangat mudah digunakan tetapi memiliki variasi data yang berbeda. Sudah tidak sabar, ya? Yuk, kita bahas cara penggunaan keduanya bersama-sama.

Membuat Model Menggunakan tf.data.Datasets
As usual, hal pertama yang kita lakukan adalah melakukan impor library TensorFlow agar dapat mengakses seluruh fungsi yang ada.

import tensorflow as tf
Kemudian kita buat objek untuk menampung dataset kita dan masukkan fungsi tf.keras.datasets.<nama_dataset> pada objek tersebut. Karena pada kasus ini menggunakan dataset mnist, kode yang digunakan sebagai berikut.

mnist = tf.keras.datasets.mnist
Selanjutnya, kita perlu membagi dataset yang sudah ada menjadi dua bagian yaitu pelatihan dan uji menggunakan fungsi load_data(). Fungsi load_data() dari objek dataset mengembalikan bentuk dataset yang telah dibagi menjadi atribut latih, label latih, atribut uji, dan label uji. Sangat gampang, bukan? Hanya dengan beberapa baris kode kita sudah memiliki data latih dan data uji yang siap dipakai.

(gambar_latih, label_latih), (gambar_testing, label_testing) = mnist.load_data()

Kita bisa menampilkan label dan salah satu gambar dari data latih menggunakan library matplotlib untuk memastikan data yang kita pilih sudah sesuai dan siap pakai.

import numpy as np
np.set_printoptions(linewidth=200)
import matplotlib.pyplot as plt
plt.imshow(gambar_latih[0])
print(label_latih[0])

Namun, dataset yang kita miliki belum dinormalisasi sehingga kita harus melakukannya secara manual. Kode di bawah berfungsi untuk membagi setiap piksel pada gambar sebesar 255 karena nilai sebuah piksel berkisar dari 0 sampai 255. 

gambar_latih  = gambar_latih / 255.0
gambar_testing = gambar_testing / 255.0
Dalam konteks pemrosesan citra atau pengenalan pola, normalisasi gambar sangat penting untuk memastikan bahwa nilai piksel berada dalam rentang yang dapat diolah dengan baik oleh model. Dalam hal ini, kode tersebut membagi nilai piksel dari gambar pelatihan (gambar_latih) dengan 255.0.

Karena nilai piksel pada umumnya dalam rentang 0 hingga 255, pembagian dengan nilai 255.0 akan menghasilkan nilai piksel yang lebih kecil yaitu dalam rentang 0 hingga 1. Ini membantu model pembelajaran mesin untuk belajar lebih efisien karena nilai yang lebih kecil cenderung memiliki efek yang lebih stabil selama proses pembelajaran.

Selanjutnya, kita akan menggunakan 3 layer sebagai arsitektur model ini. Untuk layer pertama, kita menggunakan layer khusus yaitu Flatten yang berfungsi untuk mengubah input kita yang berupa matriks 2 dimensi menjadi array 1 dimensi. 

Untuk melakukan hal tersebut tentu sangatlah mudah jika menggunakan TensorFlow, kita hanya perlu memanggil layer Flatten dan mengisi parameter seperti biasanya.

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

Terakhir, tentukan optimizer dan loss untuk model, lalu lakukan pelatihan pada model.

model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(gambar_latih, label_latih, epochs=5)

 

Membuat Model Menggunakan TensorFlow Dataset:
TensorFlow menyediakan kumpulan dataset siap pakai untuk dilatih dengan TensorFlow, Jax, dan framework machine learning lainnya. TensorFlow Dataset (TFDS) bertugas untuk melakukan pengunduhan dan menyiapkan data secara deterministik dengan menggunakan tf.data.Dataset (atau np.array).

Sebelumnya, kita sudah mempelajari tf.data.Datasets, sekarang kita mengenal rivalnya yaitu TFDS. Namanya, cukup mirip, ya? Jangan bingung antara TFDS (library ini) dengan tf.data.Datasets (API dari TensorFlow). 

TFDS adalah wrapper tingkat tinggi di sekitar tf.data. Jika Anda tidak terbiasa dengan API ini, kami menyarankan Anda untuk membaca panduan resmi dari TensorFlow pada tautan berikut: tf.data.

Di sisi lain, TFDS juga menyediakan lebih banyak datasets yang siap digunakan. Tanpa berlama-lama, mari kita mulai perjalanan ini dengan melakukan impor library yang akan digunakan.

import tensorflow as tf
import tensorflow_datasets as tfds

Setelah berhasil menginstal library TFDS, kita dapat melihat list atau daftar dari dataset yang tersedia pada TFDS. Untuk melihat seluruh dataset yang tersedia, kita bisa menggunakan fungsi tfds.list_builders() atau dapat dilihat pada katalog TensorFlow Datasets.

tfds.list_builders()

(Datasetnya beneran banyak jir, sampe ada kdd untuk anomaly detection) 

 Seperti yang Anda lihat pada gambar di atas, ada banyak sekali dataset yang tersedia ketika kita menggunakan TFDS, baik itu untuk pembelajaran maupun eksplorasi mandiri. Setelah melihat banyak sekali dataset, tentunya kita harus menentukan dataset mana yang akan digunakan, bukan? Tenang saja, caranya sangat mudah. Kita hanya perlu menggunakan fungsi tfds.load() seperti berikut.

ds = tfds.load('mnist', split='train', shuffle_files=True)
assert isinstance(ds, tf.data.Dataset)

Kode di atas memuat dataset MNIST menggunakan TensorFlow Datasets. Mari kita jelaskan satu per satu detail dari kode di atas.

- Fungsi load() memungkinkan Anda untuk menggunakan dataset yang telah disediakan oleh TensorFlow Datasets. 
- Parameter pertama ('mnist') menunjukkan nama dataset yang ingin dimuat. 
- Parameter kedua ('split='train'') menunjukkan bahwa kita hanya memuat bagian pelatihan dari dataset MNIST. 
- Parameter ketiga ('shuffle_files=True') mengindikasikan bahwa file-file dataset akan diacak sebelum dimuat, sehingga urutan data tidak akan tetap sama setiap kali dataset dimuat ulang. 


Pada baris kedua mungkin ada sebuah sintaksis yang cukup asing bagi kita, yaitu assert isinstance. Wah, apa tuh bang? Fungsi tersebut adalah sebuah pernyataan asersi (assertion statement) yang memeriksa apakah variabel ds adalah sebuah objek tf.data.Dataset. Assertion ini berguna untuk memastikan bahwa data yang dimuat benar-benar dalam bentuk dataset TensorFlow yang dapat digunakan untuk melatih model.

Masih ingat dengan ketentuan membagi data latih dan data uji? Yup, dengan menjalankan kode di atas, kita belum membagi dataset menjadi proporsi yang dibutuhkan. Untuk melakukan itu menggunakan TFDS, kita dapat melakukannya dengan kode berikut.

(train_images, train_labels), (test_images, test_labels) = tfds.as_numpy(tfds.load('mnist',
              split = ['train', 'test'],
              batch_size=-1,
              as_supervised=True))

Selanjutnya, kita akan menggunakan berbagai macam layer sebagai arsitektur model ini. Untuk layer pertama, kita menggunakan layer khusus yaitu Conv2D() yang digunakan untuk mengekstraksi fitur dari gambar input menggunakan operasi konvolusi. Dilanjutkan oleh MaxPooling2D yang berguna untuk mereduksi dimensi spasial dari representasi gambar berdasarkan layer sebelumnya.

Setelah trial and error mengenai layer Convo2D dan MaxPooling2D, barulah kita menggunakan Flatten yang berfungsi untuk mengubah input kita yang berupa matriks 2 dimensi menjadi array 1 dimensi.

Hufttt, terlihat sangat rumit, ya? Sebenarnya tidak serumit yang Anda bayangkan kok. Dengan menggunakan TensorFlow, kita hanya perlu membangun arsitektur seperti pada latihan-latihan sebelumnya. Mari kita telisik kode di bawah ini. 

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64,activation="relu"),
    tf.keras.layers.Dense(128,activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax")
])

Seperti yang Anda lihat di atas, kita menggunakan beberapa layer Conv2D dan MaxPooling2D. Hal ini sangat berfungsi untuk melakukan perhitungan konvolusi yang lebih detail terhadap data pelatihan yang kita miliki. Hal yang harus diperhatikan ada pada layer pertama, yaitu input_shape. Yup, kita harus menyesuaikan ukuran gambar yang ada pada dataset dengan model yang kita bangun.

Selanjutnya, kita juga harus menggunakan layer Flatten seperti yang sudah kita bahas berkali-kali sebelumnya. Terakhir, pastikan units yang ditetapkan sesuai dengan jumlah kelas yang ada pada kasus Anda. Karena kasus ini merupakan multi kelas dengan 10 kelas, units yang dibutuhkan adalah 10 dengan activation softmax.

Sebelum melakukan pelatihan, kita harus menentukan tiga buah argumen untuk melakukan compile model yang telah dibangun. Tentunya Anda masih ingatkan terhadap optimizers, loss function, dan metrics? Benar, kita harus menentukan konfigurasi yang tepat untuk kasus yang dihadapi 

model.compile(
    optimizer=tf.keras.optimizers.RMSprop(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy'],
)

Setelah model.compile() dipanggil, model sudah siap untuk dilatih dengan memanggil fungsi model.fit() dengan data pelatihan yang sesuai.

model.fit(train_images, train_labels, batch_size=50, epochs=5)

Terakhir, untuk memastikan semuanya aman dan memiliki performa yang dapat diandalkan, Anda perlu melakukan evaluasi terhadap data uji menggunakan kode berikut. 

model.evaluate(test_images, test_labels, batch_size=1)

Yuhuu, Anda sudah dapat membuat sebuah model menggunakan TensorFlow Datasets. Sampai di sini, ada banyak sekali petualangan yang menunggu Anda. Jumlah datasets yang ada pada TFDS sangat banyak, belum lagi Anda juga dapat menggunakan open repository lainnya. Oleh karena itu jangan lupa untuk mengisi bahan bakar karena perjalanan yang sangat panjang akan dimulai dari titik ini. 



Penggunaan Batch Loading:
Ngomong-ngomong tentang hyperparameter, masih ingatkah apa itu hyperparameter? Mari kita sedikit flashback pada modul sebelumnya. Hyperparameter adalah parameter yang tidak ditentukan oleh model itu sendiri selama proses pembelajaran, tetapi harus ditentukan oleh developer atau pembuat model sebelum pelatihan dimulai. Mereka adalah variabel yang mengontrol proses pembelajaran dan arsitektur model, seperti kecepatan pembelajaran, jumlah layer dan neuron dalam jaringan saraf, ukuran batch, dan lain-lain.

Nah, Batch loading adalah proses pelatihan ketika neural network melakukan pembaruan parameternya (weight) setelah membaca sejumlah sampel data tertentu. Misal dataset kita berisi 800 buah gambar pizza. Tanpa batch size, proses pembaruan parameter terjadi untuk seluruh sampel pada dataset. Sehingga, ketika tanpa menggunakan batch size, pada 1 epoch terdapat 800 kali pembaruan weight. Ketika 1 ukuran batch adalah 32 buah gambar pizza, terdapat 25 buah batch pada dataset. Pada batch loading, model baru melakukan pembaruan parameter setelah membaca satu batch atau 32 buah gambar pizza. Sehingga, proses pembaruan parameter pada 1 epoch hanya sebanyak 25 kali.

Apakah Anda sudah menyadari apa fungsi dari batch loading? Yup benar, dengan batch loading proses pelatihan data akan menjadi lebih cepat. Kita akan melihat langsung kegunaan dari batch loading pada latihan berikut.

Untuk dataset, kita akan menggunakan dataset mnist dan model yang sama dengan materi sebelumnya.

Selanjutnya, buatlah fungsi fit() untuk melakukan pelatihan. Perlu Anda perhatikan, di sini kita mulai menggunakan batch loading. Untuk menggunakan batch loading, kita hanya menambahkan satu parameter ‘batch_size’ pada fungsi fit(). 

Tahukah Anda bahwa fungsi fit() secara default menggunakan batch loading dengan batch size sebesar 32. Sehingga, ketika kita tidak mendefinisikan nilai parameter batch_size, ukuran batch akan diisi sebesar 32 secara default. Perhatikan bahwa pada setiap epoch memakan waktu selama sekitar 3 atau 4 detik.

model.fit(training_images, training_labels, batch_size=32, epochs=5)

Agar lebih paham, sekarang kita akan menggunakan batch_size yang lebih besar yaitu 128. Dapat kita lihat bahwa semakin besar batch size, waktu eksekusi tiap epoch akan semakin cepat. 

model.fit(training_images, training_labels, batch_size=128, epochs=5)

Seperti yang Anda lihat di atas, waktu pelatihan yang dilakukan memakan waktu yang lebih singkat. Silakan Anda bereksperimen menggunakan batch size yang lain untuk pengalaman belajar yang lebih maksimal. Bagaimana hasilnya? Silakan beri kesimpulan sendiri, ya. 

Pada latihan ini, kita telah memahami bahwa dengan menggunakan batch loading dapat mempercepat pelatihan. Untuk pemilihan batch size sendiri tidak ada aturan bakunya, tetapi yang umum dipakai adalah 32, 64, dan 128. Anda harus bereksperimen sendiri guna menemukan batch size yang cocok dengan masalah atau model yang sedang dibangun. 


Apa fungsi dari fungsi aktivasi dalam setiap neuron dalam neural network?
d. Fungsi aktivasi mengonversi input dari neuron sebelumnya menjadi output yang diinginkan.

Apa perbedaan antara SGD (Stochastic Gradient Descent) dan Adam optimizer dalam konteks pelatihan neural network?
c. SGD menggunakan momentum untuk mempercepat konvergensi, sementara Adam memperhitungkan momentum dan adaptasi learning rate.

Apa fungsi dari callback Plot Loss dan Akurasi dalam TensorFlow/Keras?
b. Untuk memantau dan memvisualisasikan perubahan loss dan akurasi model selama pelatihan.

Apa tujuan dari Batch Normalization dalam pelatihan neural network?
c. Mempercepat konvergensi model dengan memperbarui parameter bobot lebih cepat. (salah)

Mengapa teknik dropout efektif dalam mengatasi masalah overfitting pada neural network?
c. Dropout secara acak menghapus sebagian unit (neuron) pada setiap iterasi pelatihan, memaksa jaringan untuk belajar fitur yang lebih umum.

Apa keuntungan menggunakan one-hot encoding dalam tugas klasifikasi multi kelas?
A. One-hot encoding mengubah label kelas menjadi vektor biner, memungkinkan model untuk memprediksi probabilitas setiap kelas secara independen.
B. One-hot encoding mempercepat proses pelatihan model dengan mengurangi kompleksitas data input.
C (salah). One-hot encoding menghasilkan representasi biner untuk setiap kelas, memudahkan interpretasi output model.
D. One-hot encoding mengatasi masalah kelas yang tidak seimbang dalam dataset.

Apa keuntungan dari batch loading dalam pelatihan neural network dibandingkan dengan memuat seluruh dataset sekaligus?
A (salah). Batch loading menghasilkan hasil yang lebih akurat karena memperbarui parameter model lebih sering.
B. Batch loading memungkinkan penggunaan dataset yang lebih besar tanpa memori yang besar.
C. Batch loading mempercepat waktu pelatihan karena hanya memuat sebagian kecil dari dataset pada setiap iterasi. (betul)
D. Batch loading mengurangi risiko overfitting karena model tidak melihat seluruh dataset pada saat yang sama.

Apa yang dimaksud dengan fungsi loss dalam konteks pelatihan neural network?
c. Fungsi loss adalah fungsi yang mengukur seberapa baik model memprediksi output yang benar.


Apa fungsi utama dari callbacks dalam pelatihan neural network menggunakan TensorFlow dan Keras?
a. Callbacks digunakan untuk memonitor metrik kinerja model dan mengambil tindakan berdasarkan perubahan metrik tersebut. 




---------------------------------------------------------------------------
                        Natural Language Processing
---------------------------------------------------------------------------

Natural language processing (NLP) adalah salah satu cabang ilmu komputer yang mempelajari cara komputer berinteraksi dengan penggunaan bahasa dalam kehidupan sehari-hari. NLP bertujuan mengembangkan teknik-teknik agar komputer dapat memahami bahasa alami manusia.

Setiap negara memiliki bahasa ibu yang berbeda-beda, bukan? Bahasa alami yang digunakan oleh manusia dari berbagai negara memiliki perbedaan dalam hal penulisan dan pengucapan. Perbedaan penulisan dan pengucapan bahasa alami antar negara memengaruhi kompleksitas dalam pengembangan NLP. Hal ini menuntut pengembang NLP untuk memperhatikan variasi bahasa dan menciptakan model yang dapat mengakomodasi perbedaan tersebut. 

Singkatnya, NLP didefinisikan sebagai kemampuan komputer untuk memproses bahasa lisan ataupun tulisan yang digunakan dalam percakapan sehari-hari manusia. Dalam komputasi, bahasa harus diwakili sebagai rangkaian simbol yang mengikuti aturan tertentu. Jadi, harapannya dengan memanfaatkan teknologi NLP, pengembang bisa membuat komputer dapat memahami perintah yang ditulis dalam bahasa manusia standar. Hmm, komputer memahami bahasa manusia, menarik sekali, ya!

Peran NLP dalam kehidupan sehari-hari: 
Hai, teman-teman! Sadar ataupun tidak, kita semua pasti sangat sering berdampingan dengan yang namanya NLP, lo! Nah, NLP ini sebenarnya seperti sihir di balik layar gadget kita. Dengan kepiawaiannya mengolah bahasa manusia, NLP dapat menjadi andalan kita dalam kehidupan sehari-hari, dari mulai berbincang dengan asisten virtual sampai membaca update media sosial. 

Namun, apakah peran NLP hanya sebatas itu? Tentu tidak! Masih banyak lagi hal menarik yang bisa kita kupas bersama. Yuk, mari kita jelajahi lebih dalam!

Search Engine (Mesin Pencari)
Saat menggunakan mesin pencari, seperti Google, NLP membantu dalam memahami kata kunci atau pertanyaan yang Anda masukkan. Kemudian, NLP digunakan untuk menganalisis jutaan halaman web serta menyajikan hasil pencarian yang paling relevan dengan memperhitungkan konteks dan makna kata-kata. 

Asisten Virtual
Chatbot, seperti yang terdapat dalam layanan pelanggan online atau asisten virtual: Siri, Google Assistant, atau Alexa, menggunakan NLP untuk memahami dan merespons percakapan manusia secara alami. NLP memungkinkan mereka untuk menjawab pertanyaan, memberikan informasi, atau menyelesaikan tugas berdasarkan instruksi verbal dari pengguna.

Penerjemahan Otomatis
Layanan penerjemahan, seperti Google Translate menggunakan NLP untuk menerjemahkan teks dari satu bahasa ke bahasa lain secara otomatis. NLP membantu mengenali struktur dan makna kalimat untuk menghasilkan terjemahan yang lebih akurat. 

Analisis Sentimen Media Sosial
NLP digunakan untuk menganalisis sentimen publik terhadap merek, produk, atau layanan berdasarkan ulasan dan komentar di media sosial. Hal ini membantu perusahaan untuk memahami umpan balik pelanggan dan mengambil tindakan yang sesuai.

Deteksi Spam
NLP membantu dalam deteksi email spam dengan menganalisis pola dan konten email untuk mengidentifikasi pesan yang tidak diinginkan. Ini membantu dalam meningkatkan keamanan dan keandalan sistem ketika memilah email.

Pemeriksaan Tanda Baca, Tata Bahasa, dan Parafrase
Dalam pengolah kata atau pesan teks, NLP digunakan saat memeriksa tanda baca, tata bahasa, dan melakukan parafrase untuk memperbaiki struktur kalimat. NLP membantu pengguna menulis dengan benar dan efektif, mengidentifikasi kesalahan tata bahasa, serta memberikan rekomendasi untuk membuat kalimat lebih jelas dan mudah dipahami.

Chatbot dalam Layanan Pelanggan
Banyak perusahaan menggunakan chatbot berbasis NLP untuk menyediakan layanan pelanggan 24/7. Chatbot ini dapat merespons pertanyaan pelanggan, membantu dalam proses pemesanan atau pembayaran, dan memberikan informasi tentang produk atau layanan.

Melalui berbagai aplikasi ini, NLP tidak hanya memfasilitasi interaksi manusia dengan teknologi, tetapi juga membuka potensi baru pada analisis data teks yang mendalam. Kemampuan NLP memproses bahasa manusia menjadi kunci untuk membangun sistem yang lebih pintar dan responsif dalam berbagai bidang kehidupan.

Tensorflow + NLP, Mengapa?

Jadi, ketika membicarakan TensorFlow dan NLP, seakan-akan kita membahas dua kekuatan besar dalam dunia teknologi. Mengapa mereka bisa menjadi pasangan yang sangat kuat? Nah, TensorFlow bisa diibaratkan sebagai senjata utamanya. Melalui beragam tools yang dimilikinya, TensorFlow membantu kita dalam melatih model machine learning dengan sangat efisien. 

Di sisi lain, NLP menjadi tujuan utamanya, yaitu mengolah dan memahami bahasa manusia. Ketika kedua kekuatan tersebut digabungkan, ini seperti memberikan senjata canggih kepada pahlawan untuk menaklukkan segala tantangan yang berkaitan dengan bahasa. Tertarik untuk mengeksplorasi lebih dalam kisah epik tentang TensorFlow dan NLP? Let's goooooooo!

Jadi, TensorFlow adalah sebuah framework yang sangat populer dalam dunia machine learning dan deep learning yang dikembangkan oleh Google. Salah satu alasan utama TensorFlow sering digunakan dalam konteks NLP adalah karena kemampuannya yang sangat baik saat melakukan komputasi numerik. Komputasi dilakukan secara efisien, terutama pada operasi-operasi matriks yang sering digunakan dalam model NLP, seperti neural networks.

Selain itu, TensorFlow menyediakan berbagai model serta algoritma siap pakai untuk tugas-tugas NLP, seperti pengenalan teks, penerjemahan mesin, analisis sentimen, dan lainnya. Melalui TensorFlow Hub, pengguna mudah mengakses repository model yang dioptimalkan pada tugas NLP dan menggunakan model tersebut untuk membangun aplikasi NLP dengan cepat.

TensorFlow juga memiliki TensorFlow Extended (TFX). Itu adalah platform end-to-end untuk pengembangan, pelatihan, pengujian, dan implementasi model machine learning, termasuk model-model NLP. Ini memudahkan integrasi model NLP dalam lingkungan produksi secara lebih terstruktur.

Kelebihan lainnya adalah dokumentasi lengkap dan komunitas aktif di sekitar TensorFlow, membuatnya menjadi framework yang mudah dipelajari serta digunakan untuk pengembangan aplikasi NLP. TensorFlow juga mendukung integrasi dengan TensorFlow.js untuk menjalankan model NLP pada browser dan TensorFlow Lite untuk menjalankan model pada perangkat mobile atau embedded.

Dengan berbagai fitur ini, TensorFlow menjadi salah satu pilihan utama bagi para pengembang untuk membangun aplikasi NLP yang kuat, efisien, dan scalable. Namun, selain TensorFlow, ada juga framework dan library lain, seperti PyTorch, Hugging Face Transformers, dan spaCy. Framework-framework tersebut juga populer di kalangan praktisi NLP. Pilihan framework yang tepat tergantung pada kebutuhan proyek dan preferensi pengembang.



Text Preprocessing:
Nah, dalam konteks NLP, text preprocessing adalah langkah awal yang krusial sebelum kita masuk ke tahap analisis atau pemodelan. Jadi, mari kita telusuri bersama-sama hal yang sebenarnya dilakukan dalam proses ini serta alasan ini begitu penting untuk perjalanan kita dalam memahami dan memanfaatkan teks secara efektif.

Ide awalnya, pada NLP, data yang akan diolah seringkali berupa teks tidak terstruktur atau memiliki struktur tidak teratur. Contohnya, data teks bisa berupa dokumen, artikel, tweet, atau ulasan pelanggan yang tidak mengikuti format atau struktur tertentu. Oleh karena itu, sebelum menerapkan teknik-teknik NLP, seperti sentiment analysis atau topic modelling, kita perlu melakukan proses pengubahan bentuk data ini menjadi format yang lebih terstruktur.

Langkah-langkah pra-pemrosesan teks dalam NLP sangat penting untuk mengubah teks menjadi bentuk numerik yang dapat dipahami oleh komputer. Algoritma dan komputer secara alami memahami data dalam bentuk numerik, seperti vektor atau matriks. 

Let's prepare first!
Salah satu keunggulan Python adalah dukungannya terhadap banyak library sumber terbuka (open-source) dan memungkinkan implementasi yang kuat dalam NLP. Ada beragam library Python yang dapat digunakan untuk memproses dan menerapkan solusi dalam NLP. Beberapa di antaranya yang akan kita bahas adalah NLTK dan Sastrawi.

Natural Language Toolkit (NLTK)
NLTK adalah sebuah library Python yang digunakan untuk memanipulasi dan menganalisis teks dalam NLP. NLTK menyediakan berbagai alat yang sangat berguna untuk persiapan teks sebelum digunakan dalam machine learning atau algoritma deep learning. Tujuan utama NLTK adalah membantu dalam pemrosesan teks untuk keperluan analisis atau pengembangan model NLP.

Salah satu kelebihan NLTK adalah kemampuannya untuk melakukan berbagai tugas pra-pemrosesan teks, yaitu tokenisasi (pemecahan teks menjadi token), stemming (menghilangkan akhiran kata), lematisasi (mengubah kata menjadi bentuk dasarnya), penghilangan stopwords (kata-kata umum yang tidak memberikan nilai tambah), dan banyak lagi. 

Selain itu, NLTK juga dilengkapi dengan koleksi besar data bahasa (corpora) yang dapat digunakan untuk melatih model bahasa atau melakukan penelitian dalam bidang NLP. Cara termudah untuk menginstal NLTK adalah menggunakan pip, yaitu alat manajemen paket untuk Python, melalui perintah berikut pada command line atau terminal.

!pip install nltk
Dengan NLTK, pengguna dapat dengan mudah mengimplementasikan teknik-teknik NLP yang diperlukan untuk memproses dan menganalisis teks sebelum menggunakan data tersebut dalam konteks machine learning atau deep learning.


Python Sastrawi:
Python Sastrawi
Python Sastrawi adalah sebuah library Python yang digunakan untuk melakukan proses stemming dalam bahasa Indonesia. Stemming adalah proses mengubah kata-kata menjadi kata dasar atau kata akar dengan cara menghilangkan imbuhan atau awalan.

Library Python Sastrawi ini didasarkan pada kamus kata dasar bahasa Indonesia yang digunakan untuk mengidentifikasi akar kata dari kata-kata dalam teks berbahasa Indonesia. Proses stemming sangat berguna dalam pengolahan teks untuk analisis teks, pengelompokan kata, atau aplikasi NLP lainnya di lingkungan berbahasa Indonesia. Cara menggunakan Python Sastrawi untuk melakukan stemming dalam bahasa Indonesia adalah dengan menginstal library tersebut terlebih dahulu menggunakan pip.

!pip install Sastrawi
Berikut adalah beberapa langkah umum dalam pra-pemrosesan teks dalam NLP.


Case Folding:
Case folding adalah langkah sederhana pada pra-pemrosesan teks yang bertujuan untuk mengubah semua huruf dalam dokumen teks menjadi huruf kecil. Tujuannya adalah membuat teks lebih seragam dan memudahkan proses analisis teks, terutama dalam pengenalan kata-kata yang sama meskipun berbeda penulisan huruf besar-kecil.

Case folding adalah proses standarisasi teks dengan mengubah semua huruf dalam teks menjadi huruf kecil. Tujuan utamanya adalah untuk menciptakan konsistensi dalam representasi teks, sehingga mempermudah dalam analisis teks selanjutnya.

Dengan melakukan case folding, perbedaan huruf besar dan kecil dalam teks diabaikan, sehingga memungkinkan perbandingan dan pencarian teks menjadi lebih mudah dan efisien.

Dalam case folding, hanya huruf dari 'a' sampai 'z' yang dapat diterima. Karakter selain huruf akan dihapus dan dianggap sebagai pemisah antar kata. Hal ini berarti tanda baca, angka, dan karakter khusus lainnya akan diabaikan atau dihapus dari teks. Pada langkah ini, kita tidak perlu mengandalkan library eksternal dan dapat menggunakan modul-modul bawaan yang tersedia dalam Python.

Mengubah teks menjadi huruf kecil adalah suatu langkah penting pada pengolahan teks, terutama dalam search engine. Pengaturan indeks pencarian harus memperhitungkan perbedaan antara huruf besar dan kecil. Jadi, pencarian dapat mencocokkan nama dengan benar tanpa memedulikan kapitalisasi. 

Misalnya, jika sistem pencarian tidak memperlakukan "Machine Learning" dan "machine learning" sebagai entitas yang sama, pengguna mungkin mengalami kesulitan dalam menemukan kontak yang mereka cari, meskipun tampilan antarmuka pengguna menampilkan nama dengan huruf besar.

Berikut adalah contoh penggunaan Python untuk mengonversi teks menjadi lowercase.

teks_asli = "Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase."
 
# Mengubah teks menjadi lowercase
teks_lowercase = teks_asli.lower()
 
# Menampilkan hasil
print("Teks asli:", teks_asli)
print("Teks setelah diubah menjadi lowercase:", teks_lowercase)


Dalam contoh di atas, kita menggunakan metode .lower() pada string teks_asli untuk mengubah semua karakter huruf menjadi huruf kecil. Hasilnya, teks_lowercase akan berisi teks yang telah diubah menjadi huruf kecil.

Output dari program ini akan menunjukkan perbedaan antara teks asli dan teks setelah diubah menjadi lowercase. Metode .lower() sangat berguna dalam pengolahan teks untuk menormalisasi teks sebelum melakukan analisis lebih lanjut, seperti tokenisasi, pencocokkan kata, atau analisis teks lainnya.



Remove Special Characters:
Dalam pemrosesan teks, seringkali kita perlu membersihkan teks dari karakter khusus yang tidak diinginkan, seperti angka, tanda baca, simbol, atau karakter khusus lainnya. Langkah ini penting untuk mempersiapkan teks agar sesuai dengan kebutuhan analisis atau pemrosesan selanjutnya.

Karakter khusus, yakni !, @, #, $, ?, dan lainnya, tidak selalu memberikan kontribusi informasi yang berguna dalam analisis teks. Sebaliknya, mereka dapat mengganggu algoritma pemrosesan teks dan menyulitkan proses komputasi.

Menghapus Angka
Menghapus angka dari teks adalah langkah penting dalam pra-pemrosesan teks untuk fokus pada informasi teks yang relevan tanpa mempertimbangkan numerik. 

def hapus_angka(teks):
    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])
    return teks_tanpa_angka
 
teks_dengan_angka = "Ini adalah contoh teks dengan angka 12345 yang akan dihapus."
 
teks_tanpa_angka = hapus_angka(teks_dengan_angka)
 
print("Teks dengan angka:", teks_dengan_angka)
print("Teks tanpa angka:", teks_tanpa_angka)

Dalam contoh di atas, kita menggunakan fungsi hapus_angka() untuk menghapus semua karakter angka dari teks menggunakan list comprehension. Fungsi isdigit() digunakan untuk memeriksa bahwa sebuah karakter adalah angka. Hasilnya, teks_tanpa_angka akan berisi teks yang tidak mengandung karakter angka.

Dengan menghapus angka dari teks, kita dapat memfokuskan analisis pada informasi teks yang bersifat non-numerik. Hal ini sering digunakan dalam pemrosesan teks untuk analisis sentimen, klasifikasi teks, atau tugas NLP lainnya ketika numerik tidak relevan dengan konteks analisis tersebut.

Terkadang, kita perlu menghilangkan angka dari teks, tetapi ingin mempertahankan angka-angka penting, seperti nomor rumah atau nomor telepon. Oleh sebab itu, kita dapat menggunakan ekspresi regular (regex) untuk mengenali dan menghapus angka yang tidak relevan.

Inilah contoh penggunaan Regex untuk menghapus angka.

import re
 
def hapus_angka_tidak_relevan(teks):
    # Menggunakan regex untuk mengidentifikasi dan menghapus angka yang tidak relevan
    # Pola untuk mengenali angka yang harus dihapus, termasuk nomor rumah dan nomor telepon
    pola_angka_tidak_relevan = r"\b(?:\d{1,3}[-\.\s]?)?(?:\d{3}[-\.\s]?)?\d{4,}\b"
    hasil = re.sub(pola_angka_tidak_relevan, "", teks)
    return hasil.strip()
 
kalimat = "Di sini ada beberapa nomor rumah yaitu  123, 456, dan 789. Silakan hubungi 081234567890 untuk informasi lebih lanjut."
 
hasil_tanpa_angka = hapus_angka_tidak_relevan(kalimat)
 
# Menampilkan hasil
print("Kalimat dengan angka:", kalimat)
print("Kalimat tanpa angka tidak relevan:", hasil_tanpa_angka)



Pada contoh di atas, kita menggunakan regex r"\b\d+\b" untuk mencocokkan dan menghapus angka sebagai kata tunggal dalam teks. Ekspresi regex ini memastikan bahwa hanya angka-angka yang muncul sebagai kata terpisah yang dihapus.

Ekspresi regex ini memiliki beberapa komponen penting sebagai berikut.

- \b adalah anchor word boundary yang menandakan batas antara karakter word (kata) dan non-word (bukan kata). Ini memastikan bahwa kita hanya mencocokkan angka yang muncul sebagai kata terpisah.
- \d+ adalah pola regex yang mencocokkan satu atau lebih digit (angka).
- \b adalah anchor word boundary lagi yang menutup pola, memastikan bahwa angka yang dicocokkan adalah kata tunggal yang berdiri sendiri. 

Dengan menggunakan ekspresi regex ini, kita memastikan bahwa hanya angka-angka yang muncul sebagai kata terpisah yang akan dihapus dari teks. Ini memungkinkan kita untuk mempertahankan angka-angka dalam konteks nomor rumah atau nomor telepon.

Menghapus Tanda Baca
Tanda baca dalam kalimat tidak memengaruhi proses pra-pemrosesan teks dalam sistem NLP. Oleh karena itu, ini perlu dihapus agar tidak mengganggu kinerja sistem. Untuk menghapus tanda baca, seperti [!”#$%&’()*+,-./:;<=>?@[]^_`{|}~], Anda dapat menggunakan Python dengan menggunakan pendekatan berikut.

mport string
 
def remove_punctuation(text):
    # Membuat set yang berisi semua tanda baca
    punctuation_set = set(string.punctuation)
 
    # Menghapus tanda baca dari teks
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)
 
    return text_without_punctuation
 
teks_asli = "Ini adalah contoh teks, dengan tanda baca! Contoh ini, digunakan? untuk demonstrasi."
 
teks_tanpa_tanda_baca = remove_punctuation(teks_asli)
 
print("Teks asli:", teks_asli)
print("Teks setelah menghapus tanda baca:", teks_tanpa_tanda_baca)

Dalam contoh di atas, kalimat mengandung beberapa tanda baca, seperti tanda seru (!), koma (,), titik (.), dan tanda tanya (?). Fungsi hapus_tanda_baca() digunakan untuk menghapus semua tanda baca dari kalimat menggunakan metode translate() dengan str.maketrans('', '', string.punctuation). Hasilnya adalah kalimat yang tidak mengandung tanda baca, seperti yang ditunjukkan dalam output.


Menghapus Whitespace dalam Teks
Whitespace (karakter kosong) merujuk kepada karakter yang tidak terlihat pada layar, seperti spasi, tab, newline, dan karakter kosong lainnya. Karakter whitespace umumnya digunakan untuk memisahkan kata-kata atau elemen dalam teks, tetapi kadang-kadang mereka dapat muncul secara tidak diinginkan di awal, akhir, atau pertengahan teks.

Menggunakan strip() untuk Menghapus Whitespace di Awal dan Akhir
Metode strip() pada string digunakan untuk menghapus whitespace di awal dan akhir string. Ini sangat berguna saat Anda ingin membersihkan teks dari spasi tambahan yang mungkin terdapat pada sebelum atau setelah teks utama. Berikut contoh penggunaannya.

teks = "   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    "
teks_setelah_strip = teks.strip()
print(teks_setelah_strip)

Metode strip() sangat berguna dalam pra-pemrosesan teks untuk membersihkan teks dari karakter kosong yang tidak diinginkan, sebelum melakukan analisis atau pemrosesan lebih lanjut.

Menggunakan replace() untuk Menghapus Whitespace di Seluruh String
Jika Anda perlu menghapus semua whitespace dalam string, Anda dapat menggunakan metode replace() untuk mengganti setiap whitespace dengan string kosong (""). Contoh penggunaannya seperti ini.

teks_dengan_whitespace = "Ini adalah    contoh kalimat    dengan spasi    di dalamnya."
teks_tanpa_whitespace = teks_dengan_whitespace.replace(" ", "")
print(teks_tanpa_whitespace)

Pada contoh ini, metode replace(" ", "") digunakan untuk mengganti setiap spasi dengan string kosong sehingga menghapus semua whitespace dalam string teks_dengan_whitespace.

Kesimpulannya, metode strip() berguna untuk menghilangkan whitespace di awal dan akhir string, sementara replace() dapat digunakan untuk menghapus whitespace di seluruh string. Pilih metode sesuai dengan kebutuhan Anda berdasarkan lokasi dan jenis whitespace yang perlu dihapus, ya!


Stopwrod Removal (Filtering)
Penghapusan stopwords adalah langkah penting dalam pra-pemrosesan teks pada NLP. Stopwords adalah kata-kata umum yang sering muncul pada teks, tetapi tidak memiliki nilai informatif tinggi dalam analisis teks. Contohnya adalah kata-kata seperti "dan", "di", "ke", "yang", dan sebagainya. Tujuan penghapusan stopwords adalah membersihkan teks dari kata-kata umum tersebut sehingga fokus analisis dapat lebih pada kata-kata kunci yang lebih bermakna. 

Pada tahap filtering dalam pemrosesan teks, kita melakukan seleksi kata-kata penting dengan menghapus kata-kata yang dianggap kurang relevan atau memiliki informasi rendah. Hal ini biasanya dilakukan dengan mengimplementasikan daftar stopword. 

Misalnya, ketika melakukan pencarian melalui mesin pencari dengan pertanyaan "apa itu pengertian machine learning?", Anda ingin sistem memfokuskan hasil pencarian pada dokumen-dokumen yang berkaitan langsung dengan "pengertian machine learning" dan tidak terganggu oleh kata-kata umum, seperti "apa" atau "itu".

Mari kita bahas lebih rinci perbedaan antara stopwords yang disediakan oleh NLTK dan Sastrawi, serta cara kita dapat menggunakannya dalam konteks pengolahan teks dengan bahasa Indonesia. 

Stopwords NLTK (Natural Language Toolkit)
NLTK adalah toolkit yang sangat populer untuk NLP dalam Python. NLTK menyediakan koleksi stopwords untuk beberapa bahasa, termasuk bahasa Indonesia. Stopwords NLTK dapat digunakan untuk membersihkan teks dari kata-kata umum yang tidak memberikan nilai tambah dalam analisis teks.

Contoh Penggunaan Stopwords NLTK untuk Bahasa Indonesia

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
 
# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt')  # Untuk tokenisasi kata
 
teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."
 
# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)
 
# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))
 
# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]
 
# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)
 
print("Teks asli:", teks)
Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.

print("Teks setelah filtering stopwords NLTK:", teks_tanpa_stopwords)
Perekonomian Indonesia pertumbuhan membanggakan.

Dalam contoh di atas, kita menggunakan NLTK untuk mengunduh daftar stopwords bahasa Indonesia. Kemudian, kita tokenisasi teks menjadi kata-kata menggunakan word_tokenize() dari NLTK. Selanjutnya, kita melakukan filtering kata-kata dengan menghapus stopwords di dalamnya. Hasilnya adalah teks yang hanya berisi kata-kata penting tanpa stopwords.


Stopwords Sastrawi
Sastrawi adalah pustaka Python yang fokus pada pemrosesan teks dalam bahasa Indonesia, terutama untuk NLP. Sastrawi menyediakan daftar stopwords yang dioptimalkan untuk bahasa Indonesia sehingga sangat cocok digunakan pada analisis teks dalam bahasa Indonesia.

Contoh Penggunaan Stopwords Sastrawi untuk Bahasa Indonesia

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from nltk.tokenize import word_tokenize
 
# Inisialisasi objek StopWordRemover dari Sastrawi
factory = StopWordRemoverFactory()
stopwords_sastrawi = factory.get_stop_words()
 
teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."
 
# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)
 
# Filtering kata-kata dengan menghapus stopwords Sastrawi
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]
 
# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)
 
print("Teks asli:", teks)
Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.

print("Teks setelah filtering stopwords Sastrawi:", teks_tanpa_stopwords)
Perekonomian Indonesia sedang pertumbuhan membanggakan.


Dalam contoh di atas, kita menggunakan Sastrawi untuk mengakses daftar stopwords bahasa Indonesia. Kemudian, kita tokenisasi teks menjadi kata-kata menggunakan word_tokenize() dari NLTK. Selanjutnya, kita melakukan filtering kata-kata dengan menghapus stopwords Sastrawi dari kata-kata tersebut. Hasilnya adalah teks yang hanya berisi kata-kata penting tanpa stopwords.

Perbedaan Utama
Perbedaan utama antara stopwords NLTK dan Sastrawi sebagai berikut.
- Bahasa yang Didukung: NLTK menyediakan stopwords untuk beberapa bahasa termasuk bahasa Indonesia, sedangkan Sastrawi dioptimalkan untuk pemrosesan teks dalam bahasa Indonesia.
- Kustomisasi: Sastrawi memungkinkan dalam mengakses daftar stopwords yang dioptimalkan khusus untuk bahasa Indonesia, sementara NLTK memiliki koleksi stopwords pada berbagai bahasa dan bisa lebih umum.

Pilihan antara NLTK dan Sastrawi untuk filtering stopwords tergantung pada bahasa teks yang Anda proses serta kebutuhan analisis NLP Anda. Untuk bahasa Indonesia, Sastrawi adalah pilihan yang lebih spesifik dan sesuai karena fokus pada bahasa tersebut. Namun, NLTK juga dapat digunakan jika Anda memerlukan stopwords dalam berbagai bahasa atau jika Anda sudah menggunakan NLTK untuk alat NLP lainnya.


Tokenizing:
Tokenisasi (tokenizing) adalah proses membagi teks menjadi potongan-potongan lebih kecil yang disebut token. Token dapat berupa kata, frasa, atau entitas lain yang lebih kecil dari teks yang dianalisis. Tujuan tokenisasi adalah memecah teks menjadi unit-unit yang lebih mudah diolah atau diinterpretasi dalam analisis teks atau pemrosesan bahasa alami.

Dalam pemrosesan teks atau NLP, tokenisasi adalah langkah penting dalam pra-pemrosesan teks sebelum analisis lebih lanjut dilakukan, seperti klasifikasi teks, pembuatan model bahasa, atau ekstraksi fitur. Tujuan utama tokenisasi adalah mengubah teks menjadi urutan token yang terstruktur untuk digunakan dalam analisis atau aplikasi NLP.

Ada berbagai metode untuk melakukan tokenisasi, tergantung pada kebutuhan dan kompleksitas tugas yang dihadapi. Berikut adalah beberapa metode tokenisasi umum.

Tokenisasi Kata (Word Tokenization)
Memecah teks menjadi kata-kata individu. Biasanya, pemisah antar kata adalah spasi atau karakter whitespace lainnya.

from nltk.tokenize import word_tokenize
 
text = "Ini adalah contoh tokenisasi kata dalam pemrosesan teks."
tokens = word_tokenize(text)
print(tokens)

['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks', '.']

Pada contoh di atas, teks dibagi menjadi token-token kata menggunakan word_tokenize() dari NLTK. Hasilnya adalah daftar kata-kata yang membentuk teks tersebut.

Tokenisasi Kalimat (Sentence Tokenization)
Memecah teks menjadi kalimat-kalimat. Pemisah antar kalimat dapat berupa tanda baca, seperti titik, tanda tanya, atau tanda seru.

text = "Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga!"
sentences = sent_tokenize(text)
print(sentences)

[‘Ini adalah contoh tokenisasi kalimat.’, ‘Apakah ini kalimat kedua?’, 

‘Ya, ini kalimat ketiga!’]


Pada contoh di atas, teks dibagi menjadi kalimat-kalimat menggunakan sent_tokenize() dari NLTK. Hasilnya adalah daftar kalimat-kalimat dalam teks tersebut.

Tokenisasi Frasa (Phrase Tokenization)
Memecah teks menjadi frasa-frasa atau unit-unit frasa yang lebih besar dari kata tunggal.

text = "Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen."
phrases = text.split(',')
print(phrases)

['Pemrosesan', 'teks', 'adalah', 'cabang', 'ilmu', 'komputer', 'yang', 'berfokus', 'pada', 'pengolahan', 'teks', 'dan', 'dokumen', '.']


Pada contoh di atas, teks dibagi menjadi frasa-frasa menggunakan TreebankWordTokenizer() dari NLTK. Hasilnya adalah daftar frasa-frasa yang membentuk teks tersebut.

Tokenisasi Berdasarkan Aturan (Rule-based Tokenization)
Menggunakan aturan linguistik atau aturan pemrosesan teks untuk membagi teks menjadi token. Contohnya termasuk tokenisasi khusus untuk URL, email, atau entitas tertentu.

import re

text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']

Pada contoh di atas, teks dibagi berdasarkan aturan regex untuk mengenali URL. Hasilnya adalah daftar URL dalam teks tersebut.


Tokenisasi Berdasarkan Model (Model-based Tokenization)
Menggunakan model statistik atau model bahasa untuk memprediksi token dalam teks. Contohnya termasuk tokenisasi berbasis mesin pembelajaran, seperti model pembelajaran berbasis Transformer.

text = "Ini adalah contoh tokenisasi berbasis model."
tokens = text.split()
print(tokens)

['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']

Dengan menggunakan berbagai metode tokenisasi ini, kita dapat memecah teks menjadi unit-unit lebih kecil sesuai dengan kebutuhan analisis atau aplikasi NLP yang sedang dilakukan.

Eitssss! Jangan ragu atau bingung! Setiap metode tokenisasi memiliki perannya sendiri, tergantung pada konteks dan tujuan analisis teks yang ingin dicapai. Anda tidak perlu menggunakan semua metode sekaligus, kok! Cukup gunakan metode tokenisasi yang dibutuhkan berdasarkan kebutuhan spesifik Anda.

Metode tokenisasi kata dan kalimat adalah yang paling umum serta sering digunakan dalam pemrosesan teks. Oleh karena itu, penting untuk memahami secara baik kedua metode tersebut agar dapat menerapkan analisis teks dengan lebih efektif.



Stemming:
Stemming adalah sebuah proses dalam bidang pemrosesan teks yang digunakan untuk menyederhanakan kata-kata ke bentuk dasarnya dengan cara menghilangkan awal ataupun akhiran kata. Tujuannya adalah agar kata-kata berbeda, tetapi berasal dari satu kata dasar, dapat direduksi menjadi bentuk yang lebih seragam.

Namun, perlu diingat bahwa hasil stemming tidak selalu sama dengan akar kata sebenarnya dalam bahasa. Stemming hanya berfokus pada pembuangan bagian awal ataupun akhir kata untuk menyederhanakan kata-kata, tanpa memperhatikan makna sebenarnya dari kata tersebut.

Misalnya, kita memiliki beberapa kata dalam bahasa Inggris sebagai berikut.

running → running
runs → runs
ran
easily
fairness → fairness

Dengan menggunakan algoritma stemming, kita dapat mereduksi kata-kata tersebut menjadi bentuk dasarnya sebagai berikut.

run
run
ran
easy
fair


from nltk.stem import PorterStemmer
 
# Inisialisasi stemmer
stemmer = PorterStemmer()
 
# Kata-kata asli
words = ["running", "runs", "runner", "ran", "easily", "fairness", "better", "best", "cats", "cacti", "geese", "rocks", "oxen"]
 
# Melakukan stemming pada setiap kata
for word in words:
    stemmed_word = stemmer.stem(word)
    print(f"Kata asli: {word}, Kata setelah stemming: {stemmed_word}")

Algoritma stemming, seperti Porter, Snowball, dan Lancaster menggunakan aturan-aturan sederhana untuk melakukan pemotongan pada kata-kata sehingga kita dapat mengelompokkan variasi kata yang mirip dalam bentuk dasar yang lebih umum. 

Stemming sangat berguna dalam pengolahan bahasa alami, pengindeksan teks, dan analisis teks untuk mengurangi variasi kata-kata serta memperlakukan kata-kata serupa dengan cara yang lebih konsisten.



Lemmatization:
Lemmatization adalah sebuah teknik dalam pemrosesan teks untuk mengubah kata-kata ke bentuk dasar mereka, yang disebut lemma (atau lema). Tujuannya adalah menyatukan kata-kata dengan akar sama agar dapat direpresentasikan melalui satu bentuk dasar yang merepresentasikan makna kata tersebut secara tepat.

Proses lemmatization melibatkan penentuan jenis kata (part of speech, POS) untuk memastikan kata dikembalikan ke lema yang benar. Misalnya, kata kerja (verb) memiliki lema yang berbeda dengan kata benda (noun).

from nltk.stem import WordNetLemmatizer
 
# Download wordnet jika belum di-download
nltk.download('wordnet')
 
# Inisialisasi lemmatizer
lemmatizer = WordNetLemmatizer()
 
# Kata-kata asli
words = ["Run", "Cat", "Good", "Goose", "Rock", "City", "Big", "Happy", "Run", "Sleep"]
 
# Melakukan lematisasi pada setiap kata
for word in words:
    lemma_word = lemmatizer.lemmatize(word.lower())  # Mengonversi ke huruf kecil untuk memastikan pemrosesan yang konsisten
    print(f"Kata asli: {word}, Kata setelah lematisasi: {lemma_word}")

Perbedaan utama antara lemmatization dan stemming adalah lemmatization lebih cermat karena mempertimbangkan aturan morfologi bahasa. Misalnya, lemmatization akan mengubah kata-kata seperti "studies", "studying" menjadi bentuk dasar "study", sementara stemming hanya memotong akhiran kata tanpa memperhatikan makna menjadi "studi" dan "study".

Stemming bertujuan memotong awalan atau akhiran kata untuk mencapai bentuk dasar, yang mungkin, menghasilkan kata "improv" dari semua bentuk tersebut. Kata ini adalah bentuk yang umum dan sering digunakan dalam proses stemming.

Sementara itu, tujuan lematisasi adalah mengembalikan kata-kata ke bentuk dasar dalam bahasa yang sesuai dengan makna sebenarnya. Dalam hal ini, "improve" dipertahankan sebagai bentuk dasar karena lemmatization memperhitungkan struktur morfologis kata tersebut.




Latihan Pra-pemrosesan Teks:

Pra-pemrosesan teks adalah tahap penting dalam analisis teks dan natural language processing (NLP). Tujuannya adalah membersihkan dan mempersiapkan teks mentah agar dapat diolah lebih lanjut dengan algoritma pemrosesan teks atau analisis.

1. Case Folding
Proses mengubah semua huruf dalam teks menjadi huruf kecil atau huruf besar agar konsisten. Misalnya, mengubah "TeKS" menjadi "teks" atau "TEKS".

2. Removal Special Characters
Menghapus karakter khusus atau simbol yang tidak relevan atau tidak diinginkan dari teks.
- Menghapus Angka: Menghilangkan semua angka dari teks.
- Menghapus Tanda Baca: Menghapus semua tanda baca dari teks.
- Menghapus White Space: Menghapus spasi tambahan atau karakter spasi ganda dari teks.
- Menggunakan strip(): Menggunakan metode strip() dalam pemrograman untuk menghapus spasi tambahan di awal dan akhir teks.
- Menggunakan replace(): Menggunakan metode replace() untuk mengganti spasi tambahan dengan string kosong sehingga menghapusnya dari seluruh teks.

3. Stopword Removal (Filtering)
Menghapus kata-kata yang umumnya tidak memberikan nilai tambah dalam analisis teks, seperti "dan", "atau", "yang", dll.
- Stopword NLTK (Natural Language Toolkit): Menggunakan koleksi kata-kata stopword yang disediakan oleh NLTK untuk menghapus stopword dari teks.
- Stopword Sastrawi: Penghapusan kata-kata stopword menggunakan kamus stopword yang disediakan oleh Sastrawi, pustaka pemrosesan bahasa alami bahasa Indonesia.

4. Tokenizing
Proses membagi teks menjadi bagian-bagian lebih kecil yang disebut token.
- Tokenisasi Kata (Word Tokenization): Memecah teks menjadi token berdasarkan kata-kata individual.
- Tokenisasi Kalimat (Sentence Tokenization): Memecah teks menjadi token berdasarkan kalimat-kalimat.
- Tokenisasi Frasa (Phrase Tokenization): Memecah teks menjadi token berdasarkan frasa-frasa atau unit-unit tertentu.
- Tokenisasi Berdasarkan Aturan (Rule-based Tokenization): Memecah teks menjadi token berdasarkan aturan tertentu, seperti pemisahan berdasarkan tanda baca.
- Tokenisasi Berdasarkan Model (Model-based Tokenization): Memecah teks menjadi token menggunakan model linguistik atau machine learning.


5. Stemming
Proses menghapus imbuhan dari kata untuk mengembalikannya ke bentuk dasarnya. Misalnya, mengubah "berlari", "berlarian", "lari" menjadi "lar".

6. Lemmatization
Proses mengubah kata-kata ke bentuk dasarnya (lema) dengan mempertimbangkan konteks dan struktur bahasa. Misalnya, mengubah "menyanyikan" menjadi "nyanyi".

teks_asli = "Ini Adalah Contoh Teks yang Akan Dikonversi Menjadi Lowercase."
 
# Mengubah teks menjadi lowercase
teks_lowercase = teks_asli.lower()

Setelah teks asli didefinisikan pada variabel teks_asli, kita menggunakan metode lower() untuk mengonversinya menjadi lowercase dan hasilnya disimpan dalam variabel teks_lowercase. Hasilnya kemudian ditampilkan menggunakan perintah print(). 

Dengan demikian, kita dapat dengan mudah melihat perbedaan antara teks asli dan yang telah diubah menjadi lowercase. Proses ini sangat berguna pada pemrosesan teks karena membuatnya menjadi konsisten dalam penggunaan huruf, memudahkan analisis, dan memungkinkan pencocokan yang lebih baik.


Removal Special Characters

Menghapus Angka
Angka seringkali dianggap tidak penting dalam pemrosesan teks dan dapat mengganggu kualitas model prediksi. Oleh karena itu, mereka perlu dihapus agar tidak mengganggu proses analisis teks. Berikut contoh kodenya. 

def hapus_angka(teks):
    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()])
    return teks_tanpa_angka
 
# Contoh teks dengan angka
teks_dengan_angka = "Ini adalah contoh teks dengan angka 12345 yang akan dihapus."
 
# Memanggil fungsi untuk menghapus angka
teks_tanpa_angka = hapus_angka(teks_dengan_angka)


Fungsi hapus_angka(teks) menggunakan list comprehension untuk mengiterasi melalui setiap karakter dalam teks dan memeriksa bahwa karakter tersebut bukan angka dengan metode isdigit(). Karakter-karakter bukan angka kemudian digabungkan kembali menjadi sebuah string baru yang tidak mengandung angka. 

Contoh teks dengan angka (teks_dengan_angka) diberikan sebagai input untuk fungsi hapus_angka(), yang menghasilkan teks tanpa angka (teks_tanpa_angka). Hasilnya ditampilkan bersama dengan teks asli menggunakan perintah print(). Proses ini membantu meningkatkan kualitas pemrosesan teks dengan menghilangkan unsur yang mungkin tidak relevan atau mengganggu. 

Menghapus Tanda Baca
Menghapus tanda baca dapat membantu meningkatkan kualitas analisis teks serta kinerja model NLP. Berikut contohnya.

def remove_punctuation(text):
    # Membuat set yang berisi semua tanda baca
    punctuation_set = set(string.punctuation)
 
    # Menghapus tanda baca dari teks
    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)
 
    return text_without_punctuation
 
# Contoh teks dengan banyak tanda baca
teks_asli = """
Dalam dunia ini, banyak hal terjadi, dari yang kecil hingga yang besar. Kita bisa melihat keindahan, tapi juga kekejaman. Ada harapan, namun juga keputusasaan. Bagaimanapun, hidup terus berjalan, tak peduli apa pun yang terjadi!
"""
 
# Menghapus tanda baca dari teks
teks_tanpa_tanda_baca = remove_punctuation(teks_asli)

Fungsi remove_punctuation(text) digunakan untuk menghapus semua tanda baca dari teks yang diberikan. Pertama, sebuah set yang berisi semua tanda baca didefinisikan menggunakan modul string. Set ini kemudian digunakan untuk memfilter karakter-karakter dalam teks bahwa hanya karakter yang tidak termasuk set tanda baca yang dipertahankan. Hasilnya adalah teks baru yang tidak mengandung tanda baca.

Contoh teks yang mengandung banyak tanda baca (teks_asli) diberikan sebagai input untuk fungsi remove_punctuation(). Hasilnya teks_tanpa_tanda_baca atau teks yang tidak mengandung tanda baca. Kedua teks tersebut ditampilkan menggunakan perintah print().


Menghapus White Space
Menghapus whitespace adalah langkah penting dalam pra-pemrosesan teks untuk membersihkan serta merapikan teks agar lebih mudah diproses dan dianalisis. Whitespace termasuk spasi, tab, dan karakter newline yang sering kali tidak memberikan informasi penting dalam teks, tetapi dapat memengaruhi hasil analisis jika tidak dikelola dengan baik. 

Berikut contohnya.

teks = "   Ini adalah contoh kalimat dengan spasi di awal dan akhir.    "
teks_dengan_whitespace = "Ini adalah    contoh kalimat    dengan spasi    di dalamnya."

teks_setelah_strip = teks.strip()
print(teks_setelah_strip)

teks_tanpa_whitespace = teks_dengan_whitespace.replace(" ", "") print(teks_tanpa_whitespace)

Dalam potongan kode tersebut, pertama-tama kita memiliki sebuah string teks yang memiliki spasi di awal dan akhir. Kita menggunakan metode strip() untuk menghapus spasi tambahan di awal dan akhir teks. Hasilnya disimpan dalam variabel teks_setelah_strip. Kemudian, kita mencetak hasilnya.

Kemudian, kita memiliki string teks_dengan_whitespace dengan spasi di antara kata-kata. Kita menggunakan metode replace() untuk mengganti setiap spasi dengan string kosong sehingga menghapus spasi dalam teks. Hasilnya disimpan dalam variabel teks_tanpa_whitespace dan kemudian dicetak.

Dengan menggunakan strip() dan replace(), kita dapat mudah mengelola whitespace pada teks, baik di awal/akhir maupun di dalamnya.


Stopword Removal
Penghapusan stopword adalah salah satu tahap penting pada pra-pemrosesan teks untuk menghilangkan kata-kata yang umumnya tidak memberikan makna signifikan dalam analisis teks. Dalam penghapusan stopword, kata-kata seperti "dan", "atau", "yang", dan sejenisnya diidentifikasi serta dihapus dari teks. 

Berikut contohnya.

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
 
# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh
nltk.download('stopwords')
nltk.download('punkt')  # Untuk tokenisasi kata
 
teks = "Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan."
 
# Tokenisasi teks menjadi kata-kata
tokens_kata = word_tokenize(teks)
 
# Ambil daftar stopwords bahasa Indonesia dari NLTK
stopwords_indonesia = set(stopwords.words('indonesian'))
 
# Filtering kata-kata dengan menghapus stopwords
kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]
 
# Gabungkan kata-kata penting kembali menjadi teks
teks_tanpa_stopwords = ' '.join(kata_penting)

Potongan kode di atas menjelaskan cara menggunakan Natural Language Toolkit (NLTK) dalam bahasa Python untuk menghapus stopwords dari teks berbahasa Indonesia. Pertama, NLTK digunakan untuk mengunduh korpus stopwords bahasa Indonesia jika belum terunduh. Selanjutnya, teks disediakan dan tokenisasi dilakukan menggunakan word_tokenize() untuk membagi teks menjadi kata-kata. 

Setelah itu, daftar stopwords bahasa Indonesia diambil dari korpus NLTK. Kata-kata dalam teks kemudian difilter dengan menghapus stopwords menggunakan list comprehension. Hasilnya adalah teks yang hanya berisi kata-kata penting setelah stopwords dihilangkan.


Tokenizing

Word Tokenization

# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Apel, jeruk, pisang, dan mangga."
phrases = text.split(',')

Potongan kode di atas adalah contoh sederhana dari tokenisasi kata atau frasa berdasarkan tanda baca koma (,). Dalam contoh ini, kita memiliki sebuah teks berisi daftar buah-buahan yang dipisahkan oleh koma. Untuk memisahkan frasa-frasa tersebut menjadi token-token, kita menggunakan metode split(',') yang membagi teks berdasarkan tanda koma. Ini menghasilkan daftar frasa-frasa yang dipisahkan. 

Output:

['Ini', 'adalah', 'contoh', 'kalimat', 'untuk', 'tokenisasi', 'kata', '.']

Hasilnya adalah sebuah list berisi frasa-frasa tersebut sebagai elemen-elemen individu. Dengan cara ini, kita dapat dengan mudah memisahkan teks menjadi bagian-bagian yang lebih kecil, yaitu frasa-frasa, berdasarkan tanda baca tertentu.

Sentence Tokenization
# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re
 
text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

Potongan kode di atas adalah contoh tokenisasi kalimat dalam bahasa Indonesia dengan menggunakan aturan khusus. Dalam contoh ini, kita ingin membagi teks menjadi token-token kata atau angka. Kita menggunakan modul re (regular expression) untuk mencocokkan pola dalam teks.

Pola yang digunakan dalam re.findall(r'\w+|\d+', text) memiliki dua bagian:

- \w+ untuk mencocokkan urutan karakter alfanumerik (kata) dan
- \d+ untuk mencocokkan urutan karakter numerik (angka).

Hasilnya adalah sebuah list berisi token-token kata atau angka dalam teks tersebut. Dengan menggunakan aturan khusus ini, kita dapat mudah melakukan tokenisasi kalimat dalam bahasa Indonesia dan menghasilkan token-token yang sesuai dengan kebutuhan analisis teks kita.


Phrase Tokenization
# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)
text = "Apel, jeruk, pisang, dan mangga."
phrases = text.split(',')
print(phrases)

Dalam potongan kode yang diberikan, kita melakukan tokenisasi frasa berdasarkan tanda baca koma (,). Teks yang diberikan adalah "Apel, jeruk, pisang, dan mangga." dan kita ingin memisahkan teks tersebut menjadi frasa-frasa berdasarkan tanda koma.

Dengan menggunakan metode split(','), kita membagi teks menjadi bagian-bagian yang dipisahkan oleh tanda koma. Hasilnya adalah sebuah list berisi frasa-frasa tersebut sebagai elemen-elemen individu. Jadi, setiap frasa dihasilkan sebagai sebuah elemen dalam list.

Output:

['Apel', ' jeruk', ' pisang', ' dan mangga.']

Namun, perlu diperhatikan bahwa ada spasi di awal kata 'jeruk', 'pisang', dan 'dan mangga.'. Jika ingin membuang spasi tersebut, kita dapat menggunakan metode strip() setelah pemisahan frasa, seperti ini: phrases = [phrase.strip() for phrase in phrases].

Rule-based Tokenization
# Contoh aturan tokenisasi khusus untuk tokenisasi kata dalam bahasa Indonesia
import re
 
text = "Pertama, kita perlu menyiapkan bahan-bahan yang diperlukan."
tokens = re.findall(r'\w+|\d+', text)
print(tokens)

Potongan kode di atas adalah contoh penggunaan aturan tokenisasi khusus untuk memisahkan kata-kata pada teks dalam bahasa Indonesia. Pada contoh ini, kita menggunakan modul re (regular expression) untuk mencari pola dalam teks yang sesuai dengan aturan.

Pola yang digunakan dalam re.findall(r'\w+|\d+', text) adalah

- \w+: mencocokkan urutan karakter alfanumerik (kata) dan
- \d+: mencocokkan urutan karakter numerik (angka).

Dengan menggunakan re.findall(), kita mencari semua pola sesuai dengan salah satu dari pola yang diberikan, yaitu urutan karakter alfanumerik atau numerik, dalam teks. Hasilnya adalah sebuah list berisi token-token kata atau angka dalam teks tersebut.

['Pertama', 'kita', 'perlu', 'menyiapkan', 'bahan', 'bahan', 'yang', 'diperlukan']

Namun, perlu diperhatikan bahwa tokenisasi ini tidak mengambil tanda baca seperti koma (,) sebagai token. Jika Anda ingin mempertahankan tanda baca, Anda dapat menyesuaikan pola regular expression yang digunakan.

Model-based Tokenization
# Misalnya menggunakan spasi sebagai pemisah kata
text = "Ini adalah contoh tokenisasi berbasis model."
tokens = text.split()
print(tokens)

Potongan kode di atas adalah contoh sederhana dari tokenisasi kata dalam teks menggunakan spasi sebagai pemisah. Pada contoh ini, kita memiliki teks "Ini adalah contoh tokenisasi berbasis model." dan ingin memisahkan kata-kata dalam teks tersebut menjadi token-token.

Dengan menggunakan metode split(), teks tersebut dipisahkan menjadi token-token berdasarkan spasi di antara kata-kata. Hasilnya adalah sebuah list berisi kata-kata tersebut sebagai elemen-elemen individu.

Hasil dari potongan kode di atas adalah berikut.

Output:

['Ini', 'adalah', 'contoh', 'tokenisasi', 'berbasis', 'model.']

Setiap kata pada teks dipisahkan dan dihasilkan sebagai sebuah elemen dalam list. Dengan cara ini, kita dapat mudah memisahkan teks menjadi token-token kata sebagai langkah awal dalam pemrosesan teks lebih lanjut. 


Stemming:

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
words = ["running", "easily", "bought", "crying", "leaves"]
stemmed_words = [stemmer.stem(word) for word in words]
print(stemmed_words)

Potongan kode di atas adalah contoh penggunaan algoritma stemming Porter dalam modul NLTK untuk mendapatkan bentuk dasar kata-kata. Dalam contoh ini, kita menggunakan PorterStemmer dari NLTK untuk melakukan stemming pada beberapa kata.

Kita memiliki daftar kata-kata, seperti "running", "easily", "bought", "crying", dan "leaves". Melalui iterasi menggunakan list comprehension, setiap kata diubah menjadi bentuk dasarnya menggunakan metode stem() dari objek PorterStemmer. Hasilnya adalah daftar kata-kata yang telah di-stem menjadi bentuk dasarnya.

Hasil dari potongan kode di atas adalah berikut.

Output:

['run', 'easili', 'bought', 'cri', 'leav']



Lemmatization:

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
 
lemmatizer = WordNetLemmatizer()
words = ["running", "easily", "bought", "crying", "leaves"]
 
lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]
print(lemmatized_words)

Potongan kode di atas adalah contoh penggunaan WordNetLemmatizer dari NLTK untuk mendapatkan bentuk kata baku (lematisasi) dari kata-kata yang diberikan. Dalam contoh ini, kita menggunakan WordNetLemmatizer untuk melakukan lematisasi pada beberapa kata.

Setelah mengimpor WordNetLemmatizer dari modul NLTK, kita mengunduh korpus WordNet menggunakan perintah nltk.download('wordnet'). Korpus WordNet adalah sumber daya leksikal besar dalam bahasa Inggris yang digunakan untuk lematisasi dan penentuan kata-kata.

Hasil dari potongan kode di atas adalah berikut.

Output:

['run', 'easily', 'buy', 'cry', 'leave']

Kemudian, kita mendefinisikan daftar kata-kata, seperti "running", "easily", "bought", "crying", dan "leaves". Melalui iterasi menggunakan list comprehension, setiap kata diubah menjadi bentuk kata baku (lematisasi) menggunakan metode lemmatize() dari objek WordNetLemmatizer. Argumen pos yang diberikan adalah wordnet.VERB menunjukkan bahwa kata-kata yang diberikan adalah kata kerja (verb).

Lematisasi berbeda dengan stemming. Lematisasi memperhitungkan konteks dan memerlukan penentuan jenis kata (pos tag) sehingga hasil lematisasi lebih akurat serta umumnya lebih mudah dipahami.




Ekstraksi Fitur:

Ekstraksi fitur ini kayak kita mengekstrak 'keistimewaan' dari data teks kita. Misalnya, kita punya teks panjang tentang makanan, kita bisa 'mengekstrak' fitur-fitur penting, kayak 'resep', 'bahan-bahan', atau 'ulasan' dari teks itu."

"Jadi kayak kita 'mempercepat' proses pembacaan mesin untuk menemukan apa yang penting dalam teks?"

"Betul! Ini kayak kita bikin highlight untuk teks. Jadi mesin bisa langsung fokus ke bagian-bagian yang penting," jelas Diana sambil memberi analogi.

"Oh, jadi kayak memberi petunjuk ke mesin tentang apa yang harus diperhatikan dalam teks,"

"Exactly! Nah, ada beberapa cara kita bisa ekstrak fitur dari teks. Salah satunya adalah dengan Bag-of-Words atau BoW," 

"BoW itu kayak kita 'menghitung' berapa kali setiap kata muncul dalam teks, tanpa memperhatikan urutan kata-kata itu."

"Betul sekali! Terus ada juga TF-IDF, yang memberi bobot pada kata-kata berdasarkan seberapa pentingnya di suatu dokumen, tapi jarang muncul di dokumen lain,"

"Oh, jadi kayak memberi nilai ke kata-kata yang lebih 'langka', tapi penting, gitu?"


Ekstraksi fitur adalah proses menyaring informasi tersebut menjadi bentuk yang lebih ringkas dan bermakna bagi model kita. 

Dengan cara ini, kita bisa mengubah teks kompleks menjadi representasi numerik yang dapat dimengerti oleh algoritma machine learning


Ekstraksi fitur pada teks adalah kunci untuk mengubah teks menjadi bentuk yang dapat dipahami oleh algoritma machine learning, yaitu numerik. Saat kita berurusan dengan teks, seperti ulasan pelanggan, artikel berita, atau dokumen bisnis, kita perlu mengubahnya menjadi representasi numerik yang dapat diinterpretasikan oleh komputer. Tujuannya tidak hanya memproses teks, tetapi juga untuk mengekstrak informasi berharga yang tersembunyi di dalamnya. 


Dengan ekstraksi fitur teks, kita dapat mengidentifikasi pola, tema, atau sentimen untuk membangun model prediksi atau klasifikasi yang cerdas. Proses ini memungkinkan komputer untuk "memahami" teks sehingga dapat membuat keputusan atau memberikan wawasan berdasarkan data teks yang diberikan. Mari jelajahi lebih dalam bahwa ekstraksi fitur teks dapat mengubah cara kita berinteraksi dengan informasi yang ditemukan pada teks sehari-hari.

Berikut adalah beberapa teknik umum yang digunakan untuk ekstraksi fitur pada teks.

- Word Embedding
- Term Frequency-Inverse Document Frequency (TF-IDF)
- Bag of Words (BoW)
- N-gram
- POS Tagging (Part of Speech Tagging)
- Entity Recognition
- Pola atau Pola Kata (Pattern Matching)


1. Word Embedding:

Word embedding adalah teknik dalam NLP untuk merepresentasikan distribusi kata-kata di ruang vektor. Setiap kata direpresentasikan sebagai vektor numerik berdimensi rendah, yakni setiap dimensi vektor menunjukkan fitur atau atribut tertentu dari kata tersebut. Representasi ini memungkinkan kata-kata dengan makna serupa untuk memiliki vektor yang mendekat secara spasial dalam ruang embedding.

Tujuan utama dari word embedding adalah menangkap hubungan semantik dan sintaktis antarkata dalam teks. Kata-kata yang memiliki makna atau konteks mirip akan cenderung direpresentasikan dengan vektor serupa atau mendekati satu sama lain dalam ruang embedding. Contoh hubungan semantik yang dapat ditangkap oleh word embedding termasuk sinonim, antonim, hubungan hierarkis, dan hubungan konseptual lainnya.

Ada beberapa teknik word embedding yang populer digunakan dalam NLP

a. Word2Vec

Dikembangkan oleh Tomas Mikolov dari Google tahun 2013, Word2Vec adalah salah satu teknik word embedding yang paling terkenal. Word2Vec melatih model neural network untuk memprediksi kata berikutnya berdasarkan kata-kata sekitarnya (skip-gram) atau sebaliknya (continuous bag of words, CBOW). Proses ini menghasilkan vektor representasi kata yang menyandikan makna kata berdasarkan konteks kata tersebut muncul.

Word2Vec memodelkan kata-kata sebagai vektor numerik berdasarkan hubungan kata-kata yang muncul bersama dalam teks. Ide dasarnya adalah kata-kata yang sering muncul bersama dan digunakan dalam konteks serupa akan memiliki representasi vektor serupa. Jadi, vektor kata-kata yang terlatih dengan baik akan menangkap makna semantik dan hubungan antara kata-kata

Ada dua pendekatan utama dalam Word2Vec.

- Skip-gram
Model Word2Vec skip-gram melatih neural network untuk memprediksi kata target berdasarkan kata-kata sekitarnya (konteks) dalam suatu kalimat. Dalam pendekatan ini, model mencoba memprediksi kata tertentu (kata target) berdasarkan kata-kata di sekitarnya (konteks).

Pada gambar ilustrasi CBOW di atas, ada sebuah kalimat "Saya suka makan nasi". Model CBOW akan mengambil kata target "nasi" dan mencoba memprediksi kata-kata di sekitarnya, yaitu "Saya", "suka", dan "makan". Proses ini diulang dengan banyak kalimat berbeda untuk melatih model dan menghasilkan vektor yang merepresentasikan makna kata "nasi".

- Continuous Bag of Words (CBOW)
Pendekatan CBOW adalah kebalikan dari skip-gram. Alih-alih memprediksi kata target berdasarkan context, skip-gram justru berupaya memprediksi kata-kata di sekitar (context) berdasarkan kata tertentu (kata target).

Pada gambar ilustrasi skip-gram di atas, kata target "nasi" menjadi fokus utama. Model skip-gram akan mengambil kata "nasi" dan mencoba memprediksi kata-kata yang sering muncul di sekitarnya, seperti "makan", "saya", dan "suka". Proses ini diulang dengan banyak kalimat berbeda untuk melatih model dan menghasilkan vektor yang merepresentasikan makna kata "nasi".

Hasil dari Word2Vec adalah representasi vektor kata yang menyandikan makna kata berdasarkan konteks kata-kata tersebut muncul. Kata-kata dengan makna serupa akan memiliki vektor yang mendekat secara spasial dalam ruang vektor Word2Vec.


b. GloVe (Global Vectors for Word Representation)

GloVe adalah metode word embedding untuk menghasilkan vektor representasi kata-kata berdasarkan statistik dari matriks frekuensi kemunculan kata dalam teks (co-occurrence matrix). Ide utama di balik GloVe adalah memahami hubungan antar kata berdasarkan seberapa sering kata-kata tersebut muncul bersama-sama dalam korpus teks.

Proses GloVe melibatkan optimasi sebuah fungsi objektif yang menyesuaikan dengan vektor representasi kata-kata sedemikian rupa sehingga menggambarkan hubungan distribusi kata-kata dalam ruang vektor yang optimal. GloVe cenderung lebih fokus pada kem


c. FastText
FastText adalah ekstensi dari Word2Vec yang dikembangkan oleh Facebook AI Research (FAIR). Salah satu keunggulan utama FastText adalah kemampuannya untuk memperhitungkan struktur internal kata (subword information) dalam pembentukan representasi vektor kata. Ini membuat FastText lebih efektif ketika menangani kata-kata yang jarang atau tidak ditemukan dalam kosakata (out-of-vocabulary words).

FastText memecah kata menjadi bagian-bagian lebih kecil, seperti n-grams dan menghitung representasi vektor untuk setiap bagian subword ini. Kemudian, representasi vektor untuk kata diperoleh dengan menggabungkan vektor subword yang membentuk kata tersebut. Pendekatan ini membantu FastText untuk mengatasi variasi morfologi kata-kata dalam teks.


2. Term Frequency-Inverse Document Frequency (TF-IDF)

Korpus adalah kumpulan dokumen yang lebih besar.

TF-IDF (term frequency-inverse document frequency) adalah sebuah metode  pengolahan teks untuk mengevaluasi seberapa penting sebuah kata pada suatu dokumen dalam konteks korpus atau kumpulan dokumen yang lebih besar. 

TF-IDF adalah hasil perkalian antara term frequency (TF) dan inverse document frequency (IDF) untuk sebuah kata t dalam sebuah dokumen d dalam korpus D.

Tujuan utama dari TF-IDF adalah menimbang kata-kata sehingga kata-kata yang sering muncul dalam satu dokumen, tetapi jarang muncul di dokumen lain, dianggap lebih penting dan memiliki bobot lebih tinggi.


a. Term Frequency (TF)

Term frequency mengukur seberapa sering sebuah kata muncul dalam sebuah dokumen. Ini untuk memberikan bobot lebih tinggi pada kata-kata yang sering muncul dan dianggap lebih penting dalam konteks dokumen tersebut. 

TF dari sebuah kata ???? dalam dokumen ???? dapat dihitung dengan rumus berikut. 

           jumlah kata t dalam d
TF(t, d) = ----------------------
           total kata dalam d


Artinya:
- jumlah kata t dalam d adalah jumlah kemunculan kata t dalam
dokumen d.
- total kata dalam d adalah total jumlah kata dalam dokumen d.


b. Inverse Document Frequency (IDF)
Inverse document frequency mengukur seberapa jarang kata tertentu muncul di seluruh dokumen dalam korpus. Kata-kata yang muncul lebih sedikit pada dokumen dianggap memiliki informasi lebih spesifik atau unik dan mendapatkan bobot yang lebih tinggi melalui IDF.

IDF dari sebuah kata ???? di seluruh korpus dokumen D dapat dihitung dengan rumus berikut.
                 
                        total dokumen dalam D
IDF(t, D) = log ( --------------------------------- ) 
                  jumlah dokumen yang mengandung t

Artinya:
- total dokumen dalam korpus D adalah jumlah seluruh dokumen dalam korpus.
- jumlah dokumen yang mengandung kata t adalah jumlah dokumen ???? dalam korpus yang mengandung kata t.


TF-IDF (Term Frequency-Inverse Document Frequency)

TF-IDF adalah hasil perkalian antara term frequency (TF) dan inverse document frequency (IDF) untuk sebuah kata t dalam sebuah dokumen d dalam korpus D.

TF - IDF (t, d, D) = TF (t, d) x IDF (t, D)

Nah, TF-IDF adalah salah satu teknik yang sangat terkenal dalam pemrosesan teks, lo! Apakah Anda ingin mengetahui lebih lanjut mengenai perhitungan matematika di baliknya? Mari kita telusuri!

Langkah-langkahnya sebagai berikut. 

- Misalnya kita memiliki 3 kalimat sederhana.
Kalimat 1: "Saya suka makan nasi goreng"
Kalimat 2: "Nasi goreng adalah makanan favorit saya"
Kalimat 3: "Saya sering makan nasi goreng di pagi hari"

- Buat daftar kata-kata unik (term).
Term = ["Saya", "suka", "makan", "nasi", "goreng", "adalah", "makanan", "favorit", "sering", "di", "pagi", "hari"]

- Hitung Term Frequency (TF).

Kalimat	Kata	Jml Kata dalam Kalimat	Kemunculan Kata		TF

2	Suka	5			1/5			0.2	


- Hitung Document Frequency (DF)

DF dihitung sebagai jumlah kalimat yang mengandung kata tersebut. List kata didapatkan dari array term adalah Term = ["Saya", "suka", "makan", "nasi", "goreng", "adalah", "makanan", "favorit", "sering", "di", "pagi", "hari"].

Kata	DF
Suka	1


- Hitung Inverse Document Frequency (IDF)
                   N
IDF(t, D) = log ( ---- )
                  DF(t)


Artinya:
- N: jumlah kalimat dalam korpus (dalam kasus ini, N = 3).
- DF: document frequency.

Kata	DF	IDF
Suka	1	log(3/1) = 0.477


- Menghitung TF-IDF
Kalikan nilai TF dengan nilai IDF untuk setiap kata dalam setiap kalimat.

Kalimat		Kata	TF	IDF	TF-IDF
1		suka	1/5	0.477	0.0954

Setelah menghitung nilai TF-IDF, kita bisa melihat seberapa penting setiap kata dalam kalimat-kalimat yang diberikan berdasarkan koleksi dokumen tersebut. Ini membantu dalam analisis teks dan ekstraksi informasi dari dokumen.



Bag of Words (BoW)
Bag of Words (BoW) adalah sebuah pendekatan sederhana dalam pemrosesan teks yang digunakan untuk mewakili teks sebagai kumpulan kata-kata terurut tanpa memperhatikan tata urutan atau struktur kalimat. Pendekatan ini melibatkan langkah-langkah berikut.

- Tokenisasi: Teks dibagi menjadi unit-unit yang lebih kecil, seperti kata-kata atau n-gram (gabungan kata-kata berurutan sepanjang n).

n-gram adalah gabungan kata-kata berurutan sepanjang n

- Membuat vocabulary: Setiap kata unik dipetakan dalam teks pada indeks numerik. Ini menciptakan daftar kata yang disebut sebagai "vocabulary" atau "kosakata".

- Representasi vektor: Setiap dokumen direpresentasikan sebagai vektor, yakni setiap elemen vektor mengindikasikan frekuensi kemunculan kata dalam dokumen tersebut. Jadi, setiap dimensi vektor sesuai dengan kata dalam vocabulary dan nilai dalam dimensi tersebut menunjukkan jumlah kemunculan kata pada dokumen.


Langkah-langkah BoW sebagai berikut.

1. Tokenisasi
Kalimat 1: ["saya", "suka", "makan", "nasi", "goreng"]
Kalimat 2: ["nasi", "goreng", "adalah", "makanan", "favorit", "saya"]

2. Membuat Vocabulary
Kata-kata unik dari kedua kalimat: ["saya", "suka", "makan", "nasi", "goreng", "adalah", "makanan", "favorit"]

3. Representasi Vektor
Kalimat 1: [1, 1, 1, 1, 1, 0, 0, 0] # (mengindikasikan frekuensi kata dalam vocabulary)
Kalimat 2: [1, 0, 1, 1, 1, 1, 1, 1]

Di sini, setiap vektor adalah representasi numerik dari dokumen berdasarkan kosakata yang ada. Nilai pada posisi vektor menunjukkan jumlah kemunculan kata yang sesuai dalam dokumen. Vektor ini dapat digunakan sebagai input untuk algoritma pembelajaran mesin pada tugas seperti klasifikasi teks atau analisis sentimen.

Kelemahan dari Bag of Words adalah ia kehilangan informasi struktural dan urutan kata. Namun, pendekatan ini sering digunakan karena sederhana dan efektif untuk banyak aplikasi pemrosesan teks.


N-gram:
N-gram adalah sekumpulan n kata berurutan yang diambil dari sebuah teks atau urutan data. N-gram digunakan pada pemrosesan teks dan pengenalan pola untuk memahami hubungan antar kata dalam teks. Nilai n dalam "n-gram" menunjukkan jumlah kata yang diambil pada satu kali pengambilan. Contoh umum n-gram yaitu unigram (1-gram), bigram (2-gram), trigram (3-gram), dan seterusnya.

1. Unigram (1-gram): Unigram adalah satu kata tunggal yang diambil dalam urutan teks.
Contoh dari kalimat "Saya suka makan nasi goreng"
Unigram: ["Saya", "suka", "makan", "nasi", "goreng"]

2. Bigram (2-gram): Bigram adalah dua kata yang diambil dalam urutan teks.
Contoh dari kalimat yang sama: "Saya suka makan nasi goreng"
Bigram: ["Saya suka", "suka makan", "makan nasi", "nasi goreng"]

3. Trigram (3-gram): Trigram adalah tiga kata yang diambil dalam urutan teks.
Contoh dari kalimat yang sama: "Saya suka makan nasi goreng"
Trigram: ["Saya suka makan", "suka makan nasi", "makan nasi goreng"]



Latihan Ekstraksi Fitur pada Teks:
Ekstraksi fitur pada teks adalah tahap krusial dalam analisis teks, yaitu teks mentah diubah menjadi representasi numerik untuk digunakan oleh algoritma pembelajaran mesin. Tahapan ini menjadi penting karena algoritma pembelajaran mesin membutuhkan representasi numerik dari data untuk melakukan pemrosesan lebih lanjut. 

Dengan menggunakan fitur yang tepat, model pembelajaran mesin cenderung memiliki kinerja lebih baik dalam memprediksi atau mengklasifikasikan teks. Metode ekstraksi fitur yang umum digunakan termasuk Word Embeddings, Term Frequency-Inverse Document Frequency (TF-IDF), Bag-of-Words (BoW), dan N-gram. Melalui teknik-teknik ini, kita dapat mengungkap pola-pola penting dalam teks dan membangun model pembelajaran mesin yang memberikan wawasan berharga.


Word Embedding
Bayangkan Anda lagi main puzzle kata-kata. Anda punya sejumlah kata-kata yang tersusun dalam kalimat-kalimat, tetapi bagaimana cara mesin memahami makna sebenarnya dari kata-kata ini?

Nah, di situlah "word embedding" masuk ke permainan. Hal ini seperti memberi setiap kata-kata tempat spesial dalam ruang matematis yang besar. Bayangkan ruang itu seperti tempat parkir yang luas dan setiap kata punya tempat parkirnya sendiri.

Contohnya, kata-kata yang sering digunakan bersama-sama, seperti "anjing" dan "kucing", parkirnya dekat satu sama lain karena sering muncul dalam konteks yang sama. Sementara kata-kata seperti "meja" atau "laptop" mungkin parkirnya lebih jauh dari mereka karena maknanya berada dalam konteks berbeda.

Kode berikut adalah contoh implementasi Word2Vec menggunakan library Gensim dan NLTK di Python. Mari kita kupas bagian demi bagian.

1. Pertama-tama, kita mengimpor modul yang dibutuhkan.
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

2. Kemudian, kita men-download modul Punkt tokenizer dari NLTK.
nltk.download('punkt')

3. Langkah selanjutnya adalah mendefinisikan contoh data teks yang akan kita gunakan untuk melatih model Word2Vec.
text_data = [
    'Saya suka makan bakso',
    'Bakso enak dan lezat',
    'Makanan favorit saya adalah nasi goreng',
    'Nasi goreng pedas adalah makanan favorit saya',
    'Saya suka makanan manis seperti es krim',
]

4. Setelah itu, kita melakukan tokenisasi pada teks tersebut.
tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]

5. Sekarang, saatnya membangun model Word2Vec menggunakan data teks yang sudah di-tokenisasi.
model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)

Di sini, parameter yang kita gunakan sebagai berikut.
- sentences: Data teks yang telah di-tokenisasi.
- vector_size: Ukuran dari vektor representasi kata. Dalam kasus ini, 100.
- window: Jumlah maksimum kata-kata yang dianggap konteks dekat dalam satu kalimat.
- min_count: Jumlah minimum kemunculan sebuah kata dalam korpus agar kata tersebut diperhitungkan.
- workers: Jumlah thread yang digunakan dalam proses pembangunan model.

6. Setelah model dibangun, kita bisa menggunakan vektor kata untuk kata-kata tertentu atau mencari kata-kata yang mirip dengan kata tertentu.
word_vectors = model.wv
 
similar_words = word_vectors.most_similar('bakso', topn=3)
print("Kata-kata yang mirip dengan 'bakso':", similar_words)
 
vector = word_vectors['bakso']
print("Vektor untuk 'bakso':", vector)

Dalam contoh ini, kita mencari kata-kata yang mirip dengan 'bakso' dan mendapatkan vektor representasinya.

Jadi, dengan menggunakan Word2Vec, kita bisa melatih model untuk membuat representasi vektor dari kata-kata dalam teks yang berguna pada berbagai tugas NLP.

Output-nya berikut.

Kata-kata yang mirip dengan 'bakso': [('manis', 0.2529163062572479), ('nasi', 0.17018672823905945), ('enak', 0.15006466209888458)]


Hasil yang kita dapatkan dari kode tersebut adalah berupa dua hal, yaitu kata-kata yang mirip dengan 'bakso' dan vektor representasi untuk kata 'bakso'.

a. Kata-kata yang mirip dengan 'bakso'
Dari hasil pencarian, kita dapatkan tiga kata yang mirip dengan 'bakso', yaitu 'manis', 'nasi', dan 'enak'. Nilai yang tercantum di samping kata-kata tersebut adalah ukuran seberapa miripnya kata-kata tersebut dengan 'bakso'. Semakin tinggi nilai tersebut, semakin mirip kata-kata tersebut dengan 'bakso'. Jadi, dalam kasus ini, 'manis' adalah kata yang paling mirip dengan 'bakso', diikuti oleh 'nasi' dan 'enak'.

b. Vektor untuk 'bakso'
Vektor ini adalah representasi matematis dari kata 'bakso' dalam ruang vektor. Setiap angka dalam vektor tersebut mewakili fitur atau atribut tertentu dari kata 'bakso'.


Misalnya, nilai pertama mungkin mewakili seberapa sering kata 'bakso' muncul dalam konteks tertentu, nilai kedua mungkin mewakili hubungan kata 'bakso' dengan kata-kata lain, seperti 'makanan' atau 'kuliner' dan seterusnya. Dengan vektor ini, mesin dapat memahami kata 'bakso' secara matematis dan menggunakannya dalam berbagai analisis atau tugas pemrosesan bahasa alami.



Term Frequency-Inverse Document Frequency (TF-IDF):
Di sinilah TF-IDF (Term Frequency-Inverse Document Frequency) masuk permainan. Ini seperti memberi bobot kepada setiap kata dalam dokumen berdasarkan seberapa sering kata itu muncul pada dokumen itu sendiri (frekuensi kata) dan seberapa umumnya kata itu muncul di seluruh kumpulan dokumen.

Misalnya, kata-kata yang muncul banyak dalam satu dokumen, tetapi jarang muncul pada dokumen-dokumen lain mungkin dianggap lebih penting. Contohnya, dalam sebuah artikel tentang bakso, kata "bakso" mungkin muncul berkali-kali, tetapi mungkin jarang muncul pada artikel-artikel yang tidak berkaitan dengan makanan.

Untuk menghitung TF-IDF, kita bisa menggunakan alat seperti TfidfVectorizer pada Python. Ini membantu kita mengukur pentingnya kata-kata dalam setiap dokumen berdasarkan logika yang dibahas tadi. Jadi, dengan TF-IDF, kita bisa menemukan kata-kata yang paling mencirikan atau penting dalam setiap dokumen.

Kode berikut adalah contoh implementasi TF-IDF menggunakan library TfidfVectorizer pada Python. Mari kita kupas bagian demi bagian.

1. Pertama-tama, kita mengimpor modul yang dibutuhkan.
from sklearn.feature_extraction.text import TfidfVectorizer

2. Selanjutnya, kita memiliki data teks contoh yang terdiri dari beberapa dokumen.
documents = [
    "Saya suka makan bakso",
    "Bakso enak dan lezat",
    "Makanan favorit saya adalah nasi goreng",
    "Nasi goreng pedas adalah makanan favorit saya",
    "Saya suka makanan manis seperti es krim",
]

3. Setelah itu, kita inisialisasi objek TfidfVectorizer.
tfidf_vectorizer = TfidfVectorizer()

4. Lalu, kita menghitung TF-IDF dari dokumen-dokumen tersebut.
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

Di sini, TF-IDF (Term Frequency-Inverse Document Frequency) menghitung seberapa sering sebuah kata muncul dalam sebuah dokumen, lalu dibandingkan dengan seberapa sering kata tersebut muncul di seluruh koleksi dokumen. Ini membantu untuk menemukan kata-kata yang penting dalam sebuah dokumen.

5. Setelah itu, kita bisa melihat vocabulary (kata unik) yang dihasilkan oleh TfidfVectorizer.
print("Vocabulary:", tfidf_vectorizer.vocabulary_)

Vocabulary: {'saya': 14, 'suka': 16, 'makan': 9, 'bakso': 1, 'enak': 3, 'dan': 2, 'lezat': 8, 'makanan': 10, 'favorit': 5, 'adalah': 0, 'nasi': 12, 'goreng': 6, 'pedas': 13, 'manis': 11, 'seperti': 15, 'es': 4, 'krim': 7}


6. Terakhir, kita bisa melihat hasil dari TF-IDF matrix dalam bentuk array.
print("TF-IDF Matrix:")
print(tfidf_matrix.toarray())

Dengan TF-IDF Matrix ini, kita bisa melihat kata-kata yang paling penting dalam setiap dokumen berdasarkan konteksnya. Semakin tinggi nilai dalam tabel, semakin penting kata tersebut pada dokumen tersebut.
 


Bag of Words (BoW)
Bag of Words (BoW) adalah pendekatan sederhana dalam pemrosesan teks yang mengubah teks menjadi representasi numerik. Ide dasarnya adalah kita menganggap setiap dokumen sebagai "tas" (bag) kata-kata dan hanya peduli tentang keberadaan kata-kata dalam dokumen tersebut, bukan urutan atau konteksnya. Kemudian, untuk setiap dokumen, kita hitung berapa kali setiap kata muncul. 

Hasilnya adalah matriks, yakni setiap baris mewakili sebuah dokumen dan setiap kolom mewakili kata-kata unik dalam seluruh kumpulan dokumen. Dengan cara ini, BoW memungkinkan kita mengukur kemunculan kata-kata dalam teks secara numerik, yang dapat digunakan untuk berbagai analisis teks, yakni klasifikasi dokumen, analisis sentimen, dan banyak lagi. 

1. Import library CountVectorizer dari Scikit-learn yang digunakan untuk mengubah teks menjadi representasi Bag of Words (BoW).
from sklearn.feature_extraction.text import CountVectorizer

2. Data teks berisi beberapa dokumen contoh yang akan diolah.
documents = [
    "Ini adalah contoh dokumen pertama.",
    "Ini adalah dokumen kedua.",
    "Ini adalah dokumen ketiga.",
    "Ini adalah contoh contoh contoh."
]

3. Inisialisasi objek CountVectorizer.
vectorizer = CountVectorizer()

4. Melakukan fitting dan transformasi pada data teks menggunakan CountVectorizer. Proses ini akan menghitung frekuensi kemunculan setiap kata dalam setiap dokumen.
bow_matrix = vectorizer.fit_transform(documents)

5. Setelah transformasi, kita mendapatkan matriks Bag of Words (BoW), yang merupakan representasi numerik dari teks. Matriks ini berisi jumlah kemunculan setiap kata dalam setiap dokumen.
bow_matrix.toarray()

6. Kita juga mendapatkan daftar fitur (kata-kata) yang dihasilkan oleh CountVectorizer.
features = vectorizer.get_feature_names_out()

7. Hasilnya, kita mencetak matriks BoW beserta daftar fitur yang dihasilkan.
print("Matriks BoW:")
print(bow_matrix.toarray())
 
print("\nDaftar Fitur:")
print(features)

Matriks BoW:
[[1 1 1 1 0 0 1]
 [1 0 1 1 1 0 0]
 [1 0 1 1 0 1 0]
 [1 3 0 1 0 0 0]]
 
Daftar Fitur:
['adalah' 'contoh' 'dokumen' 'ini' 'kedua' 'ketiga' 'pertama']

Dengan menggunakan CountVectorizer, kita bisa mengonversi teks menjadi representasi numerik yang dapat diproses lebih lanjut oleh algoritma pembelajaran mesin atau analisis statistik. Ini memungkinkan kita untuk melakukan berbagai analisis dan pemrosesan teks dengan menggunakan teknik-teknik pemrosesan bahasa alami.

Matriks Bag of Words (BoW) adalah representasi numerik dari teks yang menunjukkan jumlah kemunculan setiap kata dalam setiap dokumen. Pada contoh ini, matriks BoW memiliki 4 baris yang mewakili 4 dokumen dan 7 kolom yang mewakili 7 kata unik dalam teks.

Misalnya, dalam dokumen pertama, kata 'adalah', 'contoh', 'dokumen', dan 'ini' muncul masing-masing 1 kali, sementara kata 'kedua', 'ketiga', dan 'pertama' tidak muncul. Daftar fitur adalah daftar kata-kata unik yang diurutkan secara alfabetis.



N-gram:

1. Import library yang diperlukan.
from nltk.util import ngrams

2. Tentukan beberapa kalimat contoh.
sentences = [
    "Saya suka makan bakso enak di warung dekat rumah.",
    "Nasi goreng adalah salah satu makanan favorit saya.",
    "Es krim coklat sangat lezat dan menyegarkan.",
    "Saat hari hujan, saya suka minum teh hangat.",
    "Pemandangan pegunungan di pagi hari sangat indah.",
    "Bola basket adalah olahraga favorit saya sejak kecil."
]

3. Iterasi melalui setiap kalimat dalam daftar kalimat.
for sentence in sentences:

4. Bagi setiap kalimat menjadi kata-kata individu.
words = sentence.split()

5. Buat 1-gram (unigram) dari kata-kata dalam kalimat tersebut.
unigrams = list(ngrams(words, 1))

6. Buat 2-gram (bigram) dari kata-kata dalam kalimat tersebut.
bigrams = list(ngrams(words, 2))

7. Buat 3-gram (trigram) dari kata-kata dalam kalimat tersebut.
trigrams = list(ngrams(words, 3))

8. Cetak hasil untuk setiap kalimat, termasuk unigram, bigram, dan trigram.
print("\nKalimat:", sentence)
print("1-gram:")
for gram in unigrams:
    print(gram)
print("\n2-gram:")
for gram in bigrams:
    print(gram)
print("\n3-gram:")
for gram in trigrams:
    print(gram)

Dengan tahapan ini, kita dapat membagi setiap kalimat menjadi unigram, bigram, dan trigram, serta menampilkan hasilnya. Dengan demikian, kita bisa melihat bahwa kata-kata pada teks tersebut berhubungan satu sama lain dalam berbagai tingkatan.



Binary vs Multiclass vs Multilabel Classification pada Text:

Pertama, binary classification, ini mirip dengan memilih antara dua pilihan: ya atau tidak, 0 atau 1. Di sini, kita mengelompokkan teks dalam dua kategori berbeda, misalnya positif dan negatif, spam dan bukan spam.

Kedua, multiclass classification, ini seperti memilih dari beberapa pilihan yang berbeda. Hal ini seperti saat kita harus memilih satu dari beberapa destinasi liburan favorit. Pada konteks klasifikasi teks, kita mengelompokkan teks dalam lebih dari dua kelas berbeda, seperti genre film atau kategori produk.

Ketiga, multilabel classification, ini serupa dengan memilih lebih dari satu jawaban yang benar. Misalnya, jika diminta untuk memilih hobi, Anda bisa memilih lebih dari satu, seperti membaca, berenang, dan hiking. Pada klasifikasi teks, kita dapat memiliki teks yang termasuk dalam beberapa kategori atau label sekaligus.

Jadi, tergantung pada tugas dan kebutuhan, kita bisa memilih salah satu dari ketiga pendekatan ini untuk mengklasifikasikan teks sesuai dengan tujuan. Nah, mari telusuri lebih dalam tentang masing-masing pendekatan ini dan momen kita sebaiknya menggunakannya.



1. Binary Classification

Binary classification adalah sebuah teknik dalam machine learning untuk memisahkan data pada dua kelas atau kategori yang saling eksklusif berdasarkan pemberian fitur atau atribut. Dalam binary classification, ada dua kelas yang mungkin untuk diprediksi, biasanya disebut sebagai kelas positif dan kelas negatif. 

Misalnya, dalam kasus analisis sentimen teks, kita mungkin ingin memprediksi bahwa sebuah email adalah "spam" atau "tidak spam". Di sini, "spam" dan "tidak spam" akan menjadi dua kelas dalam binary classification.

Contoh algoritma yang dapat digunakan untuk binary classification, yaitu Logistic Regression, Support Vector Machines (SVM), Decision Trees, Neural Networks, dan lainnya. Pengukuran performa yang umum digunakan untuk evaluasi model binary classification meliputi akurasi (accuracy), presisi (precision), recall, F1-score, dan Area Under the Receiver Operating Characteristic Curve (AUROC).

Dengan demikian, binary classification adalah salah satu tipe dasar dari klasifikasi dalam machine learning yang penting dan banyak digunakan pada berbagai aplikasi, seperti analisis sentimen, deteksi fraud, klasifikasi medis, dan banyak lagi.



2. Multiclass Classification

Multiclass classification adalah jenis masalah klasifikasi dalam machine learning bahwa model harus memprediksi kelas atau label dari data dalam lebih dari dua kategori yang berbeda. Dalam konteks ini, setiap contoh data dapat diklasifikasikan ke salah satu dari beberapa kelas dan setiap kelas mewakili kategori yang berbeda.

Misalnya, pada masalah pengenalan gambar untuk mengklasifikasikan genre film dalam kategori, seperti "Horror", "Action", "Science Fiction", dan "Western", kita memiliki empat kelas yang berbeda. Dalam multiclass classification, model akan belajar untuk mengenali pola atau fitur yang membedakan setiap kategori ini dan mengasosiasikan instance data dengan kelas yang paling sesuai.

Beberapa algoritma yang dapat digunakan untuk multiclass classification, antara lain, Logistic Regression (yang bisa digunakan dalam bentuk One-vs-Rest atau softmax), Support Vector Machines (SVM), Decision Trees, Random Forests, k-Nearest Neighbors (k-NN), dan Neural Networks.

Metrik evaluasi yang umum digunakan untuk mengukur performa model pada multiclass classification, termasuk akurasi, presisi, recall, F1-score, dan matriks konfusi (confusion matrix) dalam melihat seberapa baik model dapat memprediksi setiap kelas. 

Multiclass classification digunakan dalam berbagai aplikasi, yaitu pengenalan gambar, klasifikasi teks, klasifikasi medis, identifikasi objek, dan banyak lagi. 


3. Multi-label Classification

Multi-label classification adalah jenis masalah klasifikasi pada machine learning bahwa setiap instance data dapat dikategorikan dalam lebih dari satu label atau kelas sekaligus. Dengan kata lain, beberapa label atau kategori yang relevan dapat diberikan kepada satu instance data.

Contoh yang umum untuk multi-label classification adalah analisis teks untuk jenis dokumen. Misalnya, untuk sebuah artikel berita, kita ingin mengidentifikasi beberapa topik atau kategori relevan yang dapat digunakan sebagai tag, seperti "Anatomy", "Disease", "Organisms", "Information Science", dan sebagainya. Suatu artikel dapat memiliki beberapa tag atau label yang mencerminkan topik-topik pembahasan di dalamnyaa 


Proses multi-label classification melibatkan pembelajaran dari data latih yang telah dilabeli dengan beberapa label yang benar. Model machine learning akan belajar untuk memahami hubungan fitur-fitur dari data dengan kumpulan label-label yang mungkin relevan.

Dalam multi-label classification, output dari model berupa himpunan (set) dari label-label yang diprediksi untuk setiap instance data. Setiap label bisa hadir atau tidak hadir sebagai bagian dari prediksi. Misalnya, pada artikel berita tertentu, prediksi model bisa berupa himpunan label seperti {"Anatomy", "Disease"} untuk menunjukkan topik-topik yang dianggap relevan.

Algoritma yang dapat digunakan untuk multi-label classification, yakni Binary Relevance, Classifier Chains, Label Powerset, dan lainnya. Algoritma-algoritma ini memungkinkan model untuk mengatasi masalah klasifikasi dengan banyak label dan menghasilkan prediksi yang sesuai dengan kebutuhan.

Metrik evaluasi yang umum digunakan untuk mengukur performa model dalam multi-label classification, antara lain, Hamming Loss, Precision at k (P@k), Average Precision Score, dan lainnya. Evaluasi dilakukan dengan mempertimbangkan seberapa baik model dapat memprediksi himpunan label yang tepat untuk setiap instance data.

Aplikasi dari multi-label classification, yakni analisis teks, pengindeksan konten, klasifikasi gambar, kategorisasi produk, sistem rekomendasi, dan banyak lagi. Pemilihan algoritma serta strategi pendekatan yang tepat sangat tergantung dari sifat data dan tujuan bisnis pada analisis yang dilakukan.

Dengan memahami perbedaan antara binary, multiclass, dan multilabel classification dalam konteks klasifikasi teks, kita sekarang memiliki landasan kuat untuk menjelajahi serta menerapkan pendekatan yang paling sesuai dengan tujuan dan kebutuhan proyek. 

Meskipun ketiganya memiliki pendekatan berbeda, tetapi semuanya memiliki peran penting dalam analisis teks yang mendalam. Mari kita terus eksplorasi serta memanfaatkan berbagai teknik ini untuk memecahkan tantangan yang kompleks dan menarik dalam dunia pemrosesan bahasa alami.





Algoritma RNN (Recurrent Neural Network):


"RNN itu istimewa karena bisa memahami konteks dari urutan data. Misalnya, kalau kita sedang membaca kalimat, RNN bisa mengingat kata-kata sebelumnya untuk memahami kata yang sedang diproses."

Algoritma ini seperti saat kita menemukan kunci yang dapat membuka pintu untuk menjelajahi dunia baru dalam pemrosesan data berurutan. 

Dengan kekuatan dan fleksibilitasnya, RNN memungkinkan kita untuk memahami serta menganalisis teks, waktu, atau audio dengan cara yang lebih mendalam dan intuitif. Bayangkan RNN seperti alur cerita yang terhubung satu sama lain dan memungkinkan kita untuk "mengingat" informasi dari masa lalu ketika memproses input saat ini.

Pemrosesan Data Sekuensial

Pemrosesan data sekuensial adalah bidang ilmu berfokus pada analisis dan pemodelan data dengan keterkaitan antar elemennya, serta urutan data menjadi kunci untuk memahami maknanya. Data sekuensial ini berbeda dengan data tradisional yang statis dan independen satu sama lain.

Contoh data sekuensial yang umum ditemukan sebagai berikut.
- Teks: Urutan kata dalam kalimat, paragraf, dan dokumen.
- Audio: Sinyal suara yang berubah-ubah waktu, seperti pidato, musik, dan efek suara.
- Video: Urutan gambar yang membentuk sebuah video dengan informasi gerak dan perubahan waktu.
- Sinyal sensor: Data yang dikumpulkan dari sensor untuk memantau perubahan lingkungan, seperti suhu, tekanan, dan akselerasi.
- Aktivitas manusia: Urutan tindakan yang dilakukan manusia, seperti pola pergerakan, interaksi sosial, dan aktivitas sehari-hari.

Pemrosesan data sekuensial adalah cara kita memproses atau mengolah data satu per satu secara berurutan, seperti membaca buku dari halaman pertama hingga halaman terakhir. Hal ini berarti kita mengambil satu data pada satu waktu dan melakukan sesuatu dengan data tersebut sebelum melanjutkan ke data berikutnya.

Gagasan tentang pemrosesan data sekuensial mengharuskan pengembangan algoritma yang canggih untuk mengatasi tantangan ini. Salah satu contoh yang terkenal adalah RNN yang akan dipelajari lebih lanjut dalam materi berikutnya. Stay tuned, ya!


Pengenalan RNN

RNN adalah jenis arsitektur deep learning yang dirancang khusus untuk mengolah data sekuensial. Artinya, informasi dari langkah-langkah sebelumnya dalam urutan data memiliki pengaruh pada langkah-langkah selanjutnya. RNN dapat mengatasi masalah bahwa data yang diproses memiliki struktur berurutan atau bergantung pada konteks waktu.

Keunggulan utama RNN adalah kemampuannya untuk menangani urutan input variabel dalam panjang dan mengingat informasi dari langkah-langkah sebelumnya. Ini membuat RNN sangat berguna dalam berbagai tugas pemrosesan bahasa alami, pemodelan waktu, pemrosesan bahasa, dan banyak lagi.


Mekanisme Dasar RNN:
RNN memiliki struktur mirip dengan artificial neural network (ANN), dengan neuron-neuron yang terhubung dalam lapisan. Namun, ada perbedaan utama sebagai berikut.

- Loop internal: RNN memiliki loop internal yang menghubungkan neuron pada layer tersembunyi. Loop ini memungkinkan informasi dari input sebelumnya untuk disimpan dan digunakan dalam pemrosesan input selanjutnya.
- State: RNN memiliki state internal yang diperbarui dengan setiap input baru. State ini mewakili informasi yang "diingat" oleh RNN dari input sebelumnya.

Misalkan RNN memproses sebuah kalimat. Pada kata pertama, RNN menerima input kata tersebut dan memprosesnya. Informasi ini kemudian disimpan dalam memori jangka pendek. Saat kata kedua diterima, RNN tidak hanya memproses kata tersebut, tetapi juga mempertimbangkan informasi dari kata pertama yang disimpan dalam memori. Hal ini memungkinkan RNN untuk memahami konteks kalimat dan menghasilkan output yang lebih akurat.

Selain memori jangka pendek, RNN juga memiliki state. State ini mewakili representasi internal dari urutan input yang telah diproses. State diperbarui dengan setiap input baru, ini memungkinkan RNN untuk mengikuti perkembangan urutan data.

Misalkan RNN memproses deret waktu harga saham. Pada setiap langkah waktu, RNN menerima harga saham saat ini dan memperbaikinya. State RNN akan terus berubah seiring dengan perubahan harga saham. Ini memungkinkan RNN untuk mempelajari pola dan tren dalam deret waktu.


Struktur RNN:
RNN adalah jenis jaringan saraf yang memiliki struktur sedikit berbeda dari jaringan saraf biasa. Dalam RNN, ada lapisan masukan (input layer), lapisan tersembunyi (hidden layer), dan lapisan keluaran (output layer), mirip dengan jaringan saraf lainnya.

Namun, pembedanya dengan RNN adalah adanya koneksi rekuren, yang memungkinkan informasi untuk mengalir dari satu langkah waktu ke langkah waktu berikutnya dalam urutan data. Ini berarti setiap langkah waktu memiliki koneksi ke langkah waktu sebelumnya. Ini memungkinkan RNN untuk "mengingat" informasi dari langkah-langkah sebelumnya dalam urutan tersebut.

Struktur dasar RNN terdiri dari tiga komponen utama.
1. Input
Input ke RNN dapat berupa urutan data apa pun, seperti kata-kata dalam kalimat, sampel dalam sinyal waktu, atau piksel dalam gambar.

2. Recurrent Unit
Jantung dari RNN adalah unit berulang, yakni fungsi non-linear yang memproses input saat ini dan keadaan tersembunyi sebelumnya untuk menghasilkan output dan keadaan tersembunyi baru. Keadaan tersembunyi berisi informasi tentang input yang telah dilihat jaringan sejauh ini dan digunakan untuk memprediksi output di masa depan.

3. Output
Output dari RNN dapat berupa apa pun yang Anda inginkan, seperti kata berikutnya dalam kalimat, nilai berikutnya dalam sinyal waktu, atau deskripsi gambar.


Ciri utama RNN adalah adanya "rekurensi" atau siklus yang memungkinkan informasi untuk dioperasikan berulang kali dalam jaringan. Hal ini memungkinkan RNN untuk mempertimbangkan konteks historis dari urutan data saat ini yang sedang diproses.

Namun, RNN memiliki beberapa tantangan, termasuk kesulitan dalam mempertahankan informasi jangka panjang dan mengatasi masalah gradien yang melemah (vanishing gradient problem) saat melatih model dengan urutan panjang. Untuk mengatasi masalah ini, beberapa varian RNN telah dikembangkan, seperti long short-term memory (LSTM) dan gated recurrent unit (GRU), yang dirancang khusus mengatasi masalah memori jangka panjang dan gradien yang melemah.


A. Long Short-Term Memory (LSTM)
Jaringan LSTM diperkenalkan oleh Sepp Hochreiter dan Jürgen Schmidhuber pada tahun 1997 untuk mengatasi keterbatasan ini. LSTM memiliki struktur yang lebih kompleks dibandingkan dengan RNN tradisional, dengan fitur sel memori dan berbagai mekanisme gating yang mengontrol aliran informasi.

LSTM adalah jenis ANN yang termasuk kategori Recurrent Neural Network (RNN). LSTM didesain khusus untuk mengatasi keterbatasan RNN konvensional dalam menangani data sekuensial dengan ketergantungan jangka panjang.

Keunggulan Utama LSTM:
- Memiliki memori jangka panjang: LSTM mampu menyimpan informasi penting dari data sekuensial dalam jangka waktu yang lama, bahkan ketika terdapat celah waktu yang panjang antara data yang relevan.

- Mengatasi masalah vanishing gradient problem: LSTM tidak mudah terpengaruh oleh masalah gradien menghilang yang sering terjadi pada RNN konvensional sehingga memungkinkan LSTM untuk belajar dari data sekuensial yang lebih panjang.

- Lebih fleksibel: LSTM dapat dimodifikasi dengan mudah untuk berbagai aplikasi, seperti terjemahan mesin, pengenalan suara, dan analisis deret waktu.


Struktur LSTM:

LSTM terdiri dari beberapa komponen utama sebagai berikut.

- Sel memori: Menyimpan informasi penting dari data sekuensial.
- Gerbang atau Gate: Mengontrol aliran informasi ke dalam dan luar sel memori. Ada tiga jenis gerbang.
	- Input Gate: Mengatur informasi baru yang akan ditambahkan ke sel memori.
	- Forget Gate: Mengatur informasi yang akan dihapus dari sel memori.
	- Output Gate: Mengatur informasi yang akan dikeluarkan dari sel memori.


Berbicara terkait jenis-jenis gate pada LSTM, berikut penjelasan lebih lengkapnya.

- Forget Gate:
Forget gate adalah gerbang yang memutuskan seberapa banyak informasi dari memori sel sebelumnya harus dilupakan. Nilai forget gate berkisar antara 0 dan 1, 0 berarti semua informasi akan dihapus dan 1 berarti tidak ada informasi yang dihapus.

Bayangkan Anda memiliki kotak memori untuk menyimpan informasi penting. Kotak ini memiliki kapasitas terbatas, jadi Anda tidak bisa menyimpan semuanya. Di sinilah forget gate berperan, seperti petugas kebersihan khusus untuk kotak memori Anda.

Forget gate menerima dua input.
- Input saat ini ????(????), yang dapat berupa input baru dalam urutan atau data apa pun yang masuk ke LSTM pada langkah waktu ????.

- Output dari sel LSTM sebelumnya ht−1, disebut juga sebagai hidden state pada langkah waktu sebelumnya.

Rumus Forget Gate LSTM:

- Ft adalah output dari forget gate pada langkah waktu t.
- o- adalah fungsi sigmoid, yang menghasilkan nilai antara 0 dan 1.
- Wf adalah matriks bobot untuk forget gate.
- ht−1 adalah output (hidden state) dari langkah waktu sebelumnya.
- xt adalah input pada langkah waktu t.
- bf adalah bias untuk forget gate.

Operasi [ht−1, xt] menunjukkan penggabungan (concatenation) output dari langkah waktu sebelumnya dan input pada langkah waktu saat ini. Output operasi ini adalah nilai biner yang menentukan seberapa banyak informasi dari sel LSTM sebelumnya akan dilupakan. Nilai yang mendekati 1 menunjukkan bahwa informasi tersebut harus diingat, sementara nilai yang mendekati 0 menunjukkan bahwa informasi tersebut harus dilupakan.


- Input Gate
Input gate dalam LSTM adalah komponen yang bertanggung jawab untuk mengatur seberapa banyak informasi baru akan ditambahkan ke memori jangka panjang sel LSTM. Input gate memutuskan seberapa pentingnya setiap bagian informasi baru yang masuk dan berkontribusi terhadap pembaruan memori sel.

LSTM itu seperti menghadiri pesta dengan banyak orang yang ingin berbagi informasi (data). Input gate berperan sebagai penjaga selektif untuk mengatur siapa saja yang boleh masuk dan berinteraksi dengan Anda (sel memori) di pesta tersebut.

Ada 2 langkah yang dilakukan oleh input gate.
- Input gate menggunakan sigmoid untuk menentukan pentingnya setiap elemen dalam input baru; mendekati 1 untuk elemen penting dan mendekati 0 untuk yang tidak penting. 
- Kemudian, nilai ini digunakan untuk mengalikan kandidat nilai baru yang dihasilkan oleh fungsi tanh dan menghasilkan usulan informasi baru untuk ditambahkan ke memori sel LSTM.

Rumus:

- ft adalah output dari input gate pada langkah waktu t.
- o- adalah fungsi sigmoid.
- Wf adalah matriks bobot untuk input gate.
- ht−1 adalah output (hidden state) dari langkah waktu sebelumnya.
- xt adalah input pada langkah waktu t.
- bf adalah bias untuk input gate. 

 Dengan menggunakan xt  dan ht−1, LSTM dapat mempertimbangkan informasi baru dan informasi yang telah disimpan sebelumnya saat membuat keputusan tentang hal yang harus dilakukan selanjutnya, seperti hal yang harus diingat, dilupakan, atau diperbarui dalam memori jangka panjang.


- Output Gate
Output gate dalam LSTM bertanggung jawab untuk mengontrol seberapa banyak informasi dari memori sel yang akan dikeluarkan sebagai output pada langkah waktu tertentu. Hal ini memungkinkan LSTM untuk memilih informasi paling relevan dan membatasi informasi kurang penting atau tidak relevan yang akan disampaikan ke langkah waktu berikutnya atau lapisan keluaran (output layer) dari model. 

Secara matematis, output gate dihitung menggunakan rumus berikut.

- Ot adalah output dari output gate pada langkah waktu t.
- o- adalah fungsi sigmoid.
- Wo adalah matriks bobot untuk output gate.
- Ht−1 adalah output (hidden state) dari langkah waktu sebelumnya.
- xt adalah input pada langkah waktu t.
- bo adalah bias untuk output gate.


Output gate menghasilkan nilai antara 0 dan 1 untuk setiap elemen dalam output, menentukan seberapa banyak informasi dari memori sel yang akan dikeluarkan. Nilai mendekati 1 menunjukkan elemen yang sangat penting dan harus dikeluarkan, sedangkan nilai yang mendekati 0 menunjukkan elemen kurang penting dan bisa diabaikan.


Gated Recurrent Unit (GRU):

GRU adalah jenis unit pada RNN yang dikembangkan untuk memodelkan ketergantungan jarak jauh dalam data urutan dengan cara yang lebih efisien daripada model sebelumnya, seperti unit LSTM. GRU diperkenalkan oleh Kyunghyun Cho pada tahun 2014 sebagai alternatif yang lebih sederhana dan komputasional lebih efisien.

Berbeda dengan LSTM yang memiliki tiga gate (forget gate, input gate, dan output gate), GRU hanya memiliki dua gate (update gate dan reset gate). Hal ini membuat struktur GRU menjadi lebih sederhana dan komputasional lebih efisien.

Keunggulan Utama GRU:
1. Komputasi Lebih Ringan: GRU memiliki struktur yang lebih sederhana daripada LSTM sehingga membutuhkan lebih sedikit sumber daya komputasi.
2. Training yang Lebih Cepat: Karena strukturnya yang lebih sederhana, GRU cenderung lebih cepat dalam proses pelatihan dibandingkan dengan LSTM.
3. Pencegahan Vanishing Gradient: GRU memiliki mekanisme update yang memungkinkannya mengatasi masalah vanishing gradient, meskipun tidak sekompleks LSTM.
4. Penghematan Memori: GRU memerlukan lebih sedikit parameter daripada LSTM, ini dapat menghemat memori saat melakukan pelatihan dan inferensi pada model.

Struktur GRU:
GRU terdiri dari beberapa komponen utama sebagai berikut.

- Sel memori: Menyimpan informasi penting dari data sekuensial.
- Gerbang atau Gate: Mengontrol aliran informasi ke dalam dan luar dari sel memori. Ada dua jenis gate.

	- Reset gate: Mengontrol seberapa banyak informasi dari status tersembunyi sebelumnya yang akan disimpan.
	- Update Gate: Mengontrol seberapa banyak informasi baru dari masukan saat ini yang akan ditambahkan ke hidden state

Berbicara terkait jenis-jenis gate pada GRU, berikut penjelasan lebih lengkapnya.

1. Reset Gate
Reset gate, diwakili oleh rt, memutuskan seberapa banyak informasi dari masa lalu yang akan diabaikan atau dilupakan dalam komputasi saat ini. Inputnya terdiri dari output dari langkah waktu sebelumnya, ht−1, dan input saat ini, xt 

Bayangkan Anda sedang belajar di sebuah ruangan. Setiap kali Anda memasuki ruangan untuk belajar, Anda melihatnya dalam keadaan kosong. Namun, sebelum mulai belajar, Anda melakukan pengecekan cepat untuk melihat adakah informasi penting dari sesi belajar sebelumnya yang harus diingat.

Jika tidak ada yang penting, Anda "mengatur ulang" ruangan itu dengan menghapus semua yang tidak relevan. Jika ada sesuatu yang masih relevan atau penting untuk diingat, Anda membiarkannya tetap ada. 

Persamaan untuk reset gate adalah berikut.
- Wr adalah matriks bobot,
- ht−1 adalah output sebelumnya,
- xt adalah input saat ini,
- br adalah bias, dan
- o- adalah fungsi sigmoid.


Update gate menentukan banyak informasi dari output terakhir yang akan diteruskan ke output saat ini. Output dari reset gate adalah vektor nilai antara 0 dan 1, yang menentukan seberapa banyak informasi dari masa lalu yang harus dilupakan atau diabaikan dalam komputasi saat ini.


2. Update Gate
Update gate, diwakili oleh zt, menentukan seberapa banyak informasi dari output terakhir yang akan diteruskan ke output saat ini. Inputnya juga terdiri dari output pada langkah waktu sebelumnya, ht−1, dan input saat ini, xt. 

Sekarang, bayangkan bahwa Anda belajar di ruangan yang memiliki papan tulis. Setiap kali belajar, Anda menulis informasi baru di papan tulis tersebut. Namun, sebelum menulis informasi baru, Anda memutuskan bahwa informasi yang telah ada di papan tulis masih relevan atau perlu diperbarui. 

Jika informasi sebelumnya masih relevan, Anda membiarkannya tetap ada. Namun, jika informasi sebelumnya tidak lagi relevan atau perlu diperbarui dengan informasi baru, Anda menghapusnya sebelum menulis yang baruu.

Persamaan untuk update gate sebagai berikut.
Wz adalah matriks bobot,
ht−1 dan xt adalah output dan input,
bz adalah bias, dan
o- adalah fungsi sigmoid.

Output dari update gate adalah vektor nilai antara 0 dan 1 dan menentukan seberapa banyak informasi dari output terakhir yang akan diteruskan ke output saat ini. Ketika update gate mendekati 1, itu menunjukkan bahwa model harus mengingat informasi saat ini dengan sangat baik. Ketika update gate mendekati 0, itu menunjukkan bahwa model harus memperbarui informasi lama dengan yang baru.


Contoh Pengaplikasian RNN
Berikut adalah beberapa contoh pengaplikasian RNN.

Natural language processing (NLP): Terjemahan bahasa, analisis sentimen, dll.
Pengenalan suara: Pengenalan ucapan, generasi ucapan, dll.
Pengenalan gambar: Klasifikasi gambar, deskripsi gambar, dll.
Time series: Prediksi harga saham, deteksi anomali, dll.
Text generation: Pembuatan cerita, penyusunan lagu, dll.
Robotika: Kontrol gerakan robot.
Biomedis: Pendeteksian penyakit berdasarkan data medis.



Studi Kasus Implementasi Klasifikasi Teks pada NLP: Sentimen Analisis Review APK Play Store

Sentimen Analisis Review APK Play Store adalah proses menganalisis dan mengevaluasi sentimen atau perasaan yang diungkapkan oleh pengguna dalam ulasan atau review mereka di Play Store, platform distribusi aplikasi Android resmi dari Google. 

Tujuannya adalah memahami bahwa ulasan-ulasan tersebut cenderung positif, negatif, atau netral terhadap suatu aplikasi. Analisis sentimen dapat dilakukan dengan menggunakan berbagai teknik pemrosesan bahasa alami dan alat analisis data untuk mengidentifikasi pola-pola dalam teks ulasan, seperti kata-kata kunci, nada, atau konteks kalimat, yang mengindikasikan evaluasi positif atau negatif.

Proses ini penting bagi pengembang aplikasi untuk memahami umpan balik pengguna mereka, mengidentifikasi area-area yang perlu diperbaiki, dan menanggapi masalah-masalah yang mungkin muncul. Dengan memahami sentimen pengguna, pengembang dapat mengambil tindakan untuk meningkatkan kualitas dan kepuasan terhadap aplikasi mereka.

Analisis sentimen adalah salah satu cabang penting dalam pemrosesan bahasa alami yang memungkinkan kita untuk secara otomatis mengidentifikasi apakah ulasan pengguna cenderung positif atau negatif. Dengan demikian, proyek ini akan membantu kita dalam memahami umpan balik pengguna dengan lebih baik, dan dapat digunakan untuk mengambil keputusan yang lebih baik dalam pengembangan dan perbaikan aplikasi.

Dalam perjalanan ini, kita akan menggali konsep-konsep penting dalam analisis sentimen, seperti pengolahan teks, ekstraksi fitur, dan penggunaan model machine learning untuk memprediksi sentimen ulasan. Kami akan menggunakan bahasa pemrograman Python dan beberapa pustaka populer seperti NLTK, scikit-learn, dan TensorFlow.

from google_play_scraper import app, reviews, Sort, reviews_all
 
import pandas as pd
pd.options.mode.chained_assignment = None
import numpy as np
seed = 0
np.random.seed(seed)
import matplotlib.pyplot as plt
import seaborn as sns 
 
import datetime as dt 
import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
 
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
 
from wordcloud import WordCloud 
import nltk 


Scraping Dataset
Scraping data adalah cara untuk mengambil informasi dari halaman web dengan otomatis. Hal ini seperti menyapu (scrape) data dari sebuah situs web, mirip seperti cara Anda mengambil informasi dari buku atau majalah dengan membacanya sekilas.

Ketika ingin menganalisis ulasan atau pendapat orang tentang sebuah aplikasi di Google Play Store, kita bisa menggunakan teknik scraping untuk mengumpulkan ulasan-ulasan tersebut secara otomatis. Ini memungkinkan kita untuk memiliki banyak data yang bisa dianalisis lebih lanjut.

Ketika mengambil data dari Google Play Store atau situs web lainnya, kita harus berhati-hati untuk tidak melanggar peraturan. Kadang-kadang, situs web memiliki aturan yang melarang pengambilan data secara otomatis. Jadi, pastikan untuk membaca dan mengikuti aturan-aturan tersebut.

from google_play_scraper import app, reviews_all, Sort

scrapreview = reviews_all(
    'com.byu.id',          # ID aplikasi
    lang='id',             # Bahasa ulasan (default: 'en')
    country='id',          # Negara (default: 'us')
    sort=Sort.MOST_RELEVANT, # Urutan ulasan (default: Sort.MOST_RELEVANT)
    count=1000             # Jumlah maksimum ulasan yang ingin diambil
)

Kode di atas kita gunakan untuk mengambil semua ulasan dari sebuah aplikasi di Google Play Store dengan ID 'com.byu.id'. Kita menggunakan pustaka google_play_scraper untuk mengakses ulasan dan informasi aplikasi dari Google Play Store. Dalam kode ini, kita menggunakan fungsi reviews_all() untuk mengambil semua ulasan dari aplikasi tersebut. Proses ini mungkin memerlukan beberapa saat untuk menyelesaikan tugasnya, terutama jika ada banyak ulasan yang perlu diambil. 

Anda juga dapat menyesuaikan bahasa ulasan (dalam contoh ini, kita menggunakan bahasa Indonesia) dan negara (dalam contoh ini, kita menggunakan Indonesia) sesuai  dengan kebutuhan. Pengguna juga dapat menentukan jumlah maksimum ulasan yang ingin diambil (dalam contoh ini, 1000 ulasan). Ulasan akan diurutkan berdasarkan relevansi, ini berarti ulasan yang dianggap paling relevan akan ditampilkan terlebih dahulu.

import csv
 
with open('ulasan_aplikasi.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Review'])  # Menulis header kolom
    for review in scrapreview:
        writer.writerow([review['content']])  # Menulis konten ulasan ke dalam file CSV

Kode ini untuk menyimpan ulasan sebuah aplikasi dalam file CSV. Pertama, kita membuka file CSV baru untuk penulisan. Kemudian, kita menambahkan header kolom "Review". Selanjutnya, kita iterasi melalui setiap ulasan dalam variabel scrapreview dan menulis konten ulasan dalam file CSV. Jadi, file CSV akan berisi semua ulasan aplikasi dengan struktur yang sesuai.

Setelah menjalankan kode tersebut, dataset ulasan aplikasi akan muncul di bagian kiri Google Colab Anda. Anda dapat mengklik pada nama file 'ulasan_aplikasi.csv' pada panel file untuk melihatnya atau melakukan operasi lain, seperti mengunduhnya atau mengunggahnya ke penyimpanan cloud.

app_reviews_df.to_csv('ulasan_aplikasi.csv', index=False)
app_reviews_df = pd.DataFrame(scrapreview)

Kode ini untuk mengambil ulasan aplikasi menggunakan scrapreview dan menyimpannya dalam sebuah DataFrame. Setelah itu, kita menampilkan jumlah baris dan kolom dalam dataset serta lima baris pertama untuk tinjauan cepat. Akhirnya, dataset disimpan dengan format CSV dan nama 'ulasan_aplikasi.csv' untuk digunakan dalam analisis selanjutnya.

Info dataset ini memberikan gambaran komprehensif tentang struktur dan konten dari DataFrame app_reviews_df dengan total 199 baris dan 11 kolom. Kolom-kolom tersebut meliputi reviewId, userName, userImage, content, score, thumbsUpCount, reviewCreatedVersion, at, replyContent, repliedAt, dan appVersion. Jenis data dari kolom-kolom ini bervariasi, termasuk tipe data objek (string), integer, dan datetime.

Ada beberapa kolom dengan nilai null, khususnya replyContent dan repliedAt, yang menunjukkan bahwa tidak semua ulasan memiliki balasan dari pengembang aplikasi. Dengan demikian, info dataset ini memberikan pemahaman yang baik tentang struktur dan keberadaan data sehingga memudahkan kita dalam analisis selanjutnya.

clean_df = app_reviews_df.dropna()

Kode ini membuat DataFrame baru disebut clean_df dari DataFrame app_reviews_df dengan menghapus baris dengan nilai yang hilang (NaN). Metode dropna() digunakan untuk menghapus baris dengan setidaknya satu nilai yang hilang. Dengan menggunakan langkah ini, kita dapat membersihkan dataset dari baris yang tidak lengkap sehingga memastikan keberlangsungan analisis data untuk lebih konsisten.



Preprocessing Text;
Langkah-langkah preprocessing ini bertujuan menghilangkan noise, mengonversi teks ke format yang konsisten, serta mengekstraksi fitur-fitur penting untuk analisis lebih lanjut.

Fungsi-fungsi yang disediakan akan sangat berguna untuk langkah-langkah preprocessing teks. Mari kita jelaskan masing-masing fungsi dengan lebih singkat.


import re
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from Sastrawi.Stemmer.Factory import StemmerFactory


1. cleaningText(text): Membersihkan teks dengan menghapus mention, hashtag, RT (retweet), tautan (link), angka, dan tanda baca. Selain itu, karakter newline diganti dengan spasi dan spasi ekstra pada awal dan akhir teks dihapus.

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', '', text) # menghapus karakter selain huruf dan angka
 
    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

2. casefoldingText(text): Mengonversi semua karakter dalam teks menjadi huruf kecil (lowercase) untuk membuat teks menjadi seragam.

def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text


3. tokenizingText(text): Memecah teks menjadi daftar kata atau token. Ini membantu dalam mengurai teks menjadi komponen-komponen dasar untuk analisis lebih lanjut.

def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

4. filteringText(text): Menghapus kata-kata berhenti (stopwords) dalam teks. Daftar kata-kata berhenti telah diperbarui dengan beberapa kata tambahan.

def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

5. stemmingText(text): Menerapkan stemming pada teks, yakni mengurangi kata-kata menjadi bentuk dasarnya. Anda menggunakan pustaka Sastrawi untuk melakukan stemming dalam bahasa Indonesia.

def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
 
    # Memecah teks menjadi daftar kata
    words = text.split()
 
    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]
 
    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)
 
    return stemmed_text

6. toSentence(list_words): Menggabungkan daftar kata-kata menjadi sebuah kalimat.

def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

Dengan menggunakan fungsi-fungsi ini, Anda dapat membersihkan, memproses, dan mempersiapkan teks sebelum melakukan analisis sentimen atau tugas analisis teks lainnya. Pastikan untuk memanggil fungsi-fungsi ini dengan benar sesuai dengan langkah-langkah preprocessing teks yang Anda butuhkan dalam proyek Anda.

Selanjutnya adalah penghapus kumpulan slang words atau kata-kata informal yang sering digunakan dalam percakapan sehari-hari, terutama pada media sosial atau obrolan online. Setiap kata slang memiliki padanan atau substitusi dengan kata formal atau baku. Misalnya, "abis" merupakan singkatan dari "habis", "wtb" merupakan singkatan dari "beli", dan seterusnya. Kamus ini berguna untuk membantu pemahaman dan interpretasi teks yang menggunakan bahasa informal atau slang.

slangwords = {"@": "di", "abis": "habis", "wtb": "beli", "masi": "masih", "wts": "jual", "wtt": "tukar", "bgt": "banget", "maks": "maksimal" …}
def fix_slangwords(text):
    words = text.split()
    fixed_words = []
 
    for word in words:
        if word.lower() in slangwords:
            fixed_words.append(slangwords[word.lower()])
        else:
            fixed_words.append(word)
 
    fixed_text = ' '.join(fixed_words)
    return fixed_text


Setelah semua langkah preprocessing telah ditetapkan, langkah berikutnya adalah menerapkannya.


clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)

....



Pelabelan:
Sebelum masuk ke tahap pemodelan, langkah yang dilakukan adalah pelabelan. Pelabelan adalah proses pemberian kategori atau label pada setiap entri data berdasarkan informasi yang tersedia. Dalam konteks ini, setiap entri dataset diberikan label sentimen berdasarkan analisis teksnya. Dengan demikian, tahapan pelabelan menjadi dasar untuk proses selanjutnya dalam membangun model klasifikasi sentimen.

from io import StringIO
 
# Membaca data kamus kata-kata positif dari GitHub
lexicon_positive = dict()
 
response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub
 
if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma
 
    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_positive[row[0]] = int(row[1])
        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive
else:
    print("Failed to fetch positive lexicon data")
 
# Membaca data kamus kata-kata negatif dari GitHub
lexicon_negative = dict()
 
response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')
# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub
 
if response.status_code == 200:
    # Jika permintaan berhasil
    reader = csv.reader(StringIO(response.text), delimiter=',')
    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma
 
    for row in reader:
        # Mengulangi setiap baris dalam file CSV
        lexicon_negative[row[0]] = int(row[1])
        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative
else:
    print("Failed to fetch negative lexicon data")



Kode di atas untuk melakukan analisis sentimen pada teks berbahasa Indonesia menggunakan kamus kata-kata positif dan negatif. Pertama, kita mengirim permintaan HTTP untuk mengambil data kamus kata-kata positif dan negatif dari GitHub. 

Jika permintaan berhasil, kita membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma. Kemudian, setiap baris dalam file CSV dibaca dan kata-kata beserta skornya dimasukkan ke kamus yang sesuai (lexicon_positive untuk kata-kata positif dan lexicon_negative untuk kata-kata negatif).

def sentiment_analysis_lexicon_indonesia(text):
    #for word in text:
 
    score = 0
    # Inisialisasi skor sentimen ke 0
 
    for word in text:
        # Mengulangi setiap kata dalam teks
 
        if (word in lexicon_positive):
            score = score + lexicon_positive[word]
            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen
 
    for word in text:
        # Mengulangi setiap kata dalam teks (sekali lagi)
 
        if (word in lexicon_negative):
            score = score + lexicon_negative[word]
            # Jika kata ada dalam kamus negatif, kurangkan skornya dari skor sentimen
 
    polarity=''
    # Inisialisasi variabel polaritas
 
    if (score >= 0):
        polarity = 'positive'
        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif
    elif (score < 0):
        polarity = 'negative'
        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif
 
    # else:
    #     polarity = 'neutral'
    # Ini adalah bagian yang bisa digunakan untuk menentukan polaritas netral jika diperlukan
 
    return score, polarity
    # Mengembalikan skor sentimen dan polaritas teks

Setelah kamus-kamus tersebut terisi, kita mendefinisikan fungsi sentiment_analysis_lexicon_indonesia yang akan menerima teks sebagai input. Dalam fungsi ini, setiap kata pada teks akan diperiksa, ada dalam kamus positif atau negatif. Jika ada, skor sentimen akan ditambahkan atau dikurangkan sesuai dengan skor kata tersebut dalam kamus. Setelah semua kata diperiksa, skor sentimen akan digunakan untuk menentukan polaritas teks, positif atau negatif. 

results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
clean_df['polarity_score'] = results[0]
clean_df['polarity'] = results[1]
print(clean_df['polarity'].value_counts())

Kemudian, skor sentimen dan polaritas dipisahkan dari hasil tersebut menggunakan fungsi zip serta disimpan dalam kolom 'polarity_score' dan 'polarity' secara berturut-turut. Terakhir, jumlah kemunculan setiap polaritas sentimen dicetak dengan value_counts() untuk mengetahui distribusinya.

Pada DataFrame clean_df, ada 423 teks dengan polaritas negatif dan 173 teks dengan polaritas positif. Nah, setelah mendapatkan jumlah pasti dari penyebaran label dataset, selanjutnya kita akan melakukan eksplorasi jumlahnya. Dari hasil analisis, 71% dari teks memiliki polaritas negatif, sementara 29% memiliki polaritas positif. 


Eksplorasi Label:
Teman-teman, bagaimana pendapat Anda terkait visualisasi berikut? Berikut adalah visualisasi dari kata-kata yang sering muncul pada dataset review aplikasi By.U. Visualisasi ini menggunakan WorldCloud. WordCloud adalah representasi visual dari kata-kata yang muncul dalam teks, ketika ukurannya menunjukkan seberapa sering kata tersebut muncul.  

Nah, bagaimana jika Anda menulis pendapat dan analisis pada Forum Diskusi? Mungkin saja kita bisa melihat persepsi teman-teman yang lainnya. Kue ape kue cucur, meluncuuurr!

Di bawah ini, ada tiga visualisasi, yaitu (1) WordCloud secara general; (2) WordCloud untuk Positive Tweets Data; dan (3) Word Cloud untuk Negative Tweets Data. Berikan analisis Anda terhadap tiga gambar tersebut secara lengkap dan komprehensif! Kami tunggu di Forum Diskusi, ya. ????


Data Splitting dan Ekstraksi Fitur dengan TF-IDF
Nah, setelah berhasil menetapkan label untuk setiap entri dalam dataset, sekarang kita dapat melangkah ke tahap berikutnya, yaitu ekstraksi fitur dan pemisahan data. Apakah teman-teman masih ingat tentang ekstraksi fitur yang telah kita pelajari sebelumnya? Pada contoh kali ini, kita akan menggunakan metode ekstraksi fitur yang disebut TF-IDF.


X = clean_df['text_akhir']
y = clean_df['polarity']
 
# Ekstraksi fitur dengan TF-IDF
tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )
X_tfidf = tfidf.fit_transform(X)
 
# Konversi hasil ekstraksi fitur menjadi dataframe
features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())
 
# Menampilkan hasil ekstraksi fitur
features_df
 
# Bagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)


Di sini, data dipisahkan menjadi fitur (tweet) dan label (sentimen). Fitur tweet direpresentasikan oleh variabel X dan label sentimen direpresentasikan oleh variabel y. Kemudian, fitur tersebut diekstraksi menggunakan metode TF-IDF.

Pengaturan spesifik untuk TF-IDF termasuk jumlah fitur maksimum (max_features), frekuensi minimum kata (min_df), dan maksimum dokumen yang memuat kata tersebut (max_df). Hasil ekstraksi fitur kemudian dikonversi menjadi DataFrame untuk memudahkan analisis lebih lanjut. 

Selanjutnya, data dibagi menjadi data latih dan data uji menggunakan train_test_split dengan proporsi data latih sebesar 80% serta data uji sebesar 20%. Tahapan ini akan memungkinkan kita untuk melatih dan menguji model klasifikasi sentimen menggunakan fitur-fitur yang telah diekstraksi.


Modeling:
Setelah proses ekstraksi fitur dan pemisahan data, tahapan terakhir adalah melakukan pemodelan. Pada contoh ini, kita menggunakan empat algoritma yang berbeda: Naive Bayes (NB), Random Forest (RF), Logistic Regression (LR), dan Decision Tree (DT). 

from sklearn.ensemble import RandomForestClassifier
 
# Membuat objek model Random Forest
random_forest = RandomForestClassifier()
 
# Melatih model Random Forest pada data pelatihan
random_forest.fit(X_train.toarray(), y_train)
 
# Prediksi sentimen pada data pelatihan dan data uji
y_pred_train_rf = random_forest.predict(X_train.toarray())
y_pred_test_rf = random_forest.predict(X_test.toarray())
 
# Evaluasi akurasi model Random Forest
accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)
accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)
 
# Menampilkan akurasi
print('Random Forest - accuracy_train:', accuracy_train_rf)
print('Random Forest - accuracy_test:', accuracy_test_rf)

Setelah melatih model dengan data latih, kita mengukur akurasinya menggunakan data uji. Hasilnya berikut: Naive Bayes memiliki akurasi sebesar 83,33%, Logistic Regression sebesar 82,50%, Random Forest sebesar 81,67%, dan Decision Tree sebesar 73,33%. Dengan demikian, kita dapat menentukan bahwa Naive Bayes adalah model terbaik dalam kasus ini, diikuti oleh Logistic Regression, Random Forest, dan Decision Tree.



Rangkuman:

Gated Recurrent Unit (GRU)

GRU adalah varian RNN yang lebih sederhana dan efisien secara komputasi dibandingkan LSTM, dengan hanya dua gate (update gate dan reset gate).

Komponen GRU:
- Reset Gate: Mengontrol seberapa banyak informasi dari status tersembunyi sebelumnya yang akan disimpan. 
- Update Gate: Mengontrol seberapa banyak informasi baru yang akan ditambahkan ke hidden state.


Gunakan sudut pandang seorang yang mahir dan berpengalaman dalam NLP. Berikut adalah yang bukan tantangan dalam pembangunan implementasi NLP, yaitu?
a. Ketergantungan Bahasa (benar)

Apa nama kelas stemmer NLTK yang umumnya dipilih untuk melakukan stemming dalam bahasa selain Bahasa Inggris
a. Rootstemmer
b. Snowballstemmer (betul)
c. linguistikstemmer
d. basestemmer

Berikut adalah yang bukan contoh tentang klasifikasi teks.
A. Mendeteksi bahasa dari sebuah teks.
B. Menerjemahkan sebuah kalimat dari Bahasa Inggris ke Bahasa Italia.
C. Mengklasifikasikan sebuah tweet sebagai mengandung atau tidak mengandung teks bully.
D. Memberi label kepada isu Git sebagai "Usulan Peningkatan". (salah) 


Bagaimana "Named Entity Recognition" (NER) dapat digunakan dalam aplikasi dunia nya
a. Untuk mengidentifikasi dan mengekstraksi informasi penting seperti nama orang, tempat, atau organisasi dari teks.

Apa perbedaan utama antara "stemming" dan "lemmatization" dalam NLP?

A. Stemming mengubah kata ke bentuk dasarnya, sedangkan lemmatization mempertahankan akar kata yang sebenarnya.
B. Stemming lebih kompleks daripada lemmatization.
C. Stemming mempertahankan makna kata, sedangkan lemmatization mengubah kata ke bentuk dasarnya. (salah)
D. Stemming hanya berlaku untuk kata-kata dalam bahasa Inggris, sedangkan lemmatization berlaku untuk semua bahasa.  

---------------------------------------------------------------------------
                                 Time Series
---------------------------------------------------------------------------

Secara singkat time series merupakan penerapan teknik dari ekstrapolasi, sedangkan regression adalah penerapan dari interpolasi.  

Interpolasi dan ekstrapolasi merupakan sebuah teknik statistik yang dapat menebak atau memprediksi suatu nilai data dengan memperhatikan data lain yang kita miliki. Interpolasi adalah sebuah teknik yang dapat mencari suatu nilai variabel pada rentang data yang diketahui, sedangkan ekstrapolasi merupakan teknik menemukan nilai suatu variabel di luar rentang data yang sudah diketahui.
 
Selain itu, regression dapat diterapkan pada data yang tidak terurut di mana variabel target bergantung pada nilai yang diambil oleh variabel lain, masih ingatkan tentang supervised learning? Saat membuat prediksi, nilai-nilai baru dari fitur yang disediakan akan diolah oleh model regression dan memberikan jawaban untuk variabel target. Oleh karena itu, pada dasarnya regression adalah sejenis teknik interpolasi pada ilmu statistik.

Model time series mengacu pada serangkaian data yang terurut berdasarkan suatu variabel. Model ini biasanya dapat meramal/memprediksi apa yang akan terjadi selanjutnya dalam rentang waktu tertentu. Hal ini layaknya kita sedang bermain teka-teki masa kecil di mana kita harus memperkirakan dan mengisi sesuatu pola yang ada. 

Time series dan regresi merupakan metode yang dapat melakukan analisis prediktif, tetapi keduanya memiliki asumsi, teknik, dan implementasi yang berbeda. Secara singkat, time series membutuhkan data yang tersusun dan bergantung pada suatu rentang waktu, sedangkan regresi memiliki data yang independen dan bersifat acak. Selain itu, time series lebih cocok untuk meramalkan dan mendeteksi pola pada data temporal, sedangkan regresi lebih cocok untuk memperkirakan dan menjelaskan pengaruh atau hubungan variabel terhadap suatu hasil.

Waktu adalah salah satu faktor paling penting ketika kita menjalankan suatu bisnis dalam kehidupan nyata di mana pun industri yang sedang dijalani. Jauh sebelum machine learning ramai diperbincangkan, forecasting menggunakan data time series sudah dilakukan menggunakan metode perhitungan bisnis. Hal ini karena forecasting berguna untuk memberikan informasi yang berguna seperti alokasi dana, pendanaan, keputusan tentang cash flow, pembuatan timeline untuk keputusan bisnis, dan lain sebagainya.

Zaman dahulu kala, perusahaan harus mengumpulkan data dan melakukan analisis secara manual untuk mendapatkan pola yang berguna. Secara umum ada dua tipe yang biasanya digunakan ketika melakukan forecasting, yaitu kualitatif dan kuantitatif

Untuk melakukan forecasting tentunya tidak luput dari peran matematika di belakangnya. Setelah praktisi menentukan fitur yang berguna dan diasumsikan penting dalam proses bisnisnya, mereka akan melakukan perhitungan secara manual. Salah satu caranya adalah menggunakan autoregression dengan rumus sebagai berikut.

Tentunya ada banyak sekali yang harus dipelajari mulai dari rumus matematika di atas hingga pengerjaannya yang harus menggunakan konsentrasi penuh. Selain rumus di atas, ada cara lainnya seperti metode moving average, autoregressive moving average, dan lain sebagainya. Bayangkan Anda memiliki banyak sekali data dan perlu melakukan perhitungan manual dengan rumus yang sangat kompleks, tentunya itu sangat melelahkan, bukan?

Machine learning forecasting memiliki beberapa kelebihan salah satunya adalah hasil yang lebih akurat karena dapat terhindar dari kesalahan manusia (human error) dalam melakukan perhitungan.

Namun, dengan menggunakan machine learning tentunya ada trade-off juga. Kekurangan dari machine learning adalah membutuhkan resource yang cukup mahal karena kita harus membeli sebuah perangkat yang dapat memproses banyak data dalam waktu yang singkat. 

Tentunya, semakin mahal sumber daya yang Anda gunakan, semakin cepat juga proses perhitungan yang dilakukan oleh komputer. Sehingga dengan harga yang dikeluarkan akan sebanding atau bahkan lebih banyak benefit yang diterima

Dengan menggunakan machine learning, kita dapat memproses nilai menggunakan algoritma yang dapat mempelajari pola data dan melakukan prediksi dalam rentang waktu tertentu. Machine learning akan memproses data yang Anda miliki menyesuaikan dengan algoritma yang sudah ditentukan. Lalu, bagaimana cara kita menentukan algoritma yang tepat



Tipe-Tipe Time Series:

Secara umum time series terbagi menjadi dua kelompok besar yaitu univariate dan multivariate. Mari kita bahas keduanya secara lebih mendalam.


A. Univariate Time Series
Univariate time series biasanya digunakan ketika Anda ingin membuat suatu nilai prediksi dari satu variabel, terutama ketika ada data historis yang tersedia untuk variabel tersebut. Teknik ini sangat umum dan mendasar karena sering diterapkan secara umum di berbagai bidang seperti ekonomi, keuangan, prakiraan cuaca, prakiraan permintaan dalam manajemen rantai pasokan, dan lain sebagainya.

Sebagai contoh, bayangkan Anda akan mengunjungi sebuah destinasi wisata seperti pantai dengan tujuan untuk rekreasi bersama keluarga. Tentunya, Anda ingin mendapatkan cuaca yang cerah, bukan? Anda dapat melihat histori dari tempat tujuan tersebut dari sebuah website dan mendapatkan data sebagai berikut.

Tentunya, Anda akan kesulitan untuk mengukur dan mengekstrak pola dari tren yang terjadi. Kelebihan dari data yang lebih banyak seperti histori selama satu tahun ke belakang yaitu dapat melakukan prediksi dalam rentang waktu yang lebih jauh dengan hasil yang presisi. Contohnya ketika Anda berencana untuk melakukan libur akhir tahun yang masih lama, tentunya Anda dapat memperkirakan tanggal yang tepat untuk mengambil cuti.

Analisis seperti itu akan membantu kita dalam memprediksi nilai-nilai di masa depan dan menghasilkan visualisasi sebagai berikut.

Apakah Anda memperhatikan bahwa kita hanya menggunakan satu variabel (suhu selama satu tahun terakhir)? Oleh karena itu, ini disebut analisis univariate time series.


B. Multivariate Time Series
Seperti namanya, multivariate time series memiliki lebih dari satu variabel data pada dataset time series yang akan di analisis. Setiap variabel tidak hanya bergantung pada nilai masa lalunya (histori), tetapi juga memiliki ketergantungan atau hubungan dengan variabel lainnya. Hubungan ini digunakan untuk melakukan prediksi atau meramalkan nilai-nilai yang akan datang.

Perhatikan contoh sebelumnya pada kasus univariate, kita hanya menggunakan suhu sebagai data yang akan diprediksi tanpa mempertimbangkan faktor lainnya. Hal ini menyebabkan akurasi yang dihasilkan hanya berfokus pada pola suhu yang terjadi berdasarkan histori yang ada. 

Untuk melakukan forecasting yang lebih kompleks dan banyak variabel atau faktor eksternal yang memiliki dampak teknik, multivariate akan lebih menghasilkan prediksi yang lebih baik. Misalnya, dataset kita mencakup persentase kelembaban, titik embun, kecepatan angin, persentase ketebalan awan, dan lain sebagainya selama satu tahun terakhir seperti di bawah ini

Dalam kasus ini, beberapa variabel harus dipertimbangkan dampaknya agar dapat memprediksi suhu secara optimal sehingga dapat disebut deret seperti ini adalah kategori multivariate time series. Jika divisualisasikan, data di atas kurang lebih akan seperti beriku

Selain univariate dan multivariate, time series juga dapat dibedakan lagi berdasarkan pola datanya. 

a. Pola pertama dikenal sebagai tren di mana data-data pada time series bergerak pada arah yang spesifik seperti tren menaik atau menurun. Gambar di bawah menunjukkan time series yang memiliki pola berupa tren menaik.

b. Pola lainnya disebut time series musiman (seasonal). Pada time series, pola ini terdapat pola yang berulang pada interval tertentu. Contoh time series ini adalah data temperatur setiap harinya. Pada musim dingin temperatur berada di titik terendah; beranjak ke musim hujan temperatur semakin naik; sementara pada musim panas temperatur berada pada posisi tertinggi.

c. Lalu, ada Noisy time series. Time series seperti ini banyak mengandung noise atau data random yang tidak membantu dalam proses analisis. Kenapa demikian? Sederhana saja karena tidak ada pola yang dapat dibaca dan dipelajari dengan pasti dari time series seperti ini.

d. Kemudian ada time series stationary dan non-stationary. Pada time series bertipe stationary, pola-polanya seragam. Contohnya pada grafik Hukum Moore, pola pada time series terus menaik dan tidak pernah turun.

e. Namun, pada time series non stationary, pola-pola pada time series tidak seragam. Contohnya pada data indeks pasar saham DOW di bawah. Perubahan pola tersebut dapat disebabkan oleh suatu skandal atau gelembung ekonomi (economic bubble) yang pecah.


Tentunya memprediksi masa depan bukanlah satu-satunya kasus yang dapat kita selesaikan dengan time series. Implementasi lainnya adalah imputasi, metode yang dapat mengisi nilai-nilai yang kosong pada data. Contohnya sebuah toko kehilangan catatan penjualan pada hari-hari tertentu. Imputasi dapat diterapkan untuk mendapatkan nilai-nilai yang hilang tersebut.


Satu lagi implementasi machine learning pada time series adalah saat menemukan pola-pola yang ada pada data time series. Contoh implementasinya adalah memproses pola-pola pada sebuah sinyal digital otak manusia, untuk melihat apakah seseorang sedang aktif, beristirahat, tidur, atau sedang bermimpi. 




Data Preprocesing untuk Time Series:

Data processing adalah tahapan penting dalam pembangunan model time series forecasting. Tujuan utamanya adalah untuk membersihkan, menyusun, dan mempersiapkan data sehingga model forecasting dapat bekerja dengan baik

- Membersihkan data: data mentah sering kali memiliki noise, nilai yang hilang, atau outlier yang dapat mempengaruhi kualitas prediksi. Dengan melakukan data processing, Anda dapat membersihkan data ini untuk meningkatkan keakuratan model.
- Menyusun data: data time series sering kali memiliki format yang kompleks, seperti tanggal dan waktu, yang perlu diatur dengan benar agar sesuai dengan kebutuhan model. Data processing membantu menyusun data ini menjadi format yang sesuai untuk analisis.
- Menangani nilai yang hilang: kadang-kadang data time series memiliki nilai yang hilang sehingga dapat mengganggu analisis dan prediksi. Data processing memungkinkan Anda untuk menangani nilai yang hilang dengan cara yang sesuai, seperti interpolasi atau pengisian nilai rata-rata.
- Menyederhanakan data: terkadang, data time series dapat memiliki dimensi yang besar atau variabel yang tidak relevan. Dengan melakukan data processing, Anda dapat menyederhanakan data ini untuk fokus pada informasi yang paling penting untuk forecasting
- Normalisasi dan penskalaan: proses normalisasi dan penskalaan dapat membantu memperbaiki performa model, terutama jika fitur-fitur dalam data memiliki skala yang berbeda-beda
- Ekstraksi fitur: data processing dapat melibatkan ekstraksi fitur dari data time series yang dapat membantu meningkatkan kemampuan prediktif model. Misalnya, ekstraksi pola, tren, atau musiman dari data
- Pemilihan fitur: memilih fitur-fitur yang paling relevan dapat membantu meningkatkan kinerja model dan mengurangi kompleksitasnya
- Validasi dan pembagian data: data processing juga melibatkan validasi data untuk memastikan keandalan dan konsistensinya. Selain itu, membagi data menjadi set pelatihan dan pengujian yang sesuai adalah langkah penting dalam proses ini.

Dengan melakukan data processing yang tepat, Anda dapat mempersiapkan data time series dengan baik sehingga model forecasting dapat memberikan hasil prediksi yang akurat dan berguna. Perhatikan gambar di bawah ini sebagai gambaran data yang menerapkan salah satu metode preprocessing.


Teknik di atas disebut dengan normalisasi, hal yang paling umum dilakukan untuk data time series. Normalisasi adalah pengubahan skala data dari rentang asli sehingga semua nilai berada dalam rentang 0 dan 1 atau -1 dan 1.

Normalisasi sangat diperlukan dalam beberapa algoritma machine learning ketika data time series memiliki nilai input dengan skala yang berbeda atau besar. Normalisasi mengharuskan Anda mengetahui atau memperkirakan secara akurat nilai minimum dan maksimum dari dataset. Anda mungkin dapat memperkirakan nilai-nilai ini dari data yang tersedia. Jika data time series Anda memiliki tren naik atau turun kita akan sulit untuk memperkirakan nilai-nilai yang diharapkan dan normalisasi mungkin bukan metode terbaik untuk digunakan pada masalah Anda.

Lalu, jika normalisasi bukan metode terbaik, apa pilihan lainnya? Mari kami perkenalkan dengan rival abadi dari normalisasi yaitu standardisasi! 

Berbeda dengan normalisasi, standardisasi data melibatkan pengubahan skala distribusi nilai sehingga rata-rata nilai yang dianalisis adalah 0 dan memiliki nilai standar deviasi sebesar satu. Dengan kata lain, hal ini dapat dianggap sebagai pengurangan nilai rata-rata atau pemusatan data. (setiap nilai itu dikurangi dengan rata-ratanya)

Seperti normalisasi, standardisasi dapat berguna dan bahkan diperlukan dalam beberapa kasus pembangunan machine learning ketika data time series memiliki nilai input dengan skala yang berbeda. Standardisasi mengasumsikan bahwa analisis yang dilakukan harus sesuai dengan distribusi Gaussian (kurva lonceng) dengan rata-rata dan deviasi standar yang baik. Anda masih dapat menstandardisasi data series jika ekspektasi/target tidak terpenuhi, tetapi mungkin tidak mendapatkan hasil yang dapat diandalkan.


Beberapa model machine learning akan mencapai kinerja yang lebih baik jika data time series yang ada memiliki skala atau distribusi yang konsisten. Dua teknik yang dapat Anda gunakan untuk mengubah skala data deret waktu secara konsisten adalah normalisasi dan standardisasi yang telah Anda pelajari. Jika kita buat sebuah analogi, perbedaan dari kedua metode dapat divisualisasikan sebagai berikut.

Nah, setelah data sudah siap untuk digunakan, sekarang waktunya Anda memasuki tahapan yang lebih dalam. Data yang sudah melewati tahapan-tahapan sebelumnya sudah siap untuk digunakan. Disclaimer bahwa tahapan preprocessing di atas tidak wajib dilakukan semuanya, tergantung karakteristik dan kondisi dataset yang Anda miliki. 

Hal pertama yang perlu kita lakukan setelah data siap adalah membagi data time series menjadi data training dan testing. Pembagian data pada time series dilakukan dengan membaginya menjadi ke dalam 2 periode yakni periode training dan periode validation. Dengan membagi data seperti ini, kita mendapat gambaran prediksi dari model.

For your information, saat membagi data pada time series yang bersifat seasonal, kita perlu membagi sesuai musim yang ada. Contohnya data penjualan bulanan. Untuk periode training dan validation, datanya harus dalam satuan bulanan bukan mingguan atau harian. Misalnya untuk periode validasi, gunakan data dua bulan terakhir, bukan data tiga minggu terakhir. 

Kemudian, sama seperti kasus machine learning pada umumnya, kita harus membagi data kita menjadi atribut dan label. Bagaimana kita melakukannya pada data time series? Pertama, kita mengambil sebagian kecil dari data yang dapat disebut window atau biasa disebut windowed dataset. 

Sebuah window dapat berisi sebanyak N data, atau ukuran window dapat kita tentukan sebesar interval waktu tertentu misalnya satu minggu, empat hari, dan lain sebagainya. Sebelum kita masuk ke kode, mari kita asumsikan memiliki sebuah deret data dan ingin melakukan prediksi dengan menggunakan data tujuh hari sebelumnya untuk memprediksi satu hari selanjutnya seperti berikut.

[0, 1, 2, 3, 4, 5, 6] -> [7]

[1, 2, 3, 4, 5, 6, 7] -> [8]

[2, 3, 4, 5, 6, 7, 8] -> [9]


Sliding Window

Ketika kita menggunakan data yang lebih banyak, tentunya akan sulit untuk melihat pergeseran secara menyeluruh. Dengan menggunakan windowing data sebanyak n akan terus bergeser setiap langkah yang sudah ditentukan atau biasa disebut dengan stride. Stride memiliki nama lain pada time series yaitu window slide. Teknik ini digunakan untuk melakukan analisis data time series dengan membagi data ke dalam window dan memproses setiap window secara independen. Hal ini sangat berguna untuk tugas-tugas seperti forecasting yang mengharuskan Anda untuk memperhitungkan hubungan antara titik-titik data yang berdekatan. 

Seperti yang Anda lihat pada gambar di atas, data yang “dibungkus” menjadi windowed dataset akan bergeser sebanyak x langkah. Tentunya hal tersebut dapat diatur sesuai dengan kebutuhan dan eksplorasi yang Anda lakukan. Jika Anda penasaran proses dari data time series secara utuh, perhatikan gambar berikut.

Gambar di atas bertujuan untuk mengambil sebagian kecil dari data yang dapat berisi sebanyak N data, atau ukuran window dapat kita tentukan sebesar interval waktu tertentu, seperti satu minggu, tiga hari, dan lain sebagainya.

catatan:
- horizon: : jumlah langkah waktu masa depan yang ingin kita prediksi.
- window (Window size): jumlah langkah waktu yang akan kita gunakan untuk memprediksi masa depan (horizon).

Misalnya, kita ingin menentukan ukuran window (window size) sebesar tujuh buah data. Nah, fitur-fitur pada window tersebut adalah setiap nilai yang ada pada window dan labelnya adalah nilai selanjutnya (horizon). Contohnya, kita memiliki tujuh buah data 1,2,3,4,5,6,7. Lalu, ukuran window yang kita pilih adalah tiga. Fitur dan label untuk window tersebut ditampilkan pada tabel di bawah. Perhatikan bahwa fitur 6 dan 7 tidak dimasukkan karena keduanya tidak ada nilai setelah 7.

To be honest, ada dua pendekatan utama untuk melakukan forecasting pada data time series. Sebelumnya kita sudah mempelajari tentang sliding window, sekarang saatnya kita melebarkan sayap dengan mempelajari pendekatan lainnya, yaitu expanding window!

Dalam konteks time series, expanding window merupakan perhitungan statistik yang menghitung rata-rata dari data time series melalui ukuran window yang akan diperluas setiap waktunya. Berbeda dengan sliding window yang menggunakan analisis dengan ukuran tetap, expanding window ini akan terus memperbesar ukuran window mencakup semua analisis hingga titik waktu terakhir. Perhatikan gambar berikut.

Perbedaan sliding windows dan expanding windows:
- Sliding Windows: dalam pendekatan ini window akan bergerak maju dalam data time series dengan interval yang tetap. Setiap kali window bergeser maka data window baru dibuat dengan data yang berikutnya, dan data window lama diabaikan
- Expanding Windows: dalam pendekatan expanding windows, window akan diperluas secara bertahap saat data time series berkembang. Setiap kali window diperluas, ia akan mencakup semua data sebelumnya ditambah satu titik data terbaru

Perbedaan ini dapat diilustrasikan dengan contoh sederhana. Misalkan kita memiliki data time series harian tentang penjualan selama 100 hari.
- Dengan sliding windows, kita mungkin membuat window akan bergeser 30 hari. Jadi, kita akan memiliki 71 window dengan masing-masing window berisi 30 data
- Dengan expanding window, kita mungkin membuat jendela mulai dari data hari pertama hingga data di hari ke 100. Jadi, kita akan memiliki 100 jendela dengan ukuran bertambah setiap harinya

Kedua pendekatan ini memiliki kegunaan dan aplikasi yang berbeda. Sliding windows sering digunakan dalam pemodelan dan prediksi untuk mengamati pola dan tren baru dalam data, sedangkan expanding windows sering digunakan dalam analisis retrospektif untuk menghitung statistik yang berkembang seiring waktu

Namun, pada dunia machine learning tidak ada hal yang pasti sehingga kedua pendekatan ini perlu kita coba juga ya agar mendapatkan pengalaman dan hasil yang maksimal di setiap kasus yang sedang kita hadapi

Tenang, pada materi ini, kita akan lebih fokus menggunakan pendekatan sliding window. Hal ini karena sliding window lebih sering digunakan untuk melakukan forecasting dengan resource atau spesifikasi hardware yang cenderung lebih rendah.

Bayangkan kita memiliki sebuah data DailyDelhiClimateTrain yang dapat memprediksi rata-rata suhu setiap harinya dari tanggal 2013-01-01 hingga 2017-01-01 atau sebanyak 1462 baris. Pada data ini terdapat beberapa fitur, seperti meantemp, humidity, wind_speed, dan meanpressure. Namun, pada contoh preprocessing ini kita akan menggunakan salah satu fitur saja yaitu meantemp. Pertama, mari kita lihat data yang akan kita gunakan. Anda dapat mengunduh data tersebut pada tautan di atas atau menggunakan kode berikut pada Google Colab atau environment yang digunakan.

df = pd.read_csv('https://drive.google.com/uc?id=15RfMD9lNkpS3cVN7j3_dsJKZ8_5RJG5z')
df.head()

Seperti kesepakatan kita sebelumnya, kita hanya akan menggunakan fitur date dan meantemp untuk latihan windowing kali ini. Karena ini merupakan latihan, anggap skala data yang digunakan sudah cukup baik agar tergambar proses windowing yang akan kita lakukan. 

Setelah data siap, mari kita membuat sebuah fungsi yang dapat melakukan proses windowing pada dataset yang sudah ada menggunakan kode berikut.

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)


Kode di atas adalah sebuah fungsi yang dibangun menggunakan TensorFlow untuk membuat dataset yang kita punya menjadi sebuah data window. Mari kita simak penjelasannya dari kode tersebut agar Anda semakin lihai membangun model time series.


1. def windowed_dataset(series, window_size, batch_size, shuffle_buffer):

ini adalah definisi fungsi windowed_dataset yang menerima empat parameter:
- series: data deret waktu.
- window_size: ukuran jendela (window) yang akan digunakan.
- batch_size: ukuran batch.
- shuffle_buffer: ukuran buffer untuk pengacakan (shuffle).

2. series = tf.expand_dims(series, axis=-1):
mengubah dimensi series dengan menambahkan satu dimensi baru di posisi terakhir (axis=-1). Ini biasanya dilakukan untuk menangani data deret waktu yang memiliki satu dimensi (misalnya, satu fitur. Pada kasus ini meantemp).

3. ds = tf.data.Dataset.from_tensor_slices(series):
membuat dataset TensorFlow menjadi sebuah series. Dataset ini berisi slice dari series dimana setiap slice sesuai dengan satu elemen dalam series.

4. ds = ds.window(window_size + 1, shift=1, drop_remainder=True): 
membagi dataset menjadi jendela-jendela berukuran window_size + 1 dengan jarak 1 (shift=1) antara jendela-jendela tersebut. drop_remainder=True menghapus jendela-jendela yang tidak cukup panjang untuk membentuk satu jendela dengan ukuran yang diinginkan.

5. ds = ds.flat_map(lambda w: w.batch(window_size + 1)):
mengubah setiap window menjadi sebuah batch dengan ukuran window_size + 1. Ini dilakukan menggunakan flat_map yang menjalankan fungsi yang diberikan pada setiap elemen dataset dan kemudian menggabungkan hasilnya.

6. ds = ds.shuffle(shuffle_buffer):
mengacak dataset dengan menggunakan buffer ukuran shuffle_buffer. Ini akan mengacak elemen-elemen dataset, yang berguna untuk memperkenalkan variasi dalam pelatihan model.

7. ds = ds.map(lambda w: (w[:-1], w[-1:])):
memetakan setiap batch menjadi sepasang data input dan target. Data input adalah semua elemen dalam batch kecuali elemen terakhir, sedangkan targetnya adalah elemen terakhir. Ini digunakan untuk melatih model untuk memprediksi elemen berikutnya dalam deret waktu berdasarkan elemen-elemen sebelumnya

8. return ds.batch(batch_size).prefetch(1):
mengelompokkan batch-batch dataset menjadi batch dengan ukuran batch_size, dan menambahkan prefetching ke dataset. Prefetching memungkinkan TensorFlow untuk memuat data selanjutnya ke dalam memori ketika model sedang melakukan komputasi sehingga dapat meningkatkan kinerja. Dengan nilai argumen 1, prefetch akan memastikan bahwa satu batch selalu siap untuk diproses oleh model.


Tujuannya untuk melabeli membagi data kita menjadi atribut dan label.

dates = df['date'].values
temp  = df['meantemp'].values

Kode di atas akan mengambil seluruh data dari dataset yang sebelumnya sudah kita siapkan. Selanjutnya, karena kita sudah membuat fungsi windowing, saatnya Anda menggunakan fungsi tersebut dengan menambahkan beberapa parameter seperti berikut.

train_set = windowed_dataset(temp, window_size=2, batch_size=3, shuffle_buffer=1000)

Setelah kita menggunakan fungsi tersebut, akhirnya kita memiliki sebuah data yang sudah dipisahkan oleh window. Jika kita perhatikan, sekarang kita memiliki jumlah data sebanyak 487 window. Dari mana jumlah data ini didapatkan? Mari kita jelaskan satu per satu parameter yang digunakan pada kode di atas.

- Kita menggunakan parameter temp untuk memuat dataset yang sudah kita panggil pada tahap sebelumnya (pada saat mendefine fitur yang akan digunakan).
- Window_size = 2: parameter ini menentukan ukuran jendela (window) yang akan digunakan dalam dataset. Dalam konteks ini, nilai 2 menunjukkan bahwa setiap jendela akan memiliki dua titik data waktu. Misalnya, jika data deret waktu adalah [1, 2, 3, 4, 5], jendela pertama akan berisi [1, 2], jendela kedua akan berisi [2, 3], dan seterusnya.
- batch_size=3: ini adalah ukuran batch yang akan digunakan dalam dataset. Ini menunjukkan bahwa setiap batch akan terdiri dari tiga jendela.
- shuffle_buffer=1000: parameter yang mengontrol proses pengacakan (shuffling) dataset. 1000 elemen pertama akan diambil secara acak dan ditempatkan dalam buffer untuk proses pengacakan. Semakin besar nilai shuffle_buffer, semakin banyak elemen yang akan diambil untuk pengacakan, yang dapat meningkatkan variasi data dalam pelatihan.

Catatan:
Dalam konteks time series, meskipun data telah diurutkan berdasarkan waktu, pengacakan tetap diperlukan. Hal ini karena saat menggunakan jendela-jendela (windows) untuk melatih model, kita ingin model tidak hanya belajar dari urutan yang sama dalam setiap epoch. Pengacakan memastikan bahwa setiap epoch model melihat sampel-sampel dalam urutan yang berbeda, sehingga model dapat mempelajari pola-pola yang lebih umum dan tidak hanya mengingat urutan tertentu. Semakin besar nilai shuffle_buffer, semakin banyak elemen yang akan diambil secara acak, dan semakin besar variasi yang mungkin dihasilkan dalam setiap epoch. Namun, menggunakan nilai yang terlalu besar juga dapat memakan banyak memori karena buffer tersebut harus menampung elemen-elemen dataset.

Sampai di sini, sudah paham kan cara menyiapkan data window? Benar, hal tersebut sangatlah mudah karena kita sudah didukung oleh beberapa fungsi dari TensorFlow yang membantu meringankan pekerjaan kita sebagai machine learning engineer. Namun, tidak valid rasanya jika kita tidak melihat data yang sudah dibagi tersebut agar lebih terbayang. 

for data in train_set:
  print(data)
  break

Kita menggunakan fungsi break pada Python karena data yang kita miliki cukup banyak yaitu 487 data window. Tujuan dari kode di atas sebetulnya bukan tahapan teknikal yang wajib dilakukan sehingga jika kelak Anda membangun sebuah proyek time series, tahapan ini bisa dilewati. Kita menggunakan kode ini untuk melihat secara detail satu window yang telah dibuat dengan output sebagai berikut

(<tf.Tensor: shape=(3, 2, 1), dtype=float64, numpy=
array([[[31.25 ],
        [31.   ]],
 
       [[30.25 ],
        [29.875]],
 
       [[13.75 ],
        [13.375]]])>, <tf.Tensor: shape=(3, 1, 1), dtype=float64, numpy=
array([[[28.75]],
 
       [[30.75]],
 
       [[13.25]]])>)

Seperti yang dapat kita lihat pada output di atas, dataset yang kita miliki berisikan masing-masing dua buah data. Selain itu, karena kita menentukan batch_size dengan nilai tiga, dalam satu batch hanya berisikan tiga buah window. Pada array pertama menunjukkan fitur input, sedangkan array kedua menunjukkan fitur target. Keduanya sangat mudah dimengerti, bukan?

Tentunya seperti yang kita bahas sebelumnya, ukuran tersebut hanyalah contoh. Semakin besar ukuran window dan batch size yang kita atur, semakin kompleks komputasi yang dilakukan. Tentunya hal itu berbanding lurus dengan hasil yang lebih baik. Oleh karena itu, Anda dapat melakukan eksplorasi secara mandiri untuk menentukan ukuran terbaik dari dataset dan proyek yang sedang dibangun.



Metrik Evaluasi untuk Time Series:
Untuk mengevaluasi model time series, kita dapat menghitung error yang dibuat pada model seperti kita mengevaluasi model regresi. Yup, kita bisa mendapatkan error dengan menghitung jarak antara nilai prediksi model dan nilai sebenarnya

Ada beberapa metrik yang dapat dipakai dalam evaluasi model time series, seperti MSE, RMSE, MAE, MAPE, dan lain sebagainya. Namun, hal paling sederhana yang dapat kita lakukan adalah dengan menghitung error secara langsung.

Mari kita asumsikan memiliki sebuah data aktual dan hasil prediksi (forecast) sebagai berikut.

Tentunya dengan segala kemudahan yang ada, metrik evaluasi di atas memiliki sebuah kelemahan yaitu rentan terhadap hasil negatif (minus) dan juga outlier. Lalu, bagaimana dengan metrik evaluasi yang lainnya? Yuk, kita bahas satu per satu.


1. Mean Absolute Error (MAE)
Mean Absolute Error (MAE) berfungsi untuk mengukur seberapa dekat hasil prediksi suatu model dengan nilai sebenarnya. MAE mengukur rata-rata dari selisih absolut antara prediksi model dan nilai sebenarnya dari data yang diamati. Alih-alih membiarkan nilai apa adanya, pada MAE setiap nilai yang diambil sifatnya absolut sehingga nilai negatif hilang. Berikut adalah penjelasan detail tentang Mean Absolute Error.

MAE dihitung dengan menjumlahkan selisih absolut antara setiap prediksi individu dan nilai sebenarnya, kemudian dibagi dengan jumlah total observasi yang dapat direpresentasikan sebagai berikut.

Keterangan:
- n adalah jumlah total observasi atau sampel
- yi adalah nilai aktual atau nilai sebenarnya dari observasi ke-i
- ~yi adalah nilai prediksi model untuk observasi ke-i.

Dengan menggunakan MAE, kita memiliki beberapa keuntungan seperti berikut.
- Interpretasi yang Sederhana: MAE memiliki interpretasi yang sederhana dan mudah dimengerti.
- Robust terhadap Outlier: MAE tidak terlalu sensitif terhadap outlier dalam data karena menggunakan nilai absolut.
- Diferensial: MAE dapat dengan mudah dihitung dan diferensial, membuatnya cocok untuk digunakan dalam proses optimasi.

Semakin rendah nilai MAE, semakin baik pula kinerja model dalam memprediksi nilai target. Mari kita coba implementasikan evaluasi MAE menggunakan data contoh di atas.


2. Mean Square Error (MSE)
Mean Square Error (MSE) adalah salah satu metrik evaluasi yang umum digunakan dalam statistik dan machine learning. Fungsi dari MSE adalah untuk mengukur seberapa baik suatu model dalam memetakan nilai prediksi ke nilai sebenarnya dengan menggunakan kuadrat kesalahan sebagai dasar perhitungan. MSE merupakan metrik yang sangat populer di kalangan machine learning engineer.

Pada MSE, kita memangkatkan seluruh error, kemudian menghitung rata-ratanya. Kenapa dipangkatkan? Karena error yang bernilai negatif dapat diubah menjadi positif. Bayangkan jika kita memiliki error katakanlah 2 dan -2. Jika kita menjumlahkan kedua error tersebut, hasilnya adalah 0 yang menunjukkan bahwa tidak ada error. Dengan memangkatkan 2 dan -2, nilai tersebut menjadi 4 dan 4. Ketika menjumlahkannya, error kita bukan 0 melainkan 8. Berikut rumus yang digunakan untuk menghitung MSE.

Keuntungan menggunakan MSE yaitu memiliki perhitungan derivatif yang mudah. MSE memiliki turunan matematis yang mudah dan membuatnya cocok untuk digunakan dalam proses optimasi.

Namun, MSE memiliki sebuah kekurangan yaitu MSE sangat sensitif terhadap outlier dalam data karena kuadrat kesalahan dapat memberikan bobot yang sangat besar pada nilai yang jauh dari rata-rata.

Semakin rendah nilai MSE, semakin baik pula kinerja model dalam memprediksi nilai target. Mari kita coba implementasikan evaluasi MSE menggunakan data contoh di atas.


3. Root Mean Square Error (RMSE) (akar kuadrat dari MSE)
Root Mean Square Error (RMSE) adalah salah satu metrik evaluasi yang sering digunakan dalam pemodelan statistik dan machine learning, Fungsi dari RMSE adalah untuk mengukur seberapa baik suatu model memetakan nilai prediksi ke nilai sebenarnya dengan menghitung akar kuadrat dari rata-rata dari kuadrat kesalahan. RMSE dihitung dengan mengambil akar kuadrat dari rata-rata dari kuadrat selisih antara setiap prediksi individu dan nilai sebenarnya.


4. Mean Absolute Percentage (MAPE)
Mean Absolute Percentage Error (MAPE) adalah metrik evaluasi yang digunakan untuk mengukur seberapa baik suatu model memprediksi dengan mengevaluasi rata-rata persentase kesalahan absolut dari semua prediksi terhadap nilai sebenarnya. MAPE umumnya digunakan dalam konteks ramalan atau prediksi di mana penting untuk memahami seberapa besar kesalahan relatif dari prediksi terhadap nilai sebenarnya. MAPE dihitung dengan menjumlahkan persentase kesalahan absolut dari setiap prediksi data dan nilai sebenarnya, kemudian dibagi dengan jumlah total observasi.

Namun, dengan menggunakan MAPE, ada sebuah kelemahan yaitu MAPE tidak stabil ketika terdapat nilai sebenarnya yang mendekati nol karena pembaginya adalah nilai sebenarnya. Kesimpulannya, penggunaan MAPE mungkin tidak cocok untuk kasus ketika nilai sebenarnya memiliki variabilitas yang rendah atau mendekati nol.

Nah dalam praktik sesungguhnya, MAPE sering digunakan bersamaan dengan metrik evaluasi lainnya, seperti Mean Absolute Error (MAE), Mean Squared Error (MSE), atau Root Mean Square Error (RMSE) untuk memberikan gambaran yang lebih lengkap tentang kinerja suatu model. MAPE memberikan informasi tentang kesalahan relatif dari prediksi terhadap nilai sebenarnya yang dapat menjadi aspek penting dalam pengambilan keputusan.  

Begitu pula dengan metrik evaluasi yang sebelumnya dipelajari, biasanya kita akan menggunakan beberapa metrik evaluasi untuk mengetahui pola lebih detail dan menjadi gambaran perbandingan antara metrik evaluasi. 

Kini, Anda sudah menguasai beberapa metrik evaluasi yang biasa digunakan dalam pembangunan model time series ataupun regresi. Namun sebetulnya masih banyak lagi metrik evaluasi yang dapat Anda pelajari, seperti R-Squared (R2), Adjusted R-Squared, dan lain sebagainya. 



Latihan: Time Series:

1. Univariate Time Series:
Univariate time series adalah serangkaian data yang direkam dalam interval waktu yang berurutan, di mana setiap titik data memiliki satu variabel yang diamati atau diukur. Ini berarti bahwa dalam analisis univariate time series, kita hanya memperhatikan satu variabel dalam rentang waktu tertentu.

Pada materi sebelumnya, kita telah memahami apa itu time series dan tahapan pemrosesan data time series. Selanjutnya, kita dapat mulai mengimplementasi model machine learning untuk data time series. Untuk latihan kali ini, kita akan menggunakan dataset cuaca kota Delhi pada tahun 2013 sampai 2017. Silakan unduh data tersebut pada tautan Daily Climate Series. Kita akan mengembangkan model yang dapat memprediksi cuaca dengan melanjutkan materi preprocessing.

Pada Google Colab, impor library di bawah.

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

Kemudian ubah dataset menjadi dataframe dengan fungsi read_csv(). Tampilkan 5 data teratas pada dataframe menggunakan fungsi head().

# Load dataset langsung dari Google Drive
df = pd.read_csv('https://drive.google.com/uc?id=15RfMD9lNkpS3cVN7j3_dsJKZ8_5RJG5z')
df.head()
# Load dataset mandiri
data_train = pd.read_csv('DailyDelhiClimateTrain.csv') #Tentukan Path atau lokasi penyimpanan dataset.
data_train.head()

Ngomong-ngomong karena pada latihan ini berjudul univariate, kita hanya akan menggunakan satu fitur yaitu meantemp dan date sebagai interval waktunya.

dates = df['date'].values
temp  = df['meantemp'].values

Masih ingatkan dengan tahapan preprocessing pada materi sebelumnya? Sekarang mari kita flashback dan mengambil kembali fungsi windowing yang telah dibuat sebelumnya. 

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

Secara singkat fungsi di atas mengubah data kita menjadi format yang dapat diterima oleh model. Fungsi di atas menerima sebuah series/atribut kita yang telah dikonversi menjadi tipe numpy, lalu mengembalikan label dan atribut dari dataset dalam bentuk batch. Untuk penjelasan detailnya, sudah kita bahas di materi sebelumnya. Take your time dan jangan sungkan untuk membaca kembali materi sebelumnya demi pengalaman belajar yang maksimal.

Selanjutnya untuk arsitektur model, gunakan dua buah layer LSTM. Ketika menggunakan dua buah layer LSTM atau lebih, perhatikan bahwa seluruh layer sebelum layer LSTM terakhir harus memiliki parameter return_sequences yang bernilai True.

train_set = windowed_dataset(temp, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

Lalu pada optimizer, kita akan menggunakan parameter learning rate dan momentum seperti di bawah. Penjelasan dan praktik learning rate akan dibahas lebih detail pada materi selanjutnya. Kemudian salah satu loss function yang dapat dicoba untuk ini adalah Huber yang umum digunakan pada kasus time series. Selain itu, metrik yang digunakan untuk mengevaluasi model adalah MAE.

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=100)

Selanjutnya, kita dapat melakukan prediksi dengan menggunakan fungsi berikut.

forecast = history.model.predict(train_set)
forecast



2. Multivariate Time Series:
Multivariate time series adalah rangkaian data di mana setiap titik data terdiri dari beberapa variabel yang diamati atau diukur secara bersamaan dalam interval waktu yang berurutan. Berbeda dengan univariate time series, kita tidak hanya melihat satu variabel pada satu waktu tertentu, tetapi sejumlah variabel yang diamati secara bersamaan. Ini memungkinkan kita untuk memahami bagaimana variabel-variabel tersebut berinteraksi satu sama lain seiring waktu.

Dengan kata lain, multivariate time series adalah cara untuk menganalisis dan memodelkan hubungan kompleks antara berbagai variabel dalam rentang waktu tertentu. Ini memberikan insight yang lebih mendalam tentang dinamika sistem yang diamati, memungkinkan kita untuk melihat bagaimana perubahan dalam satu variabel dapat memengaruhi variabel lainnya.

Pada kasus ini, kita akan menggunakan sebuah dataset berisikan pengukuran konsumsi daya listrik di satu rumah dengan tingkat pengambilan sampel satu menit selama hampir dua tahun. Tersedia berbagai besaran listrik dan beberapa nilai subpengukuran. Anda dapat mengunduh data pada tautan berikut: Household Electric Power atau langsung mengakses melalui Google Drive yang sudah disediakan menggunakan kode berikut.

df = pd.read_csv('https://drive.google.com/uc?id=1AZRfFoyekqSYpri5183RmJjciRGz_ood', sep=',',
                     infer_datetime_format=True, index_col='datetime', header=0)


Setelah data sudah siap digunakan, tentunya Anda masih ingatkan kita perlu melakukan preprocessing? Pada studi kasus ini, mari kita buat sebuah fungsi untuk melakukan normalisasi layaknya pada materi data preprocessing menggunakan fungsi berikut.

def normalize_series(data, min, max):
    data = data - min
    data = data / max
    return data
data = df.values
data = normalize_series(data, data.min(axis=0), data.max(axis=0))

Mengapa kita menggunakan sebuah fungsi? Karena kasus ini merupakan multivariate time series di mana data yang kita gunakan memiliki lebih dari satu fitur, maka dari itu alangkah baiknya membuat sebuah fungsi yang dapat digunakan berulang kali. 

Mengingat kita sedang menyelesaikan kasus multivariate berarti harus memastikan jumlah fiturnya, bukan? Agar tidak keliru untuk menghitung jumlah fitur yang ada mari kita gunakan kode berikut.

N_FEATURES = len(df.columns)

Setelah itu, masih ingatkah bahwa kita harus memisahkan data latih dan data uji? Hal tersebut juga berlaku ya untuk kasus multivariate time series. Kita bisa menggunakan kode berikut untuk menentukan proporsi data berdasarkan interval waktu yang sudah ditentukan.

SPLIT_TIME = int(len(data) * 0.5)
x_train = data[:SPLIT_TIME]
x_valid = data[SPLIT_TIME:]

Kode di atas akan memisahkan data menjadi dua bagian yaitu 50% untuk data latih dan 50% sisanya untuk data uji. Seperti yang Anda ingat, jumlah data yang ada pada kasus ini adalah 86.400 baris sehingga dengan menjalankan kode di atas kita akan membagi masing-masing data menjadi 43.200 baris. 

Setelah itu, mari kita buat fungsi window seperti pada kasus univariate time series. Kurang lebihnya kode yang dibangun akan serupa tetapi memiliki beberapa perbedaan. Simak kode berikut. 

def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))
    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))
    return ds.batch(batch_size).prefetch(1)

Apakah Anda sudah memperhatikan dengan saksama? Apa saja perbedaan yang Anda temukan? Sini-sini, mari kita kupas tuntas semuanya agar tidak ada salah paham antara kita. 

a. def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):

ini adalah definisi dari fungsi windowed_dataset. Fungsi ini memiliki beberapa argumen: 
- series: data berurutan yang akan diproses.
- batch_size: ukuran batch yang akan digunakan dalam dataset.
- n_past: jumlah waktu ke belakang yang akan dipertimbangkan sebagai input. 
- n_future: jumlah waktu ke depan yang akan dipertimbangkan sebagai output yang diinginkan.
- shift: perpindahan (shift) dalam jumlah waktu saat membuat jendela waktu.
Nah, seperti yang Anda lihat perbedaannya terdapat pada ketiga argumen terakhir. Nantinya kita akan membahas lebih detail terkait tiga argumen tersebut. Jadi, tahan dahulu rasa penasarannya.

b. ds = tf.data.Dataset.from_tensor_slices(series):
membuat dataset TensorFlow dari array series menggunakan from_tensor_slices. Setiap elemen dalam dataset ini akan menjadi elemen dalam array series.

c. ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True): fungsi window yang digunakan di sini untuk membagi dataset menjadi jendela waktu. Argumen size menentukan ukuran jendela (jumlah waktu ke belakang dan ke depan), shift menentukan pergeseran dalam waktu antara jendela, dan drop_remainder=True menghapus bagian dari dataset yang tidak cukup besar untuk membentuk jendela penuh.

d. ds = ds.flat_map(lambda w: w.batch(n_past + n_future)): fungsi flat_map digunakan untuk mengaplikasikan fungsi lambda ke setiap elemen jendela dalam dataset dan kemudian menggabungkan hasilnya. Di sini, fungsi lambda mengelompokkan elemen-elemen dalam setiap jendela ke dalam batch dengan ukuran n_past + n_future.

e. ds = ds.map(lambda w: (w[:n_past], w[n_past:])): setiap batch dalam dataset sekarang berisi window/jendela yang berisikan n_past titik data waktu sebelumnya dan n_future titik data waktu setelahnya. Dalam langkah ini, setiap batch diubah menjadi pasangan input-output, di mana inputnya adalah data waktu sebelumnya (sepanjang n_past) dan outputnya adalah data waktu setelahnya (sepanjang n_future).

f. return ds.batch(batch_size).prefetch(1): Akhirnya, dataset dipecah menjadi batch-batch dengan ukuran batch_size, kemudian dioptimalkan untuk performa dengan memuat data dalam batch menggunakan prefetch(1). Ini berarti saat satu batch sedang diproses oleh model, TensorFlow sudah memuat batch berikutnya untuk diproses sehingga akan meningkatkan penggunaan CPU/GPU dengan lebih efisien.


Mungkin tebersit di benak Anda sebuah pertanyaan, “Dari mana multivariate-nya? Kok tidak ada jumlah fitur yang digunakan?” Chill bro, selesaikan satu per satu dahulu tahapan windowing datasetnya. Kita juga belum menentukan parameter untuk membagi data latih dan data uji. Oleh karena itu, yuk kita buat dengan kode berikut.


BATCH_SIZE = 32
N_PAST = 24
N_FUTURE = 24
SHIFT = 1
# Kode untuk membuat windowed datasets
train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,
                                 n_past=N_PAST, n_future=N_FUTURE,
                                 shift=SHIFT)
valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,
                                 n_past=N_PAST, n_future=N_FUTURE,
                                 shift=SHIFT)


Kode di atas akan mengubah data yang sebelumnya bertipe dataframe menjadi sebuah windowed dataset. semua parameter yang diperlukan juga sudah ditentukan hingga akhirnya kita sudah memiliki data window latih dan uji yang siap digunakan.

Selanjutnya untuk arsitektur model, gunakan dua buah layer Dense. Perlu Anda perhatikan layer pertama harus memiliki parameter input_shape sesuai dengan ukuran yang sudah kita tentukan sebelumnya yaitu (24, 7). 

model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(64, input_shape=(N_PAST, N_FEATURES)),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(N_FEATURES)
    ])

Selain perbedaan tadi, apakah Anda menemukan perbedaan lainnya? Tenang saja, Anda memiliki banyak waktu untuk melakukan analisis dari kode di atas dan mencari perbedaannya. Coba deh perhatikan layer terakhir yang kita bangun. Jumlah yang ditentukan berbeda dengan latihan pada kasus univariate, ‘kan? Hal ini karena multivariate akan menghasilkan lebih dari satu nilai sebagai outputnya, pada kasus ini akan menghasilkan tujuh buah angka prediksi.

Nah, yang menarik pada latihan ini adalah kita akan menggunakan salah satu hyperparameter yang sangat berguna ketika membangun sebuah model neural network. Hyperparameter adalah parameter yang digunakan untuk mengontrol model machine learning atau model deep learning di luar parameter model itu sendiri. Dalam konteks AI, parameter model adalah variabel yang nilainya diubah selama proses pembelajaran untuk meminimalkan kesalahan model pada data pelatihan. Hyperparameter, di sisi lain, adalah konfigurasi yang ditetapkan sebelumnya yang memengaruhi proses pembelajaran, tetapi nilainya tidak dipelajari oleh model dari data.

Pada latihan ini, kita akan menggunakan myCallback sebagai salah satu komponen hyperparameter tuning dari callbacks. Fungsi dari myCallback adalah untuk memberikan logika khusus pada akhir setiap epoch selama pelatihan model. Dalam hal ini, myCallback digunakan untuk menghentikan pelatihan model jika suatu kondisi tertentu terpenuhi.  

Dengan demikian, myCallback memberikan fleksibilitas untuk mengontrol proses pelatihan model berdasarkan kriteria yang ditetapkan. Dalam kasus ini, pelatihan akan berhenti ketika tingkat kinerja model telah mencapai tingkat yang dianggap memadai, yaitu ketika nilai MAE pada data pelatihan dan data validasi sudah cukup rendah. Perhatikan kode berikut.. 

class myCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs={}):
            if (logs.get('mae') < 0.055 and logs.get('val_mae') < 0.055):
                self.model.stop_training = True
 
callbacks = myCallback()

Kode di atas mendefinisikan sebuah kelas yang disebut myCallback, yang merupakan turunan dari kelas tf.keras.callbacks.Callback. Di dalam kelas myCallback, ada sebuah metode yang di-override yaitu on_epoch_end. Metode ini akan dipanggil oleh Keras pada akhir setiap epoch. Di sanalah logika khusus dapat ditambahkan untuk dijalankan pada titik tersebut.

Di dalam metode on_epoch_end, terdapat logika untuk mengevaluasi dua kondisi, yaitu jika nilai Mean Absolute Error (MAE) pada data pelatihan (logs.get('mae')) kurang dari 0.055, dan nilai validasi (logs.get('val_mae')) kurang dari 0.055. Jika kedua kondisi tersebut terpenuhi, atribut stop_training dari model akan diatur menjadi True yang akan menghentikan proses pelatihan model. Ini berarti jika model telah mencapai tingkat kinerja yang diinginkan pada kedua dataset (pelatihan dan validasi), pelatihan akan berhenti pada epochs tersebut.

Selanjutnya, kita perlu menyusun metrik evaluasi yang akan dihitung oleh model ketika proses pelatihan, Anda dapat menambahkan kode berikut.

# Kode untuk melakukan menyusun struktur sesuai dengan machine learning
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) 
model.compile(loss='mae',
                  optimizer= optimizer,
                  metrics=["mae"])

Masih ingatkan salah satu metrik evaluasi yaitu MAE? Kita sudah berkenalan cukup baik dengannya. Oleh karena itu, kita dapat melewati penjelasan itu dan mari kita lanjutkan untuk melakukan pelatihan menggunakan fungsi .fit() seperti berikut.

model.fit(train_set,
          validation_data=(valid_set),
          epochs=100,
          callbacks=callbacks,
          verbose=1
    )

Kode tersebut adalah bagian dari proses pelatihan sebuah model menggunakan TensorFlow. Mari kita bahas masing-masing argumen yang diberikan kepada metode fit().
- train_set: ini adalah dataset yang digunakan untuk pelatihan model. Dataset ini berisi pasangan data fitur dan label yang akan digunakan untuk melatih model.
- validation_data=(valid_set): argumen ini menunjukkan dataset validasi yang akan digunakan untuk mengevaluasi kinerja model pada setiap epoch selama pelatihan. Dataset validasi ini berisi pasangan data fitur dan label yang berbeda dari data pelatihan dan digunakan untuk menghindari overfitting. valid_set adalah dataset validasi yang diberikan dan akan diuji oleh model yang dibangun.
- epochs=100: ini adalah jumlah epoch yang akan digunakan untuk melatih model. Epoch adalah satu putaran penuh melalui seluruh dataset pelatihan.
- callbacks=callbacks: ini adalah argumen yang digunakan untuk memberikan callbacks ke model. Di sini, callbacks adalah objek yang telah dibuat sebelumnya yang berisi definisi dari callback berdasarkan modifikasi myCallback. Callback ini akan dipanggil pada akhir setiap epoch selama pelatihan dan akan menghentikan pelatihan jika kriteria yang ditentukan terpenuhi.
- verbose=1:argumen ini menentukan tingkat kecerewetan (verbosity) selama pelatihan dengan ketentuan sebagai berikut. 


Jadi, kode ini digunakan untuk memulai proses pelatihan model dengan menggunakan dataset pelatihan train_set; mengevaluasi kinerja model pada setiap epoch menggunakan dataset validasi valid_set; dan menggunakan callback hasil modifikasi myCallback untuk menghentikan pelatihan jika kriteria tertentu terpenuhi. 

Proses pelatihan yang Anda jalankan di atas akan memakan waktu yang cukup lama, tergantung  hardware yang digunakan. Pada kasus ini, kita akan menggunakan Google Colab standar yang melakukan pelatihan selama 40 menit sebagai benchmark atau baseline. 



---------------------------------------------------------------------------
                             Klasifikasi Gambar
---------------------------------------------------------------------------
Dalam modul ini, Anda akan belajar terkait pemahaman yang lebih dalam tentang klasifikasi gambar menggunakan deep learning. Materi ini akan memberikan Anda pemahaman lebih tentang dunia piksel, pola, dan representasi visual yang mendalam. Di tengah-tengahnya, Anda akan menemukan algoritma-algoritma canggih yang mampu membedakan antara kucing dan anjing serta membedakan antara jenis-jenis bunga yang indah.

Namun, klasifikasi gambar bukan sekadar soal pengenalan objek. Namun, materi ini adalah pintu gerbang ke berbagai aplikasi yang mengubah cara berinteraksi dengan dunia di sekitar kita. Dari deteksi wajah untuk keamanan, hingga pengenalan pola untuk pengolahan citra medis; klasifikasi gambar memiliki dampak yang luas dan mendalam pada berbagai bidang kehidupan kita. 

Klasifikasi gambar merupakan salah satu aplikasi penting dalam deep learning untuk mengidentifikasi kategori atau label dari suatu gambar berdasarkan konten visualnya. Metode ini adalah cabang computer vision, yang merupakan subdisiplin dari kecerdasan buatan dan berfokus pada pengenalan serta pemahaman gambar oleh komputer.

Namun, cabang dari computer vision tidak hanya klasifikasi gambar. Ada juga klasifikasi dan lokalisasi (classification and localization) serta deteksi objek (object detection). Apakah perbedaannya?

Klasifikasi gambar adalah proses saat komputer belajar mengenali hal yang ada dalam gambar. Misalnya, jika ingin mengajari komputer untuk membedakan antara gambar kucing dan anjing, kita akan memberikan banyak contoh gambar kucing dan anjing dalam komputer. Kemudian, komputer akan belajar secara otomatis untuk mengenali ciri-ciri visual yang membuat gambar tersebut dikelompokkan sebagai kucing atau anjing.

Lalu, klasifikasi dan lokalisasi adalah proses tentang mengajarkan komputer untuk tidak hanya mengenali hal yang ada dalam gambar, tetapi juga memberi tahu letaknya. Misalnya, jika kita memberikan gambar seekor kucing kepada komputer, komputer tidak hanya akan mengatakan "ini adalah kucing", tetapi juga akan menandai kotak di sekitar kucing untuk menunjukkan letak kucing itu berada dalam gambar.

Sementara deteksi objek adalah proses tentang mengajarkan komputer untuk menemukan dan menandai semua objek yang ada dalam gambar. Jadi, jika gambar tersebut berisi beberapa kucing dan anjing, komputer akan mengidentifikasi serta menandai setiap kucing dan anjing dalam gambar dengan kotak di sekitar masing-masingnya.



Dasar-Dasar Convolutional Neural Network (CNNs):
Inspirasi utamanya datang dari cara otak mamalia memproses informasi visual. CNN menggunakan serangkaian layer untuk secara bertahap memahami fitur-fitur yang semakin kompleks dari gambar. Tahapan ini memungkinkan komputer untuk belajar secara otomatis dari data gambar yang diberikan.

Salah satu keunggulan utama CNN adalah kemampuannya mengenali objek dalam gambar, seperti kucing atau anjing dengan tingkat akurasi yang tinggi. Selain itu, CNN juga dapat digunakan untuk mendeteksi objek, yaitu menemukan dan menandai lokasi objek pada gambar, serta segmentasi, yang memisahkan dan mengidentifikasi bagian-bagian berbeda dalam gambar.

CNN sangat efektif pada klasifikasi gambar dengan cara mengambil gambar input atau masukan, memproses, lalu mengategorikannya dalam kelas-kelas tertentu, seperti mengidentifikasi sebuah gambar yang berisi anjing, kucing, harimau, atau singa. Untuk komputer, sebuah gambar masukan dilihat sebagai kumpulan piksel, dan cara ini bervariasi tergantung pada resolusi gambar tersebut.

Sebagai contoh, jika kita mempertimbangkan sebuah gambar berdimensi 6 × 6 × 3, ini menandakan bahwa sebuah array matriks dengan tinggi atau height (h) sebanyak 6 piksel, lebar atau width (w) sebanyak 6 piksel, dan dimensi atau dimension (d) 3 mengacu pada nilai-nilai RGB atau red, green, blue (merah, hijau, biru). Jadi, CNN memproses gambar dalam bentuk array piksel ini untuk mengambil keputusan klasifikasi.


Arsitektur CNN:
CNN dapat disamakan seperti kue lapis yang memiliki banyak lapisan. Analogi ini membantu kita memahami struktur kompleks CNN, yaitu setiap "lapisan" memiliki perannya sendiri yang krusial dalam pemrosesan data gambar. Layaknya kue lapis yang terdiri dari berbagai lapisan berbeda, CNN terdiri atas berbagai jenis lapisan, seperti konvolusi, aktivasi, dan pooling. Jadi, setiap lapisan CNN bekerja bersama-sama untuk memproses dan menganalisis data gambar dengan cara yang mirip seperti manusia dalam memahami gambar.

1. Input Layer:
Input layer adalah lapisan pertama dalam sebuah neural network. Fungsi utamanya adalah menerima data masukan atau input dari dataset yang akan diproses oleh jaringan. Setiap neuron di lapisan ini mewakili satu fitur atau atribut dalam data masukan.

Data yang diteruskan melalui input layer dapat berupa berbagai bentuk, tergantung pada jenis tugas jaringan. Misalnya, dalam pengenalan gambar, data masukan bisa berupa piksel-piksel gambar, sedangkan dalam pengenalan teks, data masukan bisa berupa vektor kata-kata atau karakter.

Input layer biasanya tidak memiliki perhitungan yang rumit karena tujuannya hanya menerima data masukan dan meneruskannya ke lapisan-lapisan berikutnya dalam jaringan. Jumlah neuron pada lapisan ini biasanya sesuai dengan jumlah fitur dalam data masukan.

2. Convolutional Layers (Lapisan Konvolusi)
Konvolusi adalah lapisan pertama dalam CNN yang digunakan untuk mengekstraksi fitur dari gambar masukan. Proses konvolusi ini mempertahankan hubungan antara piksel-piksel dalam gambar dan cara mempelajari fitur-fitur gambar menggunakan kotak-kotak kecil data masukan. Pada lapisan ini, ada operasi matematika yang melibatkan dua input, seperti matriks gambar dan filter atau kernel.

Pada convolution layer, ada tiga jenis berbeda berdasarkan berapa banyak dimensi data yang diproses, yaitu CONV1D, CONV2D, dan CONV3D.

Pertama, CONV1D dipakai ketika kita memiliki data yang bergerak satu arah, seperti urutan waktu dalam data time series atau kalimat pada data NLP. Misalnya, pada NLP, CONV1D dapat membantu kita melacak pola dalam urutan kata.

Kedua, CONV2D yang bekerja dengan data dua dimensi, seperti gambar hitam putih atau satu saluran warna. Jenis ini adalah konvolusi yang paling sering digunakan dalam pengolahan gambar. CONV2D dapat membantu kita mengidentifikasi fitur visual dalam gambar, seperti garis tepi (edges), sudut, atau pola yang lebih kompleks.

Ketiga, CONV3D yang digunakan ketika kita memiliki data tiga dimensi, seperti video atau gambar berwarna dengan tiga saluran warna (merah, hijau, biru). Pada pemrosesan video, CONV3D bisa membantu untuk melacak pola dalam ruang dan waktu sehingga kita bisa memahami cara perubahan terjadi pada video seiring berjalannya waktu. 

Ada beberapa hyperparameter pada convolutional layer sebagai berikut.
a. Kernel Size
Hyperparameter ini menentukan ukuran sliding window. Umumnya disarankan untuk menggunakan window size yang lebih kecil, terutama nilai ganjil, seperti 1, 3, 5, dan 7. 

b. Stride
Stride menunjukkan seberapa jauh kernel akan melompat setiap kali bergerak di sepanjang gambar. Misalnya, jika stride adalah 1, kotak akan bergerak satu langkah ke kanan setiap kali bergerak. Tujuannya adalah mengontrol seberapa sering algoritma "melihat" gambar.

c. Padding 
 Padding adalah cara kita memastikan bahwa bagian pinggir gambar juga diperlakukan dengan adil. Dengan menambahkan padding, kita memberi ruang tambahan di sekitar gambar sehingga kernel dapat bergerak di tepi tanpa kehilangan informasi. 

d. Number of filters or Depth
Number of filters menentukan seberapa banyak pola atau fitur yang akan dilihat oleh layer konvolusi. Bayangkan setiap filter sebagai mata yang mencari pola tertentu dalam gambar. Semakin banyak mata, semakin banyak pola yang bisa kita temukan. 

3. Activation Layers
Setelah setiap convolution layers dalam CNN, kita menerapkan activation function non-linear, seperti ReLU, ELU, atau Leaky ReLU. Activation layers adalah lapisan khusus dalam neural network yang bertanggung jawab untuk menerapkan fungsi aktivasi ke output dari lapisan sebelumnya.

Meskipun disebut sebagai lapisan, activation layer sebenarnya bukanlah "lapisan" dalam arti sejati karena tidak ada parameter atau bobot yang dipelajari pada activation layer. Selain itu, ia kadang-kadang diabaikan dari diagram arsitektur jaringan karena diasumsikan bahwa aktivasi selalu mengikuti konvolusi.

Dalam neural network, setiap lapisan terdiri dari sejumlah besar neuron atau unit kecil. Ketika data melewati suatu lapisan, setiap neuron dalam lapisan tersebut menghasilkan output berdasarkan pada masukan yang diterimanya. Namun, output ini sering kali perlu diubah atau dimodifikasi agar lebih berguna dalam mempelajari pola-pola kompleks dalam data. Itulah alasan kita memasukkan activation layers setelah setiap lapisan melakukan operasi matematika. Activation layers ini menerapkan activation function ke setiap output neuron dari lapisan sebelumnya.

Activation function ini memberikan sifat non-linear kepada neural network karena kebanyakan masalah di dunia nyata memiliki sifat non-linear. Tanpa activation function, neural network akan menjadi serangkaian operasi linear dan tidak cukup fleksibel untuk menangani masalah yang kompleks. 

4. Pooling Layer
Pooling layer adalah salah satu jenis lapisan dalam CNN untuk mengurangi dimensi spasial dari representasi gambar. Tujuannya adalah mengurangi jumlah parameter dan komputasi dalam jaringan, membantu mencegah overfitting, serta mempercepat proses pelatihan. Secara sederhana, pooling layer bekerja dengan mengambil "sampel" dari area kecil representasi gambar dan menggabungkannya menjadi satu nilai.

Ada dua jenis pooling yang umum digunakan: max pooling dan average pooling. Max pooling bekerja pada setiap area kecil (misalnya, 2 × 2 atau 3 × 3) sehingga nilai maksimum yang diambil. Misalnya, kita menggunakan max pooling dengan ukuran 2 × 2 maka nilai maksimum di antara 4 nilai dalam area tersebut akan dipertahankan dan yang lainnya diabaikan. Pada average pooling, dalam setiap area kecil, nilai rata-rata diambil. Artinya, semua nilai dalam area tersebut dijumlahkan dan hasilnya dibagi dengan jumlah nilai tersebut.

Setelah pooling dilakukan, dimensi gambar dikurangi karena ukuran gambar menjadi lebih kecil. Tahapan ini membantu dalam mengurangi jumlah parameter yang dibutuhkan untuk jaringan dan mempercepat proses komputasi. Pooling juga membantu dalam mendorong invariansi spasial, yaitu kemampuan model untuk tetap mengenali pola pada gambar meskipun terjadi pergeseran atau rotasi kecil. Lapisan dapat meningkatkan kemampuan model untuk menggeneralisasi dan mempelajari fitur-fitur yang relevan dari gambar dalam berbagai konteks.

5. Fully Connected Layers
Fully connected layers adalah bagian penting dari neural network yang terletak di bagian akhir dari arsitektur, biasanya setelah serangkaian convolution layer dan pooling layer pada CNN. Dalam fully connected layers, setiap neuron terhubung dengan setiap neuron pada lapisan sebelumnya dan setiap koneksi memiliki bobot yang harus dipelajari selama proses pelatihan jaringan. Hal ini memungkinkan jaringan untuk menyesuaikan bobot-bobot agar sesuai dengan pola-pola yang ada dalam data pelatihan.

Fungsi utama dari fully connected layers adalah menggabungkan fitur-fitur yang telah diekstraksi dari data oleh lapisan-lapisan sebelumnya dan kemudian menghasilkan output akhir. Misalnya, dalam kasus pengenalan gambar, fully connected layers akan mengambil fitur-fitur visual dari gambar dan menggunakannya untuk memutuskan kelas yang paling cocok.

6. Output Layers
Output layers adalah bagian terakhir pada sebuah neural network yang menghasilkan prediksi atau output akhir dari model. Fungsi dari lapisan ini sangat tergantung pada jenis tugas yang dilakukan oleh jaringan.

Misalnya, dalam kasus klasifikasi gambar, output layers biasanya terdiri dari neuron-neuron dan mewakili setiap kelas yang tersedia. Lalu, aktivasi dari neuron tertinggi menunjukkan kelas yang diprediksi oleh model. Untuk tugas regresi, lapisan output dapat memiliki satu atau beberapa neuron yang menghasilkan nilai-nilai kontinu.

Selama proses pelatihan, model belajar untuk menghasilkan output yang mendekati atau cocok dengan target sebenarnya. Hal ini dilakukan dengan menyesuaikan parameter-parameter dalam jaringan, seperti bobot dan bias, selama proses pembelajaran.

Dalam banyak kasus, output layers juga memiliki activation function khusus tergantung pada tugasnya. Misalnya, untuk klasifikasi biner, activation function yang umum digunakan adalah sigmoid, sedangkan untuk klasifikasi multi-kelas, softmax sering digunakan. Untuk tugas regresi, tidak ada activation function khusus yang diperlukan.

 Terkadang, kita memerlukan convolution layer untuk menangkap fitur-fitur visual, sementara pooling layer untuk mengurangi dimensi data. Tidak lupa juga, kita dapat memasukkan lapisan-lapisan, seperti batch normalization, flatten, dan dropout untuk meningkatkan kinerja dan stabilitas model.

Secara singkat, batch normalization membantu menjaga konsistensi serta stabilitas pelatihan, flatten mengubah bentuk data menjadi vektor yang sesuai, dan dropout mencegah overfitting dengan secara acak menonaktifkan sebagian kecil neuron selama pelatihan.


Tahapan Klasifikasi Gambar:
Tahapan ini meliputi pengumpulan dan pemisahan data, pra-pemprosesan data, pembuatan model CNN, pelatihan model, evaluasi, penyetelan hyperparameter, pengujian, dan penggunaan model dalam aplikasi praktis. 

1. Pengumpulan Data
Langkah pertama adalah mengumpulkan data gambar yang akan digunakan untuk melatih model klasifikasi. Data ini harus mencakup gambar-gambar mewakili setiap kelas atau label yang ingin diprediksi oleh model. 

- Mesin Pencari Gambar
Anda dapat menggunakan mesin pencari gambar, seperti Google Images, Bing Images, atau Flickr untuk mencari gambar-gambar dengan kata kunci tertentu. Namun, perlu diingat bahwa Anda harus memeriksa hak cipta dan izin penggunaan gambar tersebut.

- Basis Data Publik
Ada beberapa basis data publik yang menyediakan akses gratis ke gambar-gambar dengan label, seperti berikut.
	- ImageNet: ImageNet adalah basis data besar yang menyediakan jutaan gambar yang dianotasi untuk berbagai kategori.
	- COCO (Common Objects in Context): COCO adalah basis data gambar yang berfokus pada objek-objek umum dalam konteks tertentu.
	- Open Images Dataset: Ini adalah proyek Google yang menyediakan akses ke jutaan gambar dengan label yang bervariasi.

- UCI Machine Learning Repository
- Kerja Sama dengan Ahli Domain


2. Pemisahan Data (Data Splitting)
Setelah data dikumpulkan, data perlu dipisahkan menjadi tiga set: data pelatihan, data validasi, dan data uji. Data pelatihan digunakan untuk melatih model, data validasi untuk mengevaluasi kinerja model selama pelatihan, dan data uji untuk menguji kinerja model setelah pelatihan selesai.

 Tujuan utamanya adalah membagi dataset menjadi subset-subset yang berbeda untuk pelatihan, validasi, serta pengujian model atau pelatihan dan pengujian model.

Pertama, data pelatihan (training data) digunakan untuk melatih model klasifikasi gambar. Data ini adalah bagian terbesar dari dataset serta digunakan untuk mengajarkan model mengenali pola-pola dan fitur-fitur dari gambar-gambar yang ada. Semakin besar data pelatihan, semakin baik model dapat dilatih.

Kedua, data validasi (validation data) digunakan untuk menyetel atau menyesuaikan parameter-parameter model. Dataset ini membantu Anda memilih model terbaik dan mencegah overfitting. Anda dapat menggunakan data validasi untuk memantau kinerja model saat proses pelatihan berlangsung dan mengatur parameter-parameter, seperti learning rate atau jumlah epoch.

Terakhir, data pengujian (test data) digunakan untuk menguji kinerja model setelah proses pelatihan selesai. Data pengujian ini harus terpisah sepenuhnya dari data pelatihan dan validasi. Dengan menggunakan data pengujian yang independen, Anda bisa mengevaluasi seberapa baik model dapat mengklasifikasikan gambar-gambar baru yang tidak pernah dilihat sebelumnya


3. Pra-Pemrosesan Data Gambar
Sebelum data dimasukkan ke model, seringkali dilakukan pra-pemrosesan data. Ini bisa termasuk normalisasi piksel (mengubah rentang nilai piksel menjadi 0 hingga 1 atau -1 hingga 1), resize gambar ke ukuran yang konsisten, dan augmentasi data untuk memperluas dataset pelatihan dengan membuat variasi pada gambar.

Pembersihan Data
Langkah pertama adalah memeriksa setiap gambar dalam dataset untuk mengidentifikasi gambar-gambar yang buram, kabur, atau tidak relevan. Langkah ini dapat dilakukan dengan visualisasi gambar atau menggunakan metrik kualitas gambar, seperti kejelasan (clarity) atau kontras (contrast). Gambar-gambar yang tidak memenuhi standar kualitas kemudian dihapus dari dataset. Hasil dari tahapan ini membantu menghilangkan gangguan dan memastikan model fokus pada fitur-fitur yang penting.

Normalisasi
Normalisasi adalah proses untuk mengubah rentang nilai piksel dalam gambar sehingga setiap piksel memiliki distribusi yang seragam. Proses ini biasanya dilakukan dengan mengurangi rata-rata gambar dan menyesuaikan deviasi standar menjadi satu. Normalisasi membantu dalam menghindari masalah numerik dan mempercepat konvergensi saat pelatihan model.

Reduksi Dimensi
Jika diperlukan, Anda dapat menggunakan teknik reduksi dimensi, seperti principal component analysis (PCA) untuk mengurangi dimensi gambar. Teknik ini berguna jika Anda memiliki dataset gambar dengan dimensi tinggi yang dapat menyebabkan masalah komputasi yang berat.

Augmentasi Data
Augmentasi data adalah langkah krusial dalam deep learning karena memperluas dataset Anda tanpa harus mengumpulkan lebih banyak data aktual. Melalui penerapan variasi pada citra-citra yang ada, augmentasi data membantu mengatasi masalah "data hungry" dalam deep learning, yang memerlukan jumlah data besar untuk melatih model dengan baik. 

Dalam konteks PyTorch, augmentasi data dapat diterapkan menggunakan berbagai teknik yang populer dan efektif. Dengan mengombinasikan beberapa jenis augmentasi data, Anda bisa menghasilkan variasi cukup besar dalam dataset sehingga setiap citra memiliki beragam versi yang dapat membantu model mempelajari fitur-fitur lebih umum.

Berikut adalah beberapa jenis metode augmentasi data yang bisa Anda terapkan.

- Flip
Flip adalah salah satu teknik augmentasi data yang berguna untuk memvariasikan dataset dengan membalik posisi citra secara vertikal atau horizontal. Namun, penting untuk memperhatikan bahwa tidak semua citra cocok untuk dilakukan flip. Misalnya, citra gedung tidak sebaiknya dilakukan flip secara vertikal karena akan menghasilkan citra yang tidak realistis. Hal ini dapat membuat model kesulitan dalam mempelajari pola-pola dalam citra. Namun, untuk citra seperti bunga, flip vertikal bisa menjadi pilihan yang baik.

# Transformasi untuk flip vertikal
vertical_flip = transforms.Compose([
    transforms.RandomVerticalFlip(p=1)
])
flipped_image_vertical = vertical_flip(image)

horizontal_flip = transforms.Compose([
    transforms.RandomHorizontalFlip(p=1)
])
flipped_image_horizontal = horizontal_flip(image)

- Translasi
Pergeseran (atau disebut juga translasi) adalah teknik augmentasi data, yaitu ketika gambar dipindahkan ke arah horizontal atau vertikal. Pada proses ini, piksel-piksel gambar dipindahkan secara bersama-sama dalam arah yang ditentukan. Misalnya, jika gambar digeser ke kanan secara horizontal, setiap piksel pada gambar akan bergeser ke kanan sejumlah piksel tertentu dan piksel kosong yang muncul di sisi sebaliknya akan diisi dengan nilai piksel yang sesuai dari sisi sebelumnya.

horizontal_shift = transforms.Compose([
    transforms.RandomAffine(degrees=0, translate=(0.2, 0)),  # Menggeser citra ke kanan atau kiri secara acak hingga 20% dari lebar gambar
])
shifted_image_horizontal = horizontal_shift(image)

vertical_shift = transforms.Compose([
    transforms.RandomAffine(degrees=0, translate=(0, 0.2)),  # Menggeser citra ke atas atau bawah secara acak hingga 20% dari tinggi gambar
])
shifted_image_vertical = vertical_shift(image)



- Zoom
Zoom adalah salah satu teknik augmentasi data dengan cara gambar diperbesar atau diperkecil. Dalam proses ini, gambar diberikan perbesaran atau penyusutan proporsional terhadap ukuran aslinya. Teknik ini membantu model untuk melihat detail-detail kecil dalam gambar dan meningkatkan kemampuan model mengenali objek dengan berbagai skala.

Dengan menerapkan zoom, model dapat melihat objek dalam resolusi yang lebih tinggi atau rendah daripada data pelatihan. Hal ini memungkinkan model untuk mempelajari detail-detail halus dalam objek dan membuatnya lebih sensitif terhadap variasi tekstur, pola, atau fitur-fitur lainnya. Misalnya, dengan zoom in pada gambar wajah, model dapat belajar membedakan antara ekspresi wajah yang halus dan mendeteksi fitur-fitur, seperti mata, hidung, dan mulut secara lebih baik.

Selain itu, zoom juga membantu dalam meningkatkan invariansi terhadap skala objek. Dengan melatih model pada variasi skala yang berbeda, model menjadi lebih mampu mengenali objek dalam berbagai ukuran dan jarak. Hal ini membuat model lebih robust terhadap variasi dalam dataset dan memungkinkan pengenalan objek yang lebih baik pada situasi nyata.

- Rotation (Rotasi)
Rotasi adalah teknik augmentasi data yang melibatkan putaran gambar sejumlah derajat tertentu sehubungan dengan pusat rotasi. Pada proses rotasi, setiap piksel dalam gambar diputar searah atau berlawanan arah jarum jam sejumlah derajat tertentu.

Rotasi digunakan untuk menciptakan variasi dalam dataset dengan memperkenalkan perubahan sudut pandang pada objek dalam gambar. Teknik ini membantu model untuk belajar invariansi terhadap rotasi dan memperbaiki generalisasi. Jadi, model dapat mengenali objek dengan baik meskipun objek tersebut muncul dalam posisi yang berbeda-beda.

Dalam konteks pengolahan gambar, rotasi dapat diterapkan dengan berbagai sudut, baik searah maupun berlawanan arah jarum jam. Konteks ini memberikan fleksibilitas dalam variasi sudut pandang yang diperkenalkan pada dataset. Ini memungkinkan model untuk belajar dengan lebih baik dari berbagai sudut pandang yang mungkin dihadapi dalam situasi nyata.

rotation = transforms.Compose([
    transforms.RandomRotation(degrees=30)
])
rotated_image = rotation(image)

inverse_rotation = transforms.Compose([
    transforms.RandomRotation(degrees=(-30, 0))
])
inverse_rotated_image = inverse_rotation(image)


- Brightness Adjustment atau Penyesuaian Kecerahan
Brightness adjustment adalah teknik augmentasi data dengan cara mengubah tingkat kecerahan pada gambar. Dalam proses ini, nilai intensitas piksel pada gambar ditingkatkan atau dikurangi secara seragam. Penyesuaian kecerahan ini berguna untuk menciptakan variasi dalam dataset dengan memperkenalkan perubahan pada kondisi pencahayaan. Proses ini membantu model untuk belajar dengan lebih baik dalam berbagai kondisi pencahayaan sehingga meningkatkan kemampuan model untuk mengenali objek pada berbagai lingkungan pencahayaan yang mungkin dihadapi dalam situasi nyata.

Misalnya, melalui penyesuaian kecerahan, model dapat belajar untuk mengenali objek dengan benar, baik pada kondisi pencahayaan yang terang maupun redup. Hal ini membantu meningkatkan invariansi model terhadap variasi pencahayaan sehingga model lebih robust dalam mengenali objek pada berbagai kondisi lingkungan.

Dengan menerapkan penyesuaian kecerahan, kita dapat menghasilkan variasi lebih luas dalam dataset, yang saat gilirannya membantu meningkatkan kinerja dan generalisasi model pada tugas-tugas pengolahan gambar.

brightness_adjustment = transforms.Compose([
    transforms.ColorJitter(brightness=0.5)  # Penyesuaian kecerahan sebesar +50%
])
brightened_image = brightness_adjustment(image)

darkness_adjustment = transforms.Compose([
    transforms.ColorJitter(brightness=0.5)  # Penyesuaian kecerahan sebesar +50%
])
darkened_image = darkness_adjustment(image)



- Contrast Adjustment atau Penyesuaian Kontras
Penyesuaian kontras adalah proses peningkatan atau pengurangan kontras gambar. Kontras mengacu pada perbedaan antara intensitas piksel yang berdekatan dalam gambar. Dengan menyesuaikan kontras, kita dapat membuat perbedaan antara piksel yang berdekatan menjadi lebih jelas atau samar.

Teknik penyesuaian kontras membantu model untuk mengatasi variasi tingkat kontras dalam gambar. Dalam beberapa situasi, gambar mungkin memiliki kontras rendah, yaitu perbedaan antara piksel yang berdekatan tidak terlalu jelas. Di sisi lain, gambar dengan kontras tinggi memiliki perbedaan yang sangat jelas antara piksel yang berdekatan. Kita dapat membantu model untuk belajar lebih baik dari gambar-gambar dengan berbagai tingkat kontras melalui penyesuaian.

contrast_adjustment = transforms.Compose([
    transforms.ColorJitter(contrast=0.5)  # Penyesuaian kontras sebesar +50%
])
high_contrast_image = contrast_adjustment(image)

low_contrast_adjustment = transforms.Compose([
    transforms.ColorJitter(contrast=0.5)  # Penyesuaian kontras sebesar +50%
])
low_contrast_image = low_contrast_adjustment(image)

- Cropping
Cropping adalah teknik augmentasi data dengan cara memotong atau menghapus bagian-bagian tertentu dari gambar. Dengan melakukan cropping, kita dapat memfokuskan model pada fitur-fitur penting dalam gambar dan meningkatkan invariansi terhadap posisi objek.

Teknik cropping berguna untuk mengurangi informasi yang tidak relevan atau noise dalam gambar sehingga memungkinkan model lebih fokus pada fitur-fitur yang penting saat pengambilan keputusan. Selain itu, cropping juga dapat membantu model untuk belajar invariansi terhadap perubahan posisi objek pada gambar sehingga meningkatkan kemampuan model mengenali objek dalam berbagai posisi.

Misalnya, dalam tugas-tugas pengenalan objek, cropping dapat digunakan untuk memotong gambar sehingga hanya bagian-bagian mengandung objek yang dipertahankan. Hal ini dapat membantu model untuk belajar membedakan objek dari latar belakang dan meningkatkan akurasi pengenalan objek.

def crop_image(image, size):
    width, height = image.size
    left = (width - size) / 2
    top = (height - size) / 2
    right = (width + size) / 2
    bottom = (height + size) / 2
    cropped_image = image.crop((left, top, right, bottom))
    return cropped_image


- Shearing
Shearing adalah teknik augmentasi data dengan cara meregangkan atau melenturkan gambar dalam satu arah tertentu. Hal ini membantu model untuk mempelajari distorsi geometris dalam gambar dan meningkatkan invariansi terhadap transformasi geometris.

Pada proses shearing, setiap piksel digeser sejajar dengan sumbu tertentu dengan jumlah yang berbeda-beda tergantung posisinya dalam gambar. Proses ini menciptakan distorsi yang berguna pada data pelatihan, yang membantu model untuk belajar mengenali objek dalam berbagai posisi atau sudut pandang.

Misalnya, dalam tugas-tugas pengenalan objek, objek pada gambar mungkin mengalami distorsi geometris akibat sudut pengambilan gambar atau perspektif. Dengan menerapkan shearing pada gambar-gambar dalam dataset, kita dapat membantu model untuk belajar mengenali objek meskipun mengalami distorsi semacam itu.

shear = transforms.RandomAffine(0, shear=20)


- Resolusi
Pilih resolusi yang tepat untuk gambar-gambar tergantung pada kompleksitas tugas dan kebutuhan komputasi Anda. Kadang-kadang, Anda mungkin perlu menyesuaikan resolusi gambar untuk mempercepat waktu pelatihan atau memperbaiki kinerja model.

- Labeling
Pastikan setiap gambar diberi label dengan benar sesuai dengan kategori atau kelas yang tepat. Label yang akurat sangat penting dalam melatih model dan mengevaluasi kinerjanya.



4. Pembuatan Model CNN
Selanjutnya, model CNN dibangun. Tahapan ini melibatkan penentuan arsitektur model, termasuk jumlah serta jenis lapisan, ukuran filter, activation function, dan lain-lain. Arsitektur ini dapat disesuaikan dengan kebutuhan tugas klasifikasi dan kompleksitas dataset. 

Pertama-tama, Anda perlu mempertimbangkan kompleksitas masalah klasifikasi gambar yang dihadapi. Apakah ini masalah klasifikasi gambar yang sederhana atau kompleks? Jika memiliki dataset yang relatif sederhana, Anda mungkin ingin menggunakan arsitektur CNN lebih sederhana dengan jumlah lapisan lebih sedikit dan ukuran kernel lebih kecil. Namun, jika memiliki dataset yang lebih kompleks, Anda mungkin perlu menggunakan arsitektur lebih dalam dengan lebih banyak convolution layer.

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
 
# Inisialisasi model Sequential
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, img_channels)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

- Convolutional Layer 1
Jumlah filter: 32
Ukuran kernel: (3, 3)
Activation function: ReLU

- Convolutional Layer 2
Jumlah Filter: 64
Ukuran Kernel: (3, 3)
Activation function: ReLU

- Convolutional Layer 3
Jumlah Filter: 64
Ukuran Kernel: (3, 3)
Fungsi Aktivasi: ReLU

- Output Layer
Jumlah Neuron: Jumlah kelas pada dataset
Fungsi Aktivasi: Softmax

Anda juga perlu mempertimbangkan jumlah kelas yang berbeda dalam dataset. Misalnya, untuk masalah klasifikasi biner, Anda mungkin hanya memerlukan lapisan output dengan satu unit dan activation function sigmoid. Namun, untuk klasifikasi multi-kelas, Anda mungkin memerlukan lapisan output dengan lebih banyak unit dan activation function softmax.

Selanjutnya, Anda dapat mempertimbangkan untuk menambahkan lapisan dropout dalam mencegah overfitting. Lapisan dropout secara acak mematikan sejumlah unit selama pelatihan sehingga mencegah model Anda dari menjadi terlalu spesifik terhadap data pelatihan.

Penambahan lapisan dropout dalam arsitektur CNN berfungsi untuk mencegah overfitting dan meningkatkan kemampuan generalisasi model. Dropout adalah teknik regularisasi yang secara acak menonaktifkan sejumlah unit (neuron) dalam lapisan selama pelatihan dan mencegah model menjadi terlalu bergantung pada neuron tertentu. 

Dalam arsitektur yang diberikan, lapisan dropout pertama ditambahkan setelah setiap lapisan max pooling dengan rasio dropout sebesar 25%, yang berarti 25% dari unit pada lapisan tersebut akan dinonaktifkan secara acak. Setelah lapisan flatten, dropout dengan rasio 50% diterapkan untuk memastikan regularisasi yang lebih kuat sebelum lapisan dense. 

Selanjutnya, dropout dengan rasio 50% juga diterapkan setelah lapisan dense dengan 64 neuron. Penambahan dropout pada berbagai titik ini untuk membuat model lebih robust dan mengurangi kemungkinan overfitting sehingga model dapat berperforma lebih baik dalam data yang belum pernah dilihat.



5. Pelatihan Model
Data pelatihan dimasukkan ke model CNN untuk memulai proses pelatihan. Selama pelatihan, model akan menyesuaikan bobot dan bias berdasarkan data pelatihan dengan menggunakan algoritma backpropagation dan optimisasi yang tepat.

Setelah mendefinisikan arsitektur convolutional neural network (CNN), langkah berikutnya adalah compile dan melatih (fit) model tersebut. Proses ini melibatkan pemilihan optimizer, loss function, dan metrik evaluasi, serta pelatihan model menggunakan data yang telah disiapkan. 

a. Compile Model
Compile model adalah langkah penting sebelum pelatihan. Dalam langkah ini, kita menentukan hal-hal berikut.
- Optimizer: Algoritma yang digunakan untuk meng-update bobot model berdasarkan gradient loss function.
- Loss Function: Fungsi yang digunakan untuk mengukur seberapa baik model memprediksi target.
- Metrics: Metrik yang digunakan untuk mengevaluasi performa model selama pelatihan dan validasi.

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

Dalam tahap ini, kita menentukan beberapa komponen penting, yaitu optimizer, loss function, dan metrics. Optimizer yang dipilih adalah Adam karena terkenal dengan efisiensinya dan kemampuan adaptif dalam mengatur learning rate sehingga sering memberikan performa yang baik pada berbagai tugas machine learning. 

Loss function yang digunakan adalah categorical_crossentropy, cocok untuk masalah klasifikasi multi-kelas, yakni kita memiliki lebih dari dua kategori untuk diprediksi. Selain itu, metrik accuracy digunakan untuk mengevaluasi performa model selama pelatihan dan validasi, memberikan gambaran tentang seberapa sering prediksi model sesuai dengan label sebenarnya.

Setelah compile model, kita dapat menampilkan ringkasan arsitektur model menggunakan perintah model.summary(). Ringkasan ini memberikan informasi rinci tentang struktur model, termasuk jumlah dan jenis layer, ukuran output untuk setiap layer, serta jumlah parameter yang dapat dipelajari pada setiap laye

Informasi ini sangat berguna untuk memahami kompleksitas model dan memastikan bahwa arsitektur telah didefinisikan sesuai dengan kebutuhan masalah yang dihadapi. Ringkasan ini juga membantu dalam debugging dan optimasi lebih lanjut dari arsitektur model.


b. Melatih Model
Setelah model di-compile, kita melatihnya menggunakan metode fit(). Proses ini melibatkan proses memasukkan data pelatihan dan validasi, serta menentukan parameter pelatihan, seperti jumlah epochs dan batch size.
# Callbacks
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')
 
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

Dalam proses pelatihan model CNN, pemantauan dan pengaturan pelatihan adalah kunci untuk memastikan model yang optimal. Dua komponen penting dalam proses ini adalah ModelCheckpoint dan EarlyStopping callbacks.

ModelCheckpoint digunakan untuk menyimpan model dengan performa terbaik selama pelatihan. Dalam kasus ini, nilai loss pada data validasi dipantau, dan hanya model dengan loss terendah yang akan disimpan. Dengan menyimpan model terbaik, Anda dapat menghindari kehilangan kemajuan pelatihan dan menggunakan model yang optimal saat proses pelatihan selesai.

- ModelCheckpoint digunakan untuk menyimpan model terbaik berdasarkan performa pada data validasi.
- Parameter checkpoint menentukan lokasi dan nama file tempat model terbaik akan disimpan. Dalam contoh ini, model terbaik akan disimpan dengan nama "best_model.h5".
- save_best_only=True memastikan bahwa hanya model dengan performa terbaik yang akan disimpan. Jika parameter ini diatur pada True, hanya model dengan nilai loss terendah yang akan disimpan.
- mode='min' menunjukkan bahwa kita ingin mencari nilai loss terendah. Ini sesuai dengan pelatihan model ketika kita ingin meminimalkan loss.


Sementara itu, EarlyStopping digunakan untuk menghentikan pelatihan lebih awal jika tidak ada peningkatan dalam performa pada data validasi. Dengan memonitor nilai loss pada data validasi, Anda dapat mengidentifikasi saat pelatihan tidak lagi menghasilkan peningkatan signifikan dalam performa. Melalui penghentian pelatihan pada saat yang tepat, overfitting dihindari dan memastikan bahwa model yang dihasilkan adalah yang paling generalizable dan tidak terlalu disesuaikan dengan data pelatihan.
- patience=10 menunjukkan durasi kita akan menunggu sebelum menghentikan pelatihan jika tidak ada peningkatan. Dalam contoh ini, kita menunggu sampai 10 epoch berturut-turut tanpa peningkatan.
- restore_best_weights=True memastikan bahwa kita akan menggunakan bobot model terbaik yang telah disimpan selama pelatihan saat memutuskan untuk menghentikan pelatihan. Hal ini membantu memastikan bahwa model yang dihasilkan adalah terbaik berdasarkan performa pada data validasi.

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=epochs,
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size,
    callbacks=[checkpoint, early_stopping]
)

alam kode tersebut, `fit()` digunakan untuk menginisiasi pelatihan model. Parameter-parameter yang diberikan, antara lain, generator data pelatihan, jumlah langkah per epoch, jumlah epoch, generator data validasi, jumlah langkah validasi per epoch, dan penggunaan callback selama proses pelatihan.

Selama pelatihan, model akan menyesuaikan bobotnya berdasarkan hasil nilai loss pada setiap batch data pelatihan, dengan harapan meningkatkan performa model dalam data yang belum pernah dilihat. Callbacks seperti `ModelCheckpoint` dan `EarlyStopping` juga digunakan selama proses pelatihan untuk memantau serta mengatur pelatihan, seperti menyimpan model terbaik dan menghentikan pelatihan jika tidak ada peningkatan dalam performa model.


6. Evaluasi Model
Setelah pelatihan selesai, model dievaluasi menggunakan data validasi untuk mengukur kinerjanya. Evaluasi ini mencakup metrik, seperti akurasi, loss value, precision, recall, dan F1-Score. 

evaluation = model.evaluate(
    test_generator,
    steps=test_generator.samples // test_generator.batch_size
)
 
# Menampilkan hasil evaluasi
print("Loss:", evaluation[0])
print("Accuracy:", evaluation[1])

Untuk melakukan evaluasi, kita menggunakan metode evaluate() pada model yang telah dilatih. Pertama, kita menggunakan generator data pengujian yang telah kita persiapkan sebelumnya. Metode evaluate() akan menghitung loss dan metrik lainnya, seperti akurasi berdasarkan performa model pada data pengujian. 

Setelah proses evaluasi selesai, hasil evaluasi, seperti loss dan akurasi akan dikembalikan sebagai sebuah array. Indeks 0 dari array ini adalah loss, yang menunjukkan seberapa baik model memperkirakan kelas gambar. Sementara itu, indeks 1 adalah akurasi, yang menunjukkan seberapa sering prediksi model sesuai dengan label sebenarnya.

Dengan melakukan langkah evaluasi ini, kita bisa memahami lebih baik tentang seberapa baik model bekerja pada data baru yang belum pernah dilihat. Evaluasi ini penting untuk memastikan bahwa model dapat melakukan generalisasi dengan baik pada data baru dan tidak hanya mengingat data pelatihan.



7. Penyetelan Hyperparameter
Berdasarkan hasil evaluasi, mungkin perlu dilakukan penyetelan hyperparameter pada model untuk meningkatkan kinerja. Tahapan ini dapat melibatkan penyesuaian learning rate, jumlah lapisan atau neuron, ukuran filter, atau parameter lainnya. 

8. Pengujian Model
Setelah dianggap memadai, model diuji menggunakan data uji yang tidak terlihat selama pelatihan atau validasi. Hasil pengujian ini memberikan gambaran tentang seberapa baik model dapat melakukan klasifikasi pada data baru. 

9. Penggunaan Model
Akhirnya, model yang terlatih dan diuji dapat digunakan untuk melakukan klasifikasi gambar pada data baru dalam produksi. Tahapan akhir ini dapat digunakan dalam berbagai aplikasi, seperti klasifikasi objek pada gambar, deteksi wajah, atau pengenalan karakter tulisan tangan.


10. Penanganan Overfitting dalam Klasifikasi Gambar:
Overfitting adalah masalah yang sering muncul dalam klasifikasi gambar, yakni ketika model pembelajaran mesin atau jaringan saraf tiruan belajar terlalu detail dari data pelatihan, termasuk noise atau variasi acak yang tidak relevan. Hal ini menyebabkan model menjadi sangat baik ketika mengenali dan mengingat data pelatihan, tetapi gagal dalam menggeneralisasi pola sama pada data baru yang tidak terlihat sebelumnya. 

Akibatnya, performa model cenderung menurun ketika diuji dengan data baru karena model tidak mampu menangkap fitur-fitur penting yang dapat diterapkan secara umum. Overfitting sering terjadi ketika model terlalu kompleks, data pelatihan tidak mencukupi, atau tidak ada strategi yang efektif untuk mencegah model dari mempelajari detail-detail tidak relevan. Oleh karena itu, penting menerapkan berbagai teknik untuk mengurangi overfitting agar model dapat memiliki performa yang konsisten dan akurat pada data baru.


Solusi untuk model overfitting:
a. Data Augmentation
Teknik ini melibatkan transformasi gambar pelatihan untuk menciptakan variasi tambahan tanpa menambah data baru. Contohnya termasuk rotasi, flipping, cropping, zooming, dan perubahan brightness. Data augmentation membantu model menjadi lebih robust dan generalizable.

b. Regularization
Regularization membantu mengurangi kompleksitas model dengan menambahkan penalti pada fungsi loss untuk parameter-parameter yang besar atau kompleks. Berikut beberapa bentuk regularization yang umum digunakan.

a). Dropout
Dropout adalah teknik regularisasi yang digunakan dalam pelatihan jaringan saraf tiruan untuk mencegah overfitting. Selama pelatihan, dropout menonaktifkan neuron secara acak dalam lapisan tertentu dengan probabilitas yang telah ditentukan. Ini berarti setiap neuron memiliki peluang untuk "drop out" (tidak aktif) selama proses forward pass dan backward pass.

Akibatnya, jaringan tidak dapat terlalu bergantung pada neuron tertentu, tetapi harus belajar untuk mendistribusikan informasi secara lebih merata di seluruh jaringan. Dropout biasanya diterapkan pada lapisan dense atau fully connected, yakni ketika koneksi antar neuron sangat banyak, sehingga risiko overfitting lebih tinggi.

Contohnya, jika dropout rate diatur pada 0.5, sekitar 50% dari neuron dalam lapisan tersebut akan dinonaktifkan secara acak pada setiap iterasi pelatihan. Setelah pelatihan selesai, semua neuron diaktifkan kembali untuk proses inferensi (prediksi), tetapi nilai bobot neuron yang dilatih disesuaikan dengan mempertimbangkan penggunaan dropout rate. Dengan cara ini, dropout membantu menciptakan jaringan yang lebih robust dan mampu menggeneralisasi lebih baik pada data yang belum pernah dilihat.

b). L2 Regularization (Weight Decay)
L2 Regularization (Weight Decay) adalah teknik untuk mencegah overfitting dalam jaringan saraf tiruan dengan menambahkan penalti terhadap bobot yang besar. Tujuan dari L2 Regularization adalah membuat model lebih sederhana dan generalizable dengan menghindari pembelajaran bobot yang terlalu besar, yang dapat menyebabkan model terlalu menyesuaikan data pelatihan.

Metode ini memaksa bobot untuk tetap kecil selama pelatihan dan mencegah model menjadi terlalu kompleks ataupun terlalu cocok dengan data pelatihan. Jadi, model lebih mungkin untuk digeneralisasi dengan baik pada data baru yang tidak terlihat sebelumnya.

c. Early Stopping
Early stopping adalah teknik regularisasi yang digunakan untuk mencegah overfitting dalam pelatihan model pembelajaran mesin, khususnya jaringan saraf tiruan. Teknik ini melibatkan pemantauan performa model pada set validasi selama pelatihan dan menghentikan pelatihan ketika performa mulai menurun, bahkan jika performa pada data pelatihan terus meningkat.

Proses early stopping bekerja sebagai berikut.

a). Monitoring: Selama pelatihan, performa model dievaluasi secara berkala pada set validasi, bukan hanya pada set pelatihan.

b). Criteria: Tentukan kriteria penghentian, contohnya, jika performa (misalnya akurasi atau loss) pada set validasi tidak membaik setelah sejumlah epoch tertentu (disebut "patience").

c). Stop Training: Jika performa pada set validasi mulai menurun dan tidak membaik setelah jumlah epoch sesuai dengan nilai patience, pelatihan dihentikan.

Tujuan dari early stopping adalah menghentikan pelatihan pada titik model masih dapat digeneralisasi dengan baik pada data yang belum pernah dilihat. Ini mencegah model belajar terlalu detail dari data pelatihan, termasuk noise atau variasi acak tidak relevan, yang dapat mengakibatkan overfitting.

Misalnya, jika model terus dilatih meskipun performa pada set validasi tidak membaik atau malah menurun, model mungkin belajar pola-pola spesifik yang hanya ada dalam data pelatihan dan tidak berlaku secara umum. Dengan menghentikan pelatihan pada waktu yang tepat, early stopping membantu model untuk tetap sederhana dan generalizable.


d. Menggunakan Model yang Lebih Sederhana
Menggunakan model yang lebih sederhana adalah strategi efektif untuk mencegah overfitting, terutama ketika bekerja dengan dataset yang tidak cukup besar. Model yang terlalu kompleks memiliki kapasitas sangat tinggi untuk belajar detail-detail dalam data pelatihan, termasuk noise atau variasi acak, dan dapat mengakibatkan overfitting. Sebaliknya, model yang lebih sederhana, dengan jumlah parameter lebih sedikit, cenderung lebih baik dalam generalisasi karena tidak menangkap detail tidak relevan dari data pelatihan.

Caranya:
a). Mengurangi Jumlah Layer
Kurangi jumlah layer dalam jaringan saraf. Misalnya, alih-alih menggunakan arsitektur dengan 50 layer (seperti ResNet-50), Anda dapat menggunakan arsitektur dengan 18 layer (seperti ResNet-18) jika dataset tidak cukup besar.

b). Mengurangi Jumlah Neuron per Layer
Kurangi jumlah neuron dalam setiap layer. Ini mengurangi kapasitas model untuk menghafal data pelatihan secara berlebihan.

c). Menggunakan Model Pre-Trained dengan Fine-Tuning Minimal
Alih-alih melatih model yang sangat kompleks dari awal, gunakan model pre-trained dan lakukan fine-tuning hanya pada layer terakhir. Contoh: menggunakan MobileNet atau SqueezeNet daripada menggunakan model yang lebih kompleks, seperti VGG atau Inception

d). Menurunkan Resolusi Gambar
Kurangi resolusi gambar masukan jika memungkinkan. Gambar dengan resolusi lebih rendah memiliki informasi lebih sedikit untuk diproses sehingga model dapat lebih sederhana.


e. Cross-Validation
Cross-validation adalah teknik untuk memastikan model machine learning dapat bekerja dengan baik pada data baru melalui pembagian data pelatihan. Bayangkan Anda sedang belajar untuk ujian dan ingin memastikan bahwa benar-benar paham dengan materinya. Alih-alih hanya mengulang-ulang seluruh materi sekaligus, Anda membaginya menjadi beberapa bagian dan menguji diri pada setiap bagian secara bergantian.

Mari kita gunakan penjelasan rinci untuk menggambarkan 5-fold cross-validation dengan 5 kali iterasi seperti pada gambar di atas. Bayangkan Anda memiliki sebuah dataset yang terdiri dari 100 gambar. Dalam 5-fold cross-validation, kita membagi dataset ini menjadi 5 bagian (fold) sama besar, masing-masing berisi 20 gambar. Selanjutnya, kita melatih dan menguji model sebanyak 5 kali (5 iterasi), yaitu setiap bagian secara bergantian digunakan sebagai set validasi, sementara 4 bagian lainnya digunakan sebagai set pelatihan.

Setelah kelima iterasi selesai, kita memiliki lima nilai performa dari lima set validasi yang berbeda. Nilai-nilai ini dirata-rata untuk memberikan estimasi lebih akurat tentang kemampuan generalisasi model pada data yang belum pernah dilihat.


f. Penggunaan Teknik Transfer Learning
Penggunaan teknik transfer learning adalah metode pengambilan model yang sudah dilatih pada dataset besar dan umum, seperti ImageNet, serta menyesuaikannya untuk tugas yang lebih spesifik.

Misalnya, model seperti ResNet atau VGG sudah dilatih untuk mengenali berbagai objek pada jutaan gambar. Kita bisa menggunakan model ini dan hanya melakukan penyesuaian (fine-tuning) pada dataset kita yang lebih kecil dan spesifik, seperti gambar anjing dan kucing. Dengan transfer learning, kita memanfaatkan fitur kuat yang sudah dipelajari oleh model sehingga tidak perlu melatihnya dari awal. Tentunya metode ini dapat menghemat waktu dan sumber daya karena model sudah memiliki pengetahuan dasar tentang fitur gambar umum.



Pengenalan Transfer Learning:
Bayangkan Anda pindah ke kota baru dan perlu menemukan jalan ke tempat kerja. Anda bisa mencoba mempelajari setiap jalan sendiri, tetapi itu akan memakan banyak waktu dan tenaga. Selain itu, Anda bisa menggunakan peta dan aplikasi navigasi yang sudah ada, yang dibuat berdasarkan pengalaman ribuan orang yang sudah menjelajahi kota itu. Aplikasi ini sudah tahu rute tercepat, kondisi lalu lintas, dan tempat-tempat penting.

Transfer learning mirip dengan menggunakan aplikasi navigasi ini. Daripada melatih model dari awal, Anda menggunakan model yang sudah dilatih pada dataset besar. Model ini sudah memahami fitur-fitur dasar dalam gambar, seperti tepi, bentuk, dan pola. Kemudian, Anda hanya perlu menyesuaikan model ini dengan data spesifik). Tentunya metode ini dapat menghemat banyak waktu dan usaha, serta membantu model Anda bekerja dengan baik lebih cepat.

Ide di balik transfer learning adalah bahwa model yang telah dilatih pada dataset besar dan kompleks, seperti ImageNet yang berisi jutaan gambar, sudah memiliki pengetahuan umum tentang fitur-fitur visual bermanfaat. Pertimbangkan contoh kasus ketika Anda ingin melatih model untuk mengklasifikasikan gambar-gambar hewan, tetapi hanya memiliki dataset yang relatif kecil. 

Alih-alih melatih model dari awal dan memerlukan jumlah data besar untuk belajar fitur-fitur penting, Anda dapat memanfaatkan model yang sudah dilatih pada dataset besar, seperti model ResNet atau VGG yang dilatih pada ImageNet.

Pertama, model yang sudah dilatih dimanfaatkan sebagai kerangka dasar untuk model kita. Artinya, kita menggunakan model yang sudah ada dan diakui keahliannya, lalu menghilangkan bagian paling atasnya yang digunakan untuk tugas klasifikasi sebelumnya. Setelah itu, kita menambahkan lapisan-lapisan baru sesuai dengan tugas klasifikasi kita sendiri, seperti mengenali hewan pada dataset kita.

Kemudian, kita melakukan fine-tuning, yakni kita melatih ulang model menggunakan dataset sendiri. Di sini, beberapa bagian dari model  dibiarkan untuk disesuaikan dengan data kita, sementara bagian-bagian lainnya tetap tidak berubah atau "membeku". Ini memungkinkan model untuk menyesuaikan diri dengan data baru kita sambil tetap mempertahankan pengetahuan yang sudah ada sebelumnya.


Beberapa Model untuk Transfer Learning:

Dalam dunia deep learning, transfer learning adalah cara “pintar” untuk membuat model mengenal gambar kita menjadi lebih cerdas. Ide di balik transfer learning adalah menggunakan pengetahuan yang sudah ada dari model-model yang sudah dilatih pada banyak gambar, misalnya model yang dilatih pada dataset besar, seperti ImageNet. Dengan cara ini, kita tidak perlu memulai dari awal saat melatih model sendiri dan tentunya dapat menghemat waktu dan usaha yang diperlukan. 

Sub-modul ini berisi penjelasan beberapa model populer yang sudah ada dan cara kita bisa menggunakannya untuk membuat model pengenalan gambar sendiri menjadi lebih baik.

a. VGG (Visual Geometry Group)
VGG adalah arsitektur yang relatif sederhana, tetapi sangat efektif dalam pengenalan gambar. Arsitektur ini terdiri dari lapisan konvolusi dan max pooling yang dalam, diikuti oleh beberapa lapisan fully connected. Model VGG telah dilatih dalam dataset ImageNet yang besar dan memiliki kemampuan baik untuk mengenali berbagai objek pada gambar. Versi terkenalnya adalah VGG16 dan VGG19.

b. ResNet (Residual Network)
ResNet adalah arsitektur yang sangat dalam, terdiri dari puluhan hingga ratusan lapisan. Salah satu inovasi utamanya adalah blok residual, yang memungkinkan pembelajaran lebih efisien dalam jaringan sangat dalam. ResNet memiliki performa sangat baik dalam pengenalan gambar dan telah dilatih pada dataset ImageNet. Versi terkenalnya ResNet50, ResNet101, dan ResNet152.

c. Inception (GoogLeNet)
Inception, juga dikenal sebagai GoogLeNet, adalah arsitektur yang dikembangkan oleh Google. Arsitektur ini terkenal karena penggunaan modul Inception, yang memungkinkan model untuk mengekstraksi fitur pada berbagai skala secara efisien. Inception memiliki tingkat akurasi yang tinggi dalam pengenalan gambar dan telah dilatih pada dataset ImageNet. Versi terkenalnya InceptionV3 dan InceptionV4.

d. MobileNet
MobileNet adalah arsitektur yang dirancang khusus untuk aplikasi mobile dan perangkat dengan sumber daya terbatas. Arsitektur ini lebih ringan dan memiliki ukuran lebih kecil daripada arsitektur lainnya, tetapi masih memiliki performa yang baik dalam pengenalan gambar. MobileNet sering digunakan dalam aplikasi, yakni saat keterbatasan sumber daya komputasi menjadi pertimbangan utama. Versi terkenalnya MobileNetV1, MobileNetV2, dan MobileNetV3.

e. Xception
Xception adalah arsitektur yang dikembangkan oleh Google, berdasarkan pada ide-ide dari Inception. Xception menggunakan depthwise separable convolution, yang memisahkan operasi konvolusi menjadi dua tahap, yaitu konvolusi spasial dan konvolusi saluran. Ini membuatnya lebih efisien secara komputasi daripada arsitektur konvensional lainnya.

f. DenseNet
DenseNet adalah arsitektur yang membangun koneksi antara setiap lapisan dalam jaringan secara langsung. Hal ini memungkinkan informasi untuk mengalir langsung dari lapisan ke lapisan lainnya, mengurangi kemungkinan hilangnya informasi dalam jaringan. DenseNet telah menunjukkan performa yang baik dalam pengenalan gambar. Versi terkenalnya adalah DenseNet121, DenseNet169, dan DenseNet201.



Latihan Klasifikasi Gambar:
Setelah melatih model CNN, Anda menggunakan confusion matrix untuk mengevaluasi performanya. Apa tujuan utama dari menggunakan confusion matrix dalam klasifikasi gambar?
A. Mengevaluasi akurasi dan kesalahan prediksi


Anda mendapati bahwa model CNN Anda kesulitan dalam mengklasifikasikan gambar yang kabur atau buram. Langkah apa yang paling efektif untuk memperbaiki masalah ini?
B. Mengumpulkan lebih banyak data buram
C. Mengurangi ukuran batch selama pelatihan
D. Menggunakan teknik augmentasi data seperti blur


Anda sedang mengevaluasi performa model menggunakan metrik F1-score. Dalam konteks klasifikasi gambar yang memiliki ketidakseimbangan kelas yang signifikan, mengapa F1-score lebih cocok dibandingkan akurasi?
C. Karena F1-score memperhitungkan precision dan recall secara seimbang

Dalam proses pelatihan model deep learning untuk klasifikasi gambar, langkah preprocessing apa yang paling penting untuk meningkatkan akurasi model?
A. Normalisasi gambar dan augmentasi data


Anda memiliki dua model CNN dengan akurasi yang sama pada dataset validasi. Model A membutuhkan waktu pelatihan yang lebih singkat dibandingkan Model B. Apa alasan memilih Model A untuk deployment?

a. Karena lebih cepat dilatih dan diimplementasikan



---------------------------------------------------------------------------
                             Sistem Rekomendasi
---------------------------------------------------------------------------
Di era informasi yang berlimpah dan berharga, sistem rekomendasi menjadi teman dekat kita tanpa disadari. Dari rekomendasi film yang menghibur hingga rekomendasi produk yang memenuhi kebutuhan, kita tidak lagi tenggelam dalam lautan informasi yang terbatas.

Dalam modul ini, kalian akan menemani Diana menjelajahi inti dari sistem-sistem yang menjadikan platform-platform besar seperti Netflix, Amazon, dan Spotify begitu relevan dalam kehidupan kita sehari-hari. Kalian akan memahami algoritma di balik layar yang memprediksi preferensi kita dengan akurasi yang memukau.

Sistem rekomendasi memprediksi rating atau preferensi pengguna terhadap item tertentu. Rekomendasi ini dibuat berdasarkan perilaku pengguna di masa lalu atau perilaku pengguna lainnya. Jadi, sistem ini akan merekomendasikan sesuatu terhadap pengguna berdasarkan data perilaku atau preferensi dari waktu ke waktu. 

Secara umum, ada dua buah jenis rekomendasi yang sering digunakan, yaitu rekomendasi beranda dan rekomendasi produk terkait. 

Rekomendasi beranda adalah rekomendasi yang umum kita temui sehari-hari. Contohnya seperti ketika Anda membuka halaman Google Play Store, di sana akan terlihat aplikasi yang mungkin menarik minat Anda. Begitu juga pada layanan streaming seperti Netflix. Ketika mengunjungi beranda, terdapat film-film yang mungkin menarik untuk Anda tonton.

Di lain sisi, rekomendasi produk terkait preferensi pengguna sering kali kita temui pada e-commerce. Contohnya, jika Anda menggunakan Tokopedia dan mencari sebuah item, di sana akan terlihat produk lain yang terkait dengan produk yang sedang atau pernah Anda cari.

Rekomendasi sistem yang dibangun oleh Netflix berjalan sesuai dengan tugasnya. Hal ini dibuktikan oleh meningkatnya waktu streaming setelah menerapkan rekomendasi sistem tersebut. Menurut Todd Yellin sebagai wakil presiden dari divisi product mengatakan bahwa lebih dari 80% pengikut memercayai dan mengikuti hasil rekomendasi yang diberikan oleh sistem rekomendasi Netflix. Di sisi lain, Netflix percaya bahwa mereka dapat kehilangan hingga 1 miliar dollar setiap tahunnya jika tidak menggunakan sistem rekomendasi tersebut. 

Netflix menggunakan beberapa data yang diambil dari masing-masing pengguna sehingga dapat memperbesar kemungkinan pengguna tersebut menonton film karena sesuai dengan karakteristiknya. Faktor yang diambil dan digunakan pada sistem rekomendasi Netflix yaitu

rating yang diberikan, 
riwayat pencarian atau menonton, 
kesamaan dengan pengguna lainnya, 
informasi terkait film yang ditonton, 
jumlah waktu menonton dalam sehari, 
gawai yang digunakan, dan
waktu regional ketika pengguna menonton (siang/sore/malam). 

Setelah data yang dibutuhkan untuk menghasilkan rekomendasi dikumpulkan, selanjutnya data tersebut akan digunakan untuk menjadi input yang akan diproses oleh algoritma. Hasil dari proses itulah yang nantinya akan digunakan oleh Netflix.

Mengapa Netflix menggunakan baris dan kolom pada hasil rekomendasinya? Hal tersebut dijelaskan bahwa ada keuntungan dari dua buah sudut pandang yang berbeda yaitu pengguna dan perusahaan. Sebagai pengguna, itu membuat rekomendasi yang disajikan lebih koheren karena memiliki hubungan yang erat pada baris atau kolom yang sama. Selain itu, sebagai perusahaan akan mudah mendapatkan feedback ketika pengguna terindikasi tidak menyukai urutannya dengan cara melakukan scroll dan mengabaikan rekomendasinya. 

Berdasarkan sudut pandang perusahaan tersebut, Netflix melakukan model maintenance berdasarkan feedback dari pengguna agar dapat memberikan rekomendasi yang relevan setiap waktunya. Netflix akan melakukan pelatihan ulang model untuk meningkatkan akurasi dari prediksi yang diberikan berdasarkan film yang paling sering ditonton oleh pengguna.

Sebagai informasi, ada dua pendekatan utama dalam membangun sebuah sistem rekomendasi, yaitu Content-Based Filtering (CBF) dan Collaborative Filtering (CF). Perbedaan utama antara CF dan CBF adalah dalam sumber informasi yang digunakan untuk memberikan rekomendasi. CF menggunakan informasi dari aktivitas pengguna (misalnya, penilaian atau histori pembelian), sementara CBF menggunakan informasi dari fitur-fitur atau karakteristik item tersebut (misalnya, genre untuk film atau atribut-atribut produk untuk barang dagangan).


Collaborative Filtering:
Collaborative filtering merupakan sebuah pendekatan yang menggunakan informasi dari aktivitas pengguna untuk memberikan rekomendasi. CF mencoba menemukan pola dan hubungan antara pengguna dan item. Secara umum terdapat dua jenis CF.

1. Collaborative Filtering Berbasis Pengguna (User-Based CF): mengidentifikasi pengguna yang memiliki preferensi serupa dengan pengguna yang aktif saat ini, lalu merekomendasikan item yang disukai oleh pengguna tersebut. Pendekatan ini bergantung pada kesamaan antar pengguna.

2. Collaborative Filtering Berbasis Item (Item-Based CF): mengidentifikasi item yang sering disukai oleh pengguna yang memiliki preferensi serupa dengan pengguna yang aktif saat ini, lalu merekomendasikan item tersebut. Pendekatan ini bergantung pada kesamaan antar item.

Bayangkan jika kita memulai sebuah startup penyedia gim seperti Steam. Saat ini kita memiliki 4 orang yang berlangganan. Dari 4 orang tersebut, kita telah mengumpulkan data tentang gim apa saja yang mereka sukai.

Dari data di atas, kira-kira gim apakah yang cocok direkomendasikan untuk pengguna bernama Gilang? Yup, secara intuitif kita dapat mencari pengguna yang memiliki selera gim yang sama dengan Gilang, lantas merekomendasikan gim padanya.

Dapat dilihat jika pengguna bernama Gheddi memiliki selera yang mirip dengan Gilang karena mereka sama-sama menyukai gim Free Fire dan PUBG. Inilah yang dinamakan dengan collaborative filtering berbasis pengguna. 

Ketika kita mengembangkan model menggunakan collaborative filtering, kita dapat mengubah like yang diberikan pengguna untuk setiap gim menjadi sebuah vektor yang memiliki elemen bernilai 1 jika pengguna menyukai gim tersebut dan 0 jika tidak suka.

Eh, lalu bagaimana mengubah kumpulan angka tersebut menjadi rekomendasi? Tenang, untuk mencari kemiripan sebuah objek dengan objek lainnya agar dapat memberikan rekomendasi, kita bisa menggunakan cosine similarity.

Cosine similarity adalah metrik yang digunakan untuk mengukur seberapa mirip dua vektor dalam ruang berdimensi banyak. Ini adalah ukuran kosinus sudut antara dua vektor dalam ruang Euclidean.

Dalam konteks sistem rekomendasi, cosine similarity sering digunakan untuk mengukur seberapa mirip dua item atau dua profil pengguna. Misalnya, dalam collaborative filtering, cosine similarity sering digunakan untuk mengukur kesamaan antara dua profil pengguna atau dua item berdasarkan preferensi pengguna. Semakin tinggi nilai cosine similarity antara dua vektor, semakin serupa kedua vektor tersebut. 

Rumus cosine similarity antara dua vektor a dan b yaitu sebagai berikut.

                               a.b
Cosine Similarity (a, b) = ------------
                           ||a|| ||b||

- a.b adalah hasil perkalian titik (dot product) dari vektor a dan b
- ||a|| dan ||b|| adalah panjang Euclidean dari vektor a dan b masing-masing. 

Cosine similarity memiliki rentang nilai dari -1 hingga 1. Nilai 1 menunjukkan bahwa vektor-vektor tersebut identik, nilai 0 menunjukkan bahwa vektor-vektor tersebut ortogonal (tidak ada kesamaan), dan nilai -1 menunjukkan bahwa vektor-vektor tersebut berlawanan arah (sangat berbeda). Dalam konteks sistem rekomendasi, nilai cosine similarity yang lebih tinggi antara dua item atau dua profil pengguna menunjukkan bahwa mereka lebih mirip satu sama lain. 

Mari kita selisik salah satu pengguna agar memberikan bayangan dalam melakukan perhitungan menggunakan cosine similarity. Pada kasus ini, secara kasat mata tentu Anda akan memilih Gheddi untuk memberikan rekomendasi gim yang ia mainkan kepada Gilang. Namun, karena kita sedang membahas machine learning, tentunya komputer tidak memiliki kemampuan yang sama seperti manusia. Komputer perlu melakukan perhitungan/komputasi agar dapat memberikan rekomendasi yang tepat kepada Gilang.

Gheddi memiliki sebuah vektor [1,1,1,0,0], mari kita anggap variabel a menampung nilai vektor Gheddi. Lalu untuk Gilang, kita akan menganggap vektor miliknya disimpan pada variabel b sehingga kita memiliki data seperti berikut.

a = [1,1,1,0,0]
b = [1,1,0,0,0]

Langkah pertamanya adalah menghitung dot product (hasil perkalian titik) antara dua vektor

a.b = (1×1)+(1×1)+(1×0)=1+1+0=2

Selanjutnya, kita akan menghitung panjang Euclidean dari masing-masing vektor.
              2   2   2
||a|| = sqrt(1 + 1 + 1) = sqrt(3) = 1.7
||b|| = sqrt(2) = 1.414

cosine similarity(a.b) = 0.816

Jadi, tingkat kemiripan antara Gheddi dan Gilang adalah 0.816 dengan ketentuan semakin mendekati angka satu, semakin mirip juga kedua vektor tersebut. Untuk penjelasan tentang cosine similarity yang lebih detail, silakan Anda kunjungi artikel Cosine Similarity MachineX ya. Setelah menghitung seluruh pengguna yang ada, hasilnya akan seperti berikut.

Dapat dilihat bahwa Gheddi dan Gilang memiliki cosine similarity yang paling tinggi sehingga kita dapat merekomendasikan gim yang disukai Gheddi pada Gilang. Begitulah kira-kira proses kecil yang terjadi di belakang layar pada aplikasi yang Anda gunakan.



Content Based Filtering:
Salah satu solusinya adalah menggunakan content based filtering. Content-Based Filtering (CBF) adalah pendekatan dalam sistem rekomendasi yang menggunakan karakteristik atau fitur dari item serta profil preferensi pengguna untuk memberikan rekomendasi.

Dalam CBF, item-item direkomendasikan kepada pengguna berdasarkan kesamaan antara item yang ada dan item yang disukai pengguna sebelumnya. Ini berarti jika seorang pengguna menyukai item tertentu, sistem akan mencoba merekomendasikan item lain yang memiliki fitur atau karakteristik yang mirip dengan item yang disukai tersebut.

Misalnya, dalam sistem rekomendasi gim, fitur-fitur seperti genre, sudut pandang pemain, pembuat, dan plot dapat digunakan untuk menggambarkan setiap gim. Ketika seorang pengguna menyukai gim tertentu, sistem akan mencari gim lain yang memiliki fitur-fitur yang mirip dengan gim yang disukai oleh pengguna tersebut. Jika seorang pengguna menyukai gim FPS dengan genre tertentu, sistem akan mencoba merekomendasikan gim FPS lain yang juga memiliki pembuat yang sama atau genre yang serupa.

Keuntungan dari CBF adalah tidak memerlukan informasi dari pengguna lain, hanya memerlukan informasi tentang item itu sendiri. Ini membuat CBF lebih cocok untuk situasi di mana data pengguna terbatas atau di mana privasi pengguna menjadi perhatian utama.

Namun, kelemahan dari CBF adalah kurangnya kemampuan untuk merekomendasikan item yang tidak memiliki fitur yang sama dengan item yang disukai pengguna sebelumnya. Ini dapat menghasilkan kurangnya variasi dalam rekomendasi dan membuat pengguna kehilangan kesempatan untuk menemukan item baru yang mungkin tidak memiliki fitur yang sama dengan item yang disukai sebelumnya.

Mengapa hal itu terjadi? Itu karena content based filtering menerapkan meta-feature atau fitur bawaan dari objek. Pada kasus rekomendasi gim kita, meta feature dapat berupa apakah sebuah gim memiliki mode battle royale? apakah genre-nya action? Apakah game tersebut memiliki genre sport? 

Dari gambar di atas, dapat kita lihat bahwa game Fortnite yang akan rilis memiliki kemiripan dengan game PUBG dan Call of Duty. Gim Fortnite yang baru maupun gim lama seperti PUBG dan Call of Duty sama-sama memiliki mode battle royale. Selain itu, ia juga memiliki genre action shooter dengan game Call of Duty dan PUBG.

Jadi ketika game Fortnite terbaru sudah rilis, kita dapat merekomendasikan game tersebut pada Gheddi dan Gilang. Inilah gambaran umum bagaimana content based filtering bekerja, mudah bukan? 

Nah jika kita rekap kembali content based filtering juga memiliki beberapa kelebihan dan kekurangan. Berikut adalah kelebihan dan kekurangan yang dimiliki content based filtering.

Kelebihan CBF:
- Teknik ini baik dipakai ketika skala user yang besar.
- Teknik ini dapat menemukan ketertarikan spesifik dari seorang user dan dapat merekomendasikan item yang jarang disukai orang lain.

Kekurangan CBF:
- Karena kita yang menentukan meta feature sendiri, kualitas dari rekomendasi tergantung kualitas dari meta feature itu sendiri.

Setelah mengetahui kedua pendekatan yang biasa digunakan untuk membangun sebuah sistem rekomendasi, mungkin sebuah pertanyaan muncul di benak Anda, “Jika ini adalah kali pertama pengguna mengunjungi situs atau menggunakan aplikasi, bagaimana sistem bisa memberikan rekomendasi padahal tidak ada data riwayat apa pun di situs atau aplikasi tersebut?”

Pertanyaan yang bagus karena ini merupakan salah satu masalah penting dalam sistem rekomendasi yang biasa disebut cold start. Cold start adalah masalah yang umum dihadapi dalam pengembangan sistem rekomendasi. Ini terjadi ketika sistem tidak memiliki informasi yang cukup untuk membuat rekomendasi yang relevan kepada pengguna baru atau untuk item baru yang ditambahkan ke dalam platform. Cold start dapat terjadi pada level pengguna, level item, atau keduanya.

1. Cold Start User
Ketika seorang pengguna baru bergabung dengan platform, sistem tidak memiliki data historis tentang preferensi, kebiasaan, atau perilaku mereka. Akibatnya, sistem kesulitan dalam memahami preferensi mereka dan memberikan rekomendasi yang sesuai. Cold start pengguna dapat terjadi pada platform yang baru diluncurkan atau pada pengguna yang baru mendaftar.

2. Cold Start Item
Cold start item terjadi ketika item baru ditambahkan ke dalam platform. tetapi tidak memiliki data atau deskripsi yang cukup untuk dianalisis dengan tepat oleh sistem rekomendasi. Ini bisa terjadi jika platform baru diluncurkan atau jika item baru yang belum populer ditambahkan ke dalam katalog.

3. Cold Start Hybrid
Cold start juga dapat terjadi ketika sistem meluncurkan atau mengalami perubahan signifikan di mana kedua pengguna dan item baru hadir secara bersamaan. Ini merupakan tantangan yang sangat seru karena sistem harus belajar tentang preferensi pengguna dan karakteristik item secara bersamaan.

Cold start merupakan tantangan yang sering dijumpai dalam pengembangan sistem rekomendasi dan memerlukan kombinasi dari strategi yang cerdas dan adaptif untuk mengatasinya. Dengan pendekatan yang tepat, cold start dapat diatasi untuk memberikan pengalaman yang lebih baik bagi pengguna dan memastikan kualitas rekomendasi yang optimal.

Dengan penggunaan sistem rekomendasi yang tepat, diharapkan sistem yang dibangun dapat menyediakan cara untuk mempersonalisasi konten bagi pengguna. Ia seperti filter pada saluran hidran sehingga membuat air yang keluar tidak berlebihan. 

Dalam kasus informasi di internet, sistem rekomendasi menyaring konten yang muncul di layar agar sesuai dengan kebutuhan atau preferensi. Dengan sistem ini, kita bisa menelusuri internet secara efektif dan tidak akan kewalahan dengan berbagai informasi yang bertebaran. 

Selama lebih dari satu dekade, sistem rekomendasi tentu telah banyak mengalami perkembangan dan peningkatan. Ia juga semakin banyak diadopsi dan digunakan oleh berbagai perusahaan, organisasi, atau pelaku bisnis, baik di dalam maupun di luar negeri. Nah, praktisi data dan machine learning tentu perlu memahami bagaimana teknik ini bekerja dan bagaimana menerapkannya pada data yang ada. 

Hal lainnya yang membuat sistem rekomendasi ini terkenal adalah bukan hanya menguntungkan untuk pengguna saja, organisasi atau pelaku bisnis juga dapat meningkatkan revenue dari transaksi yang terjadi sebagai output dari sistem rekomendasi. Dengan kelebihan tersebut, sistem rekomendasi memiliki peran yang sangat penting, baik bagi pengguna, maupun organisasi bisnis. Sistem rekomendasi harus bisa menawarkan dan menyeimbangkan layanan bagi kedua pihak.



Latihan Membangung Sistem Rekomendasi:
Selama beberapa tahun terakhir jaringan saraf telah menghasilkan terobosan besar dalam bidang computer vision dan pemrosesan bahasa alami. Untuk saat ini, para pelaku industri mulai memanfaatkan jaringan saraf untuk mengembangkan sistem rekomendasi yang memperluas implementasi dari jaringan saraf pada industri.

Beberapa sistem baru ini telah menunjukkan performa yang lebih baik dan memberikan rekomendasi yang berkualitas pada user. Dibandingkan dengan model tradisional, sistem rekomendasi dengan jaringan saraf dapat lebih mengerti keinginan pengguna, karakteristik dari produk, dan interaksi antara keduanya.

Lantas bagaimana jaringan saraf dapat dipakai dalam sistem rekomendasi? Caranya, gunakan teknik yang disebut dengan “Embedding.” Embedding adalah sebuah cara untuk merepresentasikan sebuah entitas ke dalam sebuah vektor. Sederhananya, embedding mengubah sebuah objek ke dalam kumpulan angka kontinu. 

Pada gambar di atas, kita mencoba membuat sistem rekomendasi guna merekomendasikan brand yang mungkin akan disukai pengguna. Nah setelah embedding 8 dimensi diaplikasikan pada 4 buah brand di atas, hasilnya adalah vektor 8 dimensi yang merupakan fitur untuk dipelajari oleh model. 

Misalnya, model telah mempelajari fitur “makanan” dan meletakkan fitur ini pada elemen pertama. Kita dapat melihat bahwa brand yang berkaitan dengan makanan memiliki nilai yang tinggi seperti GoFood (0.96) dan GrabFood (0.92), sedangkan brand Maxim memiliki nilai (-0.97) karena tidak berkaitan dengan makanan. Jadi, ketika seorang pengguna senang menggunakan GoFood, kita mungkin dapat merekomendasikan GrabFood kepadanya.

Dengan data yang cukup, kita dapat melatih model untuk mempelajari hubungan antara tiap entitas dan meletakkan entitas yang mirip agar berdekatan. Contoh lainnya adalah ketika kita mencoba merekomendasikan buku berdasarkan genre.

Dapat dilihat bahwa buku-buku dengan genre yang sama, ditempatkan berdekatan pada ruang dimensi. Ketika data telah dikelompokkan seperti di atas, kita dapat memberikan rekomendasi buku apa yang sesuai dengan minat pembaca.

Sampai di sini, kita telah belajar tentang sistem rekomendasi serta memahami konsep-konsep dasar, seperti collaborative filtering, dan content-based filtering. Namun, terasa seperti ada yang kurang ‘kan?

import numpy as np
import pandas as pd
import collections
from mpl_toolkits.mplot3d import Axes3D
from IPython import display
from matplotlib import pyplot as plt
import sklearn
import sklearn.manifold
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
tf.logging.set_verbosity(tf.logging.ERROR)
import altair as alt

Apakah Anda melihat sesuatu yang baru pada library di atas? Yup, pada latihan ini, kita akan menggunakan library Altair untuk membuat visualisasi data agar Anda memiliki pengalaman baru di setiap langkah pembelajaran ini.

Untuk menggunakan Altair ini, kita perlu mengatur parameter renderer agar visualisasi yang kita buat dapat ditampilkan secara langsung pada tools yang digunakan.

alt.data_transformers.enable('default', max_rows=None)
alt.renderers.enable('colab')

Kode di atas memiliki dua buah sintaks yang berperan sebagai ground rules ketika menggunakan Altair sebagai library visualisasi data. Mari kita jabarkan satu per satu.

- alt.data_transformers.enable('default', max_rows=None):
baris ini mengatur transformer data Altair untuk digunakan dalam notebook. Transformer data ini mengubah data mentah menjadi format yang dapat digunakan oleh Altair untuk membuat visualisasi. Dalam baris ini, kita mengatur transformer data menjadi 'default', biasanya parameter ini dapat mengubah data menjadi format yang cocok untuk visualisasi. Pengaturan max_rows=None berperan untuk menghilangkan batasan baris data yang dapat diplot. Ini bermanfaat jika kita memiliki dataset besar yang ingin kita visualisasikan sepenuhnya.

- alt.renderers.enable('colab'):
baris ini mengatur Altair agar dapat digunakan dalam Google Colab. Renderer adalah komponen yang bertanggung jawab untuk menampilkan visualisasi di lingkungan tempat kita bekerja, seperti browser atau notebook. Dalam baris ini, kita mengatur renderer menjadi 'colab' yang cocok untuk penggunaan Altair di Google Colab. Dengan mengatur renderer ini, visualisasi yang kita buat dengan Altair akan ditampilkan secara langsung di Google Colab.

genre_cols = [
    "genre_unknown", "Action", "Adventure", "Animation", "Children", "Comedy",
    "Crime", "Documentary", "Drama", "Fantasy", "Film-Noir", "Horror",
    "Musical", "Mystery", "Romance", "Sci-Fi", "Thriller", "War", "Western"
]

Lalu, apakah Anda masih ingat bahwa genre dari masing-masing film yang ada berupa data biner? Hal tersebut akan menyulitkan proses pelatihan karena nantinya kita akan melakukan proses embeddings seperti pada modul Natural Language Processing sebelumnya. Oleh karena itu, kita perlu membuat sebuah fungsi yang dapat menggabungkan genre menjadi satu buah string.

def mark_genres(movies, genres):
  def get_random_genre(gs):
    active = [genre for genre, g in zip(genres, gs) if g==1]
    if len(active) == 0:
      return 'Other'
    return np.random.choice(active)
  def get_all_genres(gs):
    active = [genre for genre, g in zip(genres, gs) if g==1]
    if len(active) == 0:
      return 'Other'
    return '-'.join(active)
  movies['genre'] = [
      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]
  movies['all_genres'] = [
      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]
 
mark_genres(movies, genre_cols)

Sampai di sini, dataset sudah memiliki fitur yang berguna untuk pelatihan sistem rekomendasi nanti, tetapi struktur dataset sekarang masih berdiri sendiri. Untuk mengatasi permasalahan tersebut, kita dapat melakukan merging agar dataset yang ada menjadi satu buah kesatuan yang utuh.

movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')

Setelah membuat beberapa visualisasi dari data di atas, selanjutnya Anda perlu melakukan eksplorasi data kembali. Pada contoh kasus ini, kita akan melihat film dengan rating terbaik berdasarkan banyak reviu dari pelanggan.

(movies_ratings[['title', 'rating count', 'rating mean']]
 .sort_values('rating count', ascending=True)
 .head(10))

Kode di atas akan menghasilkan dataset terdiri dari fitur title, rating count, dan rating mean. Namun, ada sesuatu yang sepertinya kurang tepat dan bisa mengakibatkan pelatihan model menjadi kurang baik. Apakah Anda menyadari sesuatu? Yup, data yang ditampilkan mencakup film yang hanya memiliki satu buah rating dari pengguna. Hal ini menyebabkan nilai rata-rata rating tidak valid karena hanya ditentukan oleh satu orang saja. 

Lalu, bagaimana cara mengatasinya? Tanpa perlu analisis lebih dalam, kita dapat menentukan threshold atau batas minimal dari jumlah rating pada satu film. Angka minimum ini dapat Anda tentukan sendiri atau menyesuaikan dengan kebutuhan bisnis di tempat Anda bekerja. Selain itu, ada beberapa metode juga untuk menentukan nilai batasan ini salah satunya dengan menggunakan kuartil. Namun, pada contoh ini, mari kita asumsikan bahwa batas minimumnya adalah 20.

(movies_ratings[['title', 'rating count', 'rating mean']]
 .mask('rating count', lambda x: x > 20)
 .sort_values('rating mean', ascending=False)
 .head(10))

Nah, sampai di sini Anda sudah memiliki pengetahuan atau insight dari data yang digunakan. Selanjutnya, kita kembali lagi ke permasalahan yang akan diselesaikan. Tujuan kita adalah membuat sebuah sistem yang dapat merekomendasikan suatu film kepada pengguna. Jika dibuat rumus matematisnya, kurang lebih akan mendapatkan fungsi seperti berikut.

rumus rekomendasi sistem CF.jpeg

Keterangan:
- N adalah jumlah pengguna
- M adalah jumlah film yg tersedia
- Ui adalah urutan yang merepresentasikan pengguna
- Vj adalah urutan yang merepresentasikan film

Untuk mencapai tujuan tersebut, tentunya kita harus melakukan split dataset seperti pada latihan-latihan sebelumnya. Namun, apakah Anda ingin mendapatkan tantangan dan pelajaran yang baru? Wah, semangat yang sangat bagus. Jika sebelumnya kita melakukan split dataset menggunakan TensorFlow dan Scikit-Learn, sekarang waktunya melakukan eksplorasi dengan membuat sebuah fungsi yang dapat memisahkan data latih dan data testing.

def split_dataframe(df, holdout_fraction=0.1):
  test = df.sample(frac=holdout_fraction, replace=False)
  train = df[~df.index.isin(test.index)]
  return train, test

Kode di atas akan memisahkan DataFrame yang anda miliki menjadi data latih dan data uji menyesuaikan dengan parameter yang diberikan. Psstt, ada parameter baru yang belum pernah kita lihat sebelumnya yaitu holdout_fraction. Sebenarnya parameter ini sama seperti parameter test_size pada Scikit-Learn yang berfungsi membagi dataset sesuai dengan persentase dari baris DataFrame yang akan digunakan dalam set pengujian.


Tahukah kalian bahwa dataset yang kita gunakan memiliki matriks yang cukup besar? Pada ratings saja, kita memiliki 10.000 data. Nah, sebelum menggunakan fungsi split_dataframe, sebaiknya kita menggunakan fungsi tf.SparseTensor agar dapat melakukan komputasi dengan lebih efisien. Fungsi SparseTensor ini menggunakan tiga sensor untuk merepresentasikan matriks yaitu indices, values, dan dense_shape. Mari kita bahas ketiga parameter tersebut dengan lebih detail.

Tensor adalah generalisasi dari konsep series, vektor, dan matriks namun tensor itu untuk dimensi yang lebih tinggi. Misalnya, tensor dimensi-3 bisa dibayangkan sebagai kubus data, dan seterusnya untuk dimensi yang lebih tinggi.

1. Indices
Indices adalah tensor yang berisi koordinat atau indeks dari elemen-elemen non-nol dalam tensor sparse. Bentuk tensor indices adalah [N, ndims], di mana N adalah jumlah elemen non-nol dan ndims adalah jumlah dimensi dari tensor spars.

2. Values
Values adalah tensor yang berisi nilai-nilai elemen non-nol yang sesuai dengan indeks yang diberikan dalam indices. Bentuk tensor values adalah [N], di mana N adalah jumlah elemen non-nol.

3. dense_shape
dense_shape adalah tensor yang menentukan bentuk lengkap dari tensor spars, termasuk elemen-elemen nol. Bentuk tensor dense_shape adalah [ndims], di mana ndims adalah jumlah dimensi dari tensor spars.

Agar lebih mengerti terkait materi di atas, mari asumsikan kita memiliki dua orang pengguna dan empat film. Tiga film yang sedang tayang tersebut telah mendapatkan rating seperti berikut.

user_id		movie_id	rating
0		0		5.0
0		1		3.0
1		3		1.0

Berdasarkan DataFrame di atas, kita dapat merepresentasikannya menjadi sebuah matriks sebagai berikut.

A = [ 5.0	3.0	0	0  ]
    [ 0		0	0	1.0]

Masih ingatkah Anda jumlah data yang kita miliki? Yup, kita memiliki 1682 film dan 943 pengguna. Bayangkan berapa banyak matriks yang harus dihitung oleh komputer dengan data sebanyak itur? Tentunya akan memakan cukup banyak resource ‘kan? Nah, dengan menggunakan SparseTensor, kita akan mengubah bentuk matriks di atas menjadi lebih terstruktur sehingga komputer dapat melakukan komputasi dengan lebih efisien.

Penjelasan di atas merupakan teoritisnya saja, lalu bagaimana cara kita menerapkan materi tersebut menjadi sebuah kode? Sederhananya kita dapat membuat sebuah fungsi sesuai dengan kondisi indices dan values yang akan kita masukkan.


def build_rating_sparse_tensor(ratings_df):
  indices = ratings_df[['user_id', 'movie_id']].values
  values = ratings_df['rating'].values
  return tf.SparseTensor(
      indices=indices,
      values=values,
      dense_shape=[users.shape[0], movies.shape[0]])

Kode di atas akan mengembalikan sebuah SparseTensor berdasarkan data ratings yang sudah disesuaikan dengan struktur user_id dan movie_id. Selain membuat fungsi di atas, kita juga perlu membuat sebuah fungsi yang dapat menghitung loss menggunakan pendekatan mean square error yang telah dipelajari pada modul sebelumnya.

def sparse_mean_square_error(sparse_ratings, user_embeddings, movie_embeddings):
  predictions = tf.gather_nd(
      tf.matmul(user_embeddings, movie_embeddings, transpose_b=True),
      sparse_ratings.indices)
  loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)
  return loss

Fungsi di atas memiliki tiga buah parameter seperti sparse_ratings, user_embeddings, dan movie_embeddings. Ketiga parameter itu memiliki fungsi yang berbeda. Lalu, apakah Anda penasaran apa isi dari parameter tersebut? Yuk, kita bahas semuanya.

1. sparse_ratings: matriks SparseTensor yang telah kita buat pada kode sebelumnya dengan bentuk larik [N, M].
2. user_embeddings: sebuah Tensor padat U dengan bentuk [N, k] di mana k adalah embedding sehingga U_i adalah embedding dari pengguna i.
3. movie_embeddings: sebuah Tensor padat V dengan bentuk [M, k] di mana k adalah dimensi penyematan dimensi embedding sehingga V_j adalah embedding dari film j.

Lalu, di dalam fungsi tersebut terdapat sintaks yang mengatur perhitungan yang dilakukan oleh tf.gather, tf.matmul, dan tf.losses. Penasaran ‘kan? Mari kita bahas keduanya bersama.

1. tf.matmul(user_embeddings, movie_embeddings, transpose_b=True): fungsi ini melakukan perkalian matriks antara embedding pengguna (user_embeddings) dan embedding film (movie_embeddings). Lalu, transpose_b=True bertugas untuk mentranspose posisi kolom dan baris dari movie_embeddings sehingga dimensi cocok untuk perkalian matriks. Hasil dari operasi ini adalah sebuah matriks di mana setiap elemen [i, j] adalah skor prediksi dari pengguna i untuk film j.

2. tf.gather_nd(..., sparse_ratings.indices): bertugas untuk mengambil elemen-elemen dari hasil perkalian matriks berdasarkan indeks yang diberikan oleh sparse_ratings.indices. Indeks ini menunjukkan posisi elemen non-nol dalam sparse_ratings yang berarti kita hanya mengambil prediksi yang relevan.

3. loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)
	- sparse_ratings.values: ini adalah nilai-nilai peringkat sebenarnya yang diberikan oleh pengguna untuk film.
	- predictions: ini adalah prediksi peringkat yang diambil dari matriks hasil perkalian embedding.
	- tf.losses.mean_squared_error(...): fungsi ini menghitung MSE antara nilai sebenarnya dan prediksi.

Secara umum, kode di atas akan mengembalikan nilai mean squared error (MSE) untuk melakukan prediksi peringkat film yang diberikan oleh pengguna berdasarkan embeddings pengguna dan film. Fungsi ini dirancang untuk melakukan komputasi berdasarkan data yang sudah diubah menjadi tensor spars sehingga dapat menghemat memori dan komputasi.

Sekarang kita akan memasuki tahapan krusial, yaitu menentukan jenis sistem rekomendasi yang akan dibangun. Seperti yang Anda tahu, pendekatan umum untuk membangun sistem rekomendasi itu terbagi menjadi dua yaitu content based filtering dan collaborative filtering. Pada kasus ini, kita akan membangun sebuah sistem rekomendasi menggunakan metode collaborative filtering model. 

Tentunya kita memerlukan sebuah library yang dapat membantu proses pembangunan model tersebut, tetapi karena Anda menyukai tantangan, mari kita lupakan sejenak dan membuat sebuah kelas yang dapat mengerjakan tugas tersebut. 

Kelas sederhana yang akan kita bangun berguna untuk melatih model faktorisasi matriks menggunakan penurunan gradien stokastik. Perhatikan kode berikut.

class CFModel(object):
  def __init__(self, embedding_vars, loss, metrics=None):
    self._embedding_vars = embedding_vars
    self._loss = loss
    self._metrics = metrics
    self._embeddings = {k: None for k in embedding_vars}
    self._session = None
 
  @property
  def embeddings(self):
    return self._embeddings
 
  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,
            optimizer=tf.train.GradientDescentOptimizer):
    with self._loss.graph.as_default():
      opt = optimizer(learning_rate)
      train_op = opt.minimize(self._loss)
      local_init_op = tf.group(
          tf.variables_initializer(opt.variables()),
          tf.local_variables_initializer())
      if self._session is None:
        self._session = tf.Session()
        with self._session.as_default():
          self._session.run(tf.global_variables_initializer())
          self._session.run(tf.tables_initializer())
          tf.train.start_queue_runners()
 
    with self._session.as_default():
      local_init_op.run()
      iterations = []
      metrics = self._metrics or ({},)
      metrics_vals = [collections.defaultdict(list) for _ in self._metrics]
      for i in range(num_iterations + 1):
        _, results = self._session.run((train_op, metrics))
        if (i % 10 == 0) or i == num_iterations:
          print("\r iteration %d: " % i + ", ".join(
                ["%s=%f" % (k, v) for r in results for k, v in r.items()]),
                end='')
          iterations.append(i)
          for metric_val, result in zip(metrics_vals, results):
            for k, v in result.items():
              metric_val[k].append(v)
 
      for k, v in self._embedding_vars.items():
        self._embeddings[k] = v.eval()
 
      if plot_results:
        num_subplots = len(metrics)+1
        fig = plt.figure()
        fig.set_size_inches(num_subplots*10, 8)
        for i, metric_vals in enumerate(metrics_vals):
          ax = fig.add_subplot(1, num_subplots, i+1)
          for k, v in metric_vals.items():
            ax.plot(iterations, v, label=k)
          ax.set_xlim([1, num_iterations])
          ax.legend()
      return results


Seperti yang Anda lihat, pada kode di atas terdapat beberapa konstruktor pada kelas tersebut seperti user_embeddings, movie_embeddings, loss_optimizer, dan matriks. Agar lebih mudah dimengerti, mari kita rangkum kode tersebut untuk tujuan memahami struktur kode secara keseluruhan (bukan untuk diimplementasikan).

U_var = ...
V_var = ...
loss = ...
model = CFModel(U_var, V_var, loss)
model.train(iterations=100, learning_rate=1.0)
user_embeddings = model.embeddings['user_id']
movie_embeddings = model.embeddings['movie_id']


Sadarkah Anda bahwa sedari tadi kita membuat fungsi, tetapi belum digunakan? Mohon bersabar karena kita harus membuat sebuah fungsi terakhir untuk membangun model sistem rekomendasi. Fungsi ini akan memanggil fungsi-fungsi sebelumnya dan akan mengembalikan sebuah dataset yang siap untuk dilatih.

def build_model(ratings, embedding_dim, init_stddev):
  # Membagi dataset menjadi data latih dan data uji.
  train_ratings, test_ratings = split_dataframe(ratings)
  # Mengubah dataset menjadi SparseTensor.
  A_train = build_rating_sparse_tensor(train_ratings)
  A_test = build_rating_sparse_tensor(test_ratings)
  # Inisialisasi embedding menggunakan distribusi normal.
  U = tf.Variable(tf.random_normal(
      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))
  V = tf.Variable(tf.random_normal(
      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))
  train_loss = sparse_mean_square_error(A_train, U, V)
  test_loss = sparse_mean_square_error(A_test, U, V)
  metrics = {
      'train_error': train_loss,
      'test_error': test_loss
  }
  embeddings = {
      "user_id": U,
      "movie_id": V
  }
  return CFModel(embeddings, train_loss, [metrics])

Terakhir merupakan tahapan pamungkas, kita perlu melakukan pelatihan dengan memanggil seluruh fungsi yang telah dibuat sebelumnya. Karena seluruh fungsi sudah terhubung dengan baik, Anda hanya perlu membuat kode seperti berikut.


model = build_model(ratings, embedding_dim=30, init_stddev=0.5)
model.train(num_iterations=1000, learning_rate=10.)

Kode di atas adalah bagian dari pipeline machine learning yang digunakan untuk membangun dan melatih model. Dalam konteks ini, kita memiliki dua bagian utama yaitu membangun model dan melatih model. Mari kita jelaskan setiap bagian dengan detail.

Baris pertama memanggil fungsi build_model untuk membangun model machine learning. Penjelasan rinci terkait argumen yang diberikan adalah sebagai berikut.

1. Ratings: data yang digunakan untuk melatih model. Data ini berbentuk matriks di mana setiap baris mewakili pengguna dan kolom mewakili item (misalnya, film), dengan nilai-nilai yang menunjukkan peringkat yang diberikan oleh pengguna kepada item.

2. embedding_dim=30: parameter ini menentukan dimensi dari embedding yang akan digunakan untuk pengguna dan item. Embedding adalah representasi vektor dari pengguna dan item dalam ruang berdimensi rendah yang membantu dalam menangkap karakteristik dan hubungan mereka. Dalam hal ini, dimensi embedding ditetapkan menjadi 30 vektor.

3. init_stddev=0.5: nilai deviasi standar untuk inisialisasi awal dari embedding. Inisialisasi dengan deviasi standar 0.5 berarti nilai-nilai awal dari embedding akan diambil dari distribusi normal dengan rata-rata 0 dan deviasi standar 0.5. Inisialisasi yang baik dapat membantu model untuk konvergen lebih cepat dan lebih stabil.


Hasil dari fungsi build_model adalah sebuah objek model yang berisi struktur dan parameter dari model yang telah dibangun berdasarkan data dan parameter yang diberikan. Selanjutnya, kita perlu melatih model menggunakan fungsi model.train(). Fungsi ini memanggil metode train pada objek model untuk melatih model. Mari kita jelaskan argumen-argumen yang diberikan.

1. num_iterations=1000: parameter ini menentukan jumlah iterasi atau langkah dalam proses pelatihan. Semakin banyak iterasi, semakin lama model akan dilatih yang umumnya meningkatkan performa model sampai titik tertentu di mana overfitting dapat terjadi.

2. learning_rate=10.: parameter Ini menentukan kecepatan pembelajaran (learning rate) yang digunakan dalam optimizers. Learning rate menentukan seberapa besar langkah yang diambil pada setiap iterasi saat memperbarui parameter model. Dalam hal ini, learning rate ditetapkan sangat besar, yaitu 10. Biasanya, learning rate yang besar dapat menyebabkan model melompat-lompat di sekitar minimum dan tidak konvergen dengan baik. Nilai learning rate biasanya disesuaikan dengan hati-hati dan sering kali lebih kecil, seperti 0.001 atau 0.01.

Setelah model dilatih, apakah Anda mengingat tahapan selanjutnya? Pada machine learning workflow tentunya kita harus melakukan evaluasi agar mengetahui performa model yang telah dibangun. Pada kasus sistem rekomendasi ada beberapa cara untuk menghitung kemiripan fitur yang ada salah dua contohnya adalah menggunakan dot dan cosine similarity. Dot dan cosine similarity adalah dua metode yang umum digunakan untuk mengukur kesamaan antara dua vektor. Kedua metode ini digunakan untuk menemukan item yang serupa atau pengguna yang serupa berdasarkan fitur yang ada. Berikut penjelasan detail mengenai keduanya.

1. Dot similarity adalah ukuran kesamaan yang diperoleh dari perkalian titik (dot product) antara dua vektor. Ini digunakan dalam berbagai aplikasi, termasuk dalam sistem rekomendasi untuk mengukur seberapa mirip dua vektor, seperti vektor pengguna dan vektor item.

2. Cosine similarity adalah ukuran kesamaan yang mengukur cosinus sudut antara dua vektor dalam ruang berdimensi. Cosine similarity mengukur orientasi sehingga memberikan indikasi yang lebih akurat tentang seberapa mirip arah dua vektor. 


Dalam sistem rekomendasi, cosine similarity lebih umum digunakan dibandingkan dot similarity karena lebih efektif dalam menangani data dengan berbagai skala dan memberikan hasil yang lebih akurat tentang kesamaan fitur.

Misalnya, untuk merekomendasikan film, cosine similarity dapat digunakan untuk menemukan film yang memiliki kesamaan fitur berdasarkan preferensi pengguna, meskipun jumlah rating pengguna bervariasi.

DOT = 'dot'
COSINE = 'cosine'
def compute_scores(query_embedding, item_embeddings, measure=DOT):
  u = query_embedding
  V = item_embeddings
  if measure == COSINE:
    V = V / np.linalg.norm(V, axis=1, keepdims=True)
    u = u / np.linalg.norm(u)
  scores = u.dot(V.T)
  return scores

Fungsi di atas akan menghitung skor kesamaan antara data user dan item yang sudah melewati tahapan embedding menggunakan dua jenis ukuran kesamaan yaitu konstanta dot product dan cosine similarity. 

Sampai di sini, model yang Anda bangun belum menghasilkan rekomendasi sama sekali. Hal ini karena kita belum membangun sebuah sintaks atau fungsi yang dapat melakukan inference berdasarkan model yang sudah dibuat.

Untuk melakukan inference pada model collaborative filtering, kita bisa menggunakan dua pendekatan yaitu memberikan rekomendasi berdasarkan pengguna terdekat dan film terdekat. Mari kita bahas mulai dari pengguna terdekat.

def user_recommendations(model, measure=DOT, exclude_rated=True, k=6):
  if USER_RATINGS:
    scores = compute_scores(
        model.embeddings["user_id"][942], model.embeddings["movie_id"], measure)
    score_key = measure + ' score'
    df = pd.DataFrame({
        score_key: list(scores),
        'movie_id': movies['movie_id'],
        'titles': movies['title'],
        'genres': movies['all_genres'],
    })
    if exclude_rated:
      # Menghapus film yang sudah diberikan rating oleh user.
      rated_movies = ratings[ratings.user_id == "942"]["movie_id"].values
      df = df[df.movie_id.apply(lambda movie_id: movie_id not in rated_movies)]
    display.display(df.sort_values([score_key], ascending=False).head(k))


Fungsi user_recommendations di atas bertugas untuk menghasilkan rekomendasi film bagi seorang pengguna berdasarkan model embedding yang telah dilatih. Tujuannya untuk memberikan rekomendasi film untuk pengguna tertentu (dalam contoh kasus ini pengguna dengan ID 942). Berikut adalah langkah-langkah yang diambil fungsi ini.
1. Menghitung skor kesamaan antara embedding pengguna dan embedding semua film.
2. Membuat DataFrame yang berisi skor kesamaan, ID film, judul film, dan genre film.
3. Menghapus film yang sudah diberi rating oleh pengguna (jika exclude_rated adalah True).
4. Mengurutkan DataFrame berdasarkan skor kesamaan dan menampilkan sejumlah rekomendasi teratas yang ditentukan oleh parameter k.


Mari kita lihat output dari penggunaan fungsi berikut.
user_recommendations(model, measure=COSINE, k=5)
user_recommendations(model, measure=DOT, k=5)

Seperti yang Anda lihat pada output di atas, model kita dapat memberikan rekomendasi film yang sesuai dengan rating yang diberikan oleh pengguna dengan ID 942. Lalu, bagaimana cara kita mendapatkan rekomendasi berdasarkan fitur lainnya? Pada kasus ini, kita dapat membuat rekomendasi berdasarkan kemiripan genre dari masing-masing film. Perhatikan fungsi berikut.


def movie_neighbors(model, title_substring, measure=DOT, k=6):
  # Mencari film berdasarkan judul yang dimasukkan.
  ids =  movies[movies['title'].str.contains(title_substring)].index.values
  titles = movies.iloc[ids]['title'].values
  if len(titles) == 0:
    raise ValueError("Found no movies with title %s" % title_substring)
  print("Nearest neighbors of : %s." % titles[0])
  if len(titles) > 1:
    print("[Found more than one matching movie. Other candidates: {}]".format(
        ", ".join(titles[1:])))
  movie_id = ids[0]
  scores = compute_scores(
      model.embeddings["movie_id"][movie_id], model.embeddings["movie_id"],
      measure)
  score_key = measure + ' score'
  df = pd.DataFrame({
      score_key: list(scores),
      'titles': movies['title'],
      'genres': movies['all_genres']
  })
  display.display(df.sort_values([score_key], ascending=False).head(k))


Dengan membuat fungsi di atas, kita dapat memberikan rekomendasi kepada pengguna berdasarkan genre dari judul film yang pernah ditonton olehnya. Untuk menggunakan fungsi tersebut, Anda dapat mengikuti kode di bawah ini.

movie_neighbors(model, "Star Wars", DOT)
movie_neighbors(model, "Star Wars", COSINE)

Latihan:

1. Menghitung skor kesamaan antara embedding pengguna dan embedding semua film.
2. Membuat DataFrame yang berisi skor kesamaan, ID film, judul film, dan genre film.
3. Menghapus film yang sudah diberi rating oleh pengguna (jika exclude_rated adalah True).
4. Mengurutkan DataFrame berdasarkan skor kesamaan dan menampilkan sejumlah rekomendasi teratas yang ditentukan oleh parameter k.

Apa yang dimaksud dengan embedding dalam konteks sistem rekomendasi?
a. Representasi vektor berdimensi rendah dari pengguna atau item yang memungkinkan model untuk menangkap hubungan dan kesamaan dalam data dengan cara yang lebih efisien.

Sebuah perusahaan ingin meningkatkan sistem rekomendasinya dengan memanfaatkan data dari media sosial. Bagaimana data dari media sosial dapat digunakan untuk meningkatkan akurasi rekomendasi?

a. Dengan menganalisis pola interaksi sosial untuk menemukan pengguna yang memiliki minat serupa.

Apa tantangan utama dalam menggunakan embedding untuk merepresentasikan pengguna dan item dalam sistem rekomendasi?
d. Embedding dapat menghasilkan bias jika data pelatihan tidak representatif, dan sulit untuk menginterpretasikan makna dari dimensi laten dalam ruang embedding.

Mengapa embedding digunakan dalam sistem rekomendasi?
b. Untuk menangkap relasi kompleks antara pengguna dan item dengan merepresentasikan mereka dalam ruang vektor berukuran lebih rendah.

Anda bekerja pada sistem rekomendasi film dan menemukan bahwa beberapa pengguna memberikan rating yang sangat berbeda untuk film yang serupa. Bagaimana Anda bisa menangani masalah ini untuk meningkatkan akurasi rekomendasi?
a. Dengan menggunakan algoritma clustering untuk mengelompokkan pengguna berdasarkan pola rating mereka dan memberikan rekomendasi berdasarkan kelompok tersebut.

Pengguna A memiliki vektor preferensi A=[3,4,2] dan pengguna B memiliki vektor preferensi B=[1,0,3]. 

Hitung cosine similarity antara pengguna A dan pengguna B.
a. 0.528

Dalam sistem rekomendasi, cosine similarity lebih umum digunakan dibandingkan dot similarity karena lebih efektif dalam menangani data dengan berbagai skala dan memberikan hasil yang lebih akurat tentang kesamaan fitur.

---------------------------------------------------------------------------
                         Reinforcement Learning
---------------------------------------------------------------------------

Dalam RL, komputer atau agen belajar melalui trial and error, mengambil tindakan di lingkungan tertentu, dan kemudian memperbaharui pengetahuannya berdasarkan reward yang diterima dari lingkungan itu sendiri.

Reinforcement learning (RL) adalah salah satu cabang pembelajaran mesin (machine learning) berfokus pada cara agen belajar mengambil tindakan dalam lingkungan tertentu agar memaksimalkan hadiah (reward) yang diterima lingkungan itu sendiri. Dalam RL, agen belajar melalui percobaan dan kesalahan; agen mencoba tindakan tertentu, melihat hasilnya (reward), serta memperbarui pengetahuannya tentang tindakan yang lebih baik pada situasi tertentu.

Pada RL, agen belajar dari pengalaman interaktif dengan lingkungan. Agen tidak diberikan contoh jawaban langsung seperti pada supervised learning dan tidak juga memiliki label dalam data layaknya dalam unsupervised learning. Sebaliknya, **agen mengambil tindakan, melihat konsekuensinya dalam bentuk reward atau hukuman, dan berusaha untuk belajar strategi terbaik untuk memaksimalkan hadiah di lingkungan tersebut.**

Dalam permainan ini, agen adalah pemain yang ingin belajar cara terbaik untuk menang. Papan permainan adalah lingkungan atau environment dan agen dapat melakukan tindakan dengan menempatkan tanda dalam kotak yang tersedia. Hadiah atau "reward" diberikan berdasarkan hasil permainan. Agen akan senang mendapatkan reward positif jika menang, merasa sedih dengan reward negatif jika kalah, dan tidak ada perubahan jika permainan berakhir seri.

Dengan bermain berulang kali menghadapi lawan atau bahkan melawan dirinya sendiri, agen belajar strategi yang lebih baik untuk menang. Mereka mencoba untuk menghindari gerakan yang berisiko atau memanfaatkan kesalahan lawan. Jadi, secara bertahap, RL membantu agen untuk belajar dari pengalaman mereka, mirip dengan cara manusia belajar melalui percobaan dan kesalahan.


Komponen-Komponen Utama RL:
Di dunia RL, terdapat empat komponen utama membentuk inti dari cara agen belajar dan berinteraksi dengan lingkungan. Pertama, agent adalah entitas yang belajar dan bertindak. Kedua, environment adalah dunia tempat agen beroperasi. Ketiga, action adalah pilihan yang tersedia bagi agen. Terakhir, reward atau reinforcement memberikan umpan balik kepada agen berdasarkan tindakan yang diambilnya. Mari kita bahas satu per satu!

1. Agent
Dalam RL, agen dapat diibaratkan sebagai seorang pelajar yang belajar untuk bermain catur. Bayangkan seorang pelajar yang belajar dari pengalaman bermain catur untuk meningkatkan kemampuannya. Pelajar tersebut adalah agen dengan tujuan untuk menjadi pemain catur yang lebih baik. Untuk mencapai tujuan ini, pelajar tersebut harus berinteraksi dengan lingkungannya, yaitu permainan catur itu sendiri, dan membuat keputusan berdasarkan situasi yang dihadapinya.
Sebagai seorang agen, pelajar tersebut secara aktif terlibat dalam proses pembelajaran. Dia memperhatikan langkah-langkah yang diambil selama permainan, mengidentifikasi keputusan yang berhasil dan tidak berhasil, serta mencoba untuk memahami alasan di balik hasil-hasil tersebut. 

2. Environment atau Lingkungan
Lingkungan dalam RL adalah seperti papan catur tempat pelajar bermain. Ini adalah tempat semua interaksi antara agen dan dunianya terjadi. Dalam analogi ini, setiap kotak pada papan catur mewakili kondisi atau situasi yang mungkin dihadapi agen dan setiap gerakan agen akan mengubah posisi atau keadaan di papan catur tersebut.

Selain itu, lingkungan dalam RL bisa berubah-ubah tergantung pada situasi atau kondisi tertentu. Misalnya, jika seorang pelajar bermain dalam turnamen catur, lingkungannya akan berbeda dengan saat dia berlatih di rumah dengan teman-temannya. Turnamen catur mungkin memiliki aturan lebih ketat, tekanan kompetitif lebih besar, dan lawan-lawan lebih berpengalaman sehingga lingkungan berubah menjadi lebih menantang bagi agen untuk beroperasi.

3. Action
Action atau tindakan adalah langkah-langkah yang dapat diambil oleh agen dalam lingkungannya untuk mencapai tujuan tertentu. Dalam analogi permainan catur, tindakan adalah gerakan atau keputusan yang dapat diambil oleh pelajar selama pertandingan. Ini bisa berupa memindahkan bidak, menyerang, bertahan, atau melakukan strategi tertentu untuk menguasai posisi di papan catur.
Pentingnya tindakan dalam RL adalah bahwa agen harus membuat keputusan tentang tindakan yang diambil pada situasi tertentu berdasarkan tujuannya dan kondisi lingkungan. Setiap tindakan yang diambil dapat membawa agen satu langkah lebih dekat atau lebih jauh dari mencapai tujuannya. Oleh karena itu, agen harus memilih tindakan dengan hati-hati untuk memaksimalkan peluang keberhasilan.

4. Rewards
Rewards atau hadiah dalam RL adalah umpan balik kepada agen berdasarkan tindakan yang diambilnya. Dalam analogi permainan catur, rewards bisa diibaratkan sebagai hasil dari pertandingan yang dimainkan oleh pelajar. Jika pelajar menang, dia mungkin mendapatkan hadiah positif berupa pujian dari teman-temannya atau rasa bangga atas kemenangannya

Pentingnya rewards dalam RL adalah sebagai umpan balik yang memberi arahan kepada agen bahwa tindakan yang diambilnya mengarah pada pencapaian tujuan atau tidak. Rewards membantu agen untuk belajar dari pengalaman mereka dan memperbaiki keputusan-keputusan mereka di masa depan. Dengan memaksimalkan rewards yang diperoleh, agen dapat mencapai tujuan pembelajaran mereka secara lebih efektif. 

Metode Markov Decision Process:
 Markov decision process (MDP) adalah sebuah kerangka kerja pengambilan keputusan yang memungkinkan kita untuk memodelkan suatu lingkungan. Banyak masalah dalam reinforcement learning dapat dirumuskan sebagai MDP. MDP sangat berguna dalam berbagai bidang, seperti kecerdasan buatan, robotika, ekonomi, dan lainnya karena membantu merancang agen yang dapat membuat keputusan optimal meskipun ada ketidakpastian.
 
Komponen Utama MDP:
 Dalam dunia pengambilan keputusan di bawah ketidakpastian, MDP memainkan peran penting sebagai kerangka kerja matematis yang memungkinkan pemodelan situasi bahwa agen harus membuat keputusan berdasarkan informasi terbatas dan hasil yang tidak pasti. 

1. State (keadaan)
State atau keadaan adalah representasi dari situasi atau kondisi saat ini di lingkungan. Ini adalah informasi yang diperlukan untuk membuat keputusan. Dalam permainan catur, setiap posisi bidak adalah sebuah state. Dalam navigasi robot, lokasi dan orientasi robot dapat menjadi state.  

 2. Action (Tindakan)
Action atau tindakan adalah langkah yang dapat diambil oleh agen dalam suatu state. Ini adalah keputusan yang memengaruhi perubahan state berikutnya. Dalam permainan catur, setiap langkah yang bisa diambil oleh bidak adalah sebuah action. Dalam navigasi robot, tindakan bisa berupa bergerak maju, berbelok, atau berhenti.

3. Transition Model (Model Transisi)
Transition model atau model transisi adalah fungsi yang menentukan probabilitas berpindah dari satu state ke state lain setelah tindakan tertentu diambil. Dalam permainan catur, model transisi akan menentukan probabilitas langkah yang mungkin jika suatu bidak dipindahkan. Dalam navigasi robot, model transisi akan memperhitungkan kemungkinan berpindah ke lokasi yang berbeda setelah setiap langkah. 

4. Reward (Hadiah)
Reward atau hadiah adalah nilai yang diterima oleh agen setelah melakukan tindakan tertentu dan berpindah dari satu state ke state lainnya. Dalam permainan catur, hadiah bisa berupa kemenangan jika lawan dijaga atau kekalahan jika raja tertangkap. Dalam navigasi robot, hadiah bisa berupa mencapai tujuan yang diinginkan. 

5. Policy (Kebijakan)
Policy atau kebijakan adalah strategi atau aturan penentu tindakan yang harus diambil oleh agen dalam setiap state. Dalam permainan catur, kebijakan bisa menjadi aturan penentu langkah yang harus diambil dalam setiap situasi. Dalam navigasi robot, kebijakan bisa menjadi algoritma yang menentukan langkah-langkah untuk mencapai tujuan.

Komponen-komponen ini membentuk kerangka kerja penting untuk merancang sistem pengambilan keputusan yang optimal dalam situasi tidak pasti. Dengan memahami state, action, transition model, reward, dan policy, kita dapat mengembangkan strategi yang efektif dalam berbagai konteks, mulai dari game hingga navigasi robot, dan banyak lagi.


Cara Kerja MDP:
MDP merupakan salah satu pendekatan yang sangat efektif dalam mengatasi tantangan pengambilan keputusan di lingkungan tidak pasti. Proses ini memberikan landasan matematis kuat untuk merancang sistem kecerdasan buatan yang dapat membuat keputusan optimal dalam berbagai konteks, mulai dari permainan hingga robotika dan manajemen operasional.


1. Representasi Environment
Langkah pertama dalam MDP adalah mendefinisikan dan memetakan environment dalam serangkaian state yang mungkin. Setiap state mewakili kondisi atau situasi spesifik yang mungkin terjadi dalam environment tersebut. Representasi tersebut harus mematuhi prinsip Markov bahwa "transisi dari satu state ke state lain hanya bergantung pada state saat ini dan tindakan yang diambil, tanpa memperhitungkan sejarah sebelumnya."

2. Penentuan Action atau Aksi
Setelah lingkungan dipetakan dalam state, agen harus memutuskan tindakan yang akan diambil pada setiap state. Ini bisa dilakukan dengan mempertimbangkan kebijakan (policy) yang didefinisikan sebelumnya atau dengan menggunakan algoritma pencarian tindakan optimal untuk setiap state.

3. Evaluasi Transisi
Setelah tindakan dipilih, agen melakukan langkah ke state baru berdasarkan model transisi. Model transisi menentukan probabilitas berpindah dari satu state ke state lain setelah tindakan tertentu diambil. Agen kemudian mengevaluasi hasil dari langkah ini untuk menentukan state baru yang dihasilkan.

4. Perhitungan Reward
Setelah berpindah ke state baru, agen menerima reward yang terkait dengan transisi tersebut. Reward ini adalah nilai numerik yang memberikan umpan balik atas keberhasilan atau kegagalan tindakan. Reward bisa positif jika tindakan berhasil mendekatkan agen ke tujuan, negatif jika tindakan mengarah pada konsekuensi buruk, atau nol jika tidak ada efek signifikan.

5. Pembaruan Kebijakan
Berdasarkan reward, agen memperbarui kebijakan atau strategi yang digunakan untuk membuat keputusan di masa depan. Tujuan dari pembaruan kebijakan adalah meningkatkan kinerja agen dan memaksimalkan jumlah reward yang diperoleh dalam jangka panjang. Proses ini bisa melibatkan berbagai algoritma pembelajaran, seperti Q-learning atau metode pembelajaran yang berbasis kebijakan (policy-based learning).

6. Pengambilan Keputusan Berulang
Proses di atas terjadi secara berulang ketika agen berinteraksi dengan lingkungan. Agen terus memilih tindakan, menerima reward, memperbarui kebijakan, dan memperbaiki kinerjanya berdasarkan pengalaman baru yang diperoleh.

7. Mencari Kebijakan Optimal
Akhirnya, dengan menggunakan metode, seperti iterasi nilai (value iteration) atau iterasi kebijakan (policy iteration), agen mencoba menemukan kebijakan yang optimal. Kebijakan optimal adalah kebijakan yang memberikan jumlah reward terbesar dalam jangka panjang dan dapat membimbing agen untuk mengambil keputusan optimal pada setiap state.



Q-Learning

Q-learning adalah salah satu algoritma pembelajaran penguatan untuk mengajarkan agen cara membuat keputusan optimal saat berinteraksi dengan lingkungannya. Tujuan utama dari Q-learning adalah mempelajari nilai-nilai Q yang memperkirakan manfaat dari mengambil tindakan tertentu dalam keadaan tertentu sehingga agen dapat mengambil tindakan paling menguntungkan pada setiap langkah.

Komponen Utama Q-learning:
1. State (Keadaan): Representasi dari situasi atau kondisi saat ini dalam lingkungan. Setiap keadaan mungkin memerlukan tindakan yang berbeda dari agen.
2. Action (Tindakan): Tindakan-tindakan yang dapat diambil oleh agen dalam keadaan tertentu. Agen harus memilih tindakan yang paling bermanfaat dalam mencapai tujuan tertentu.
3. Reward (Hadiah): Nilai numerik untuk menggambarkan keuntungan atau kerugian yang diterima oleh agen setelah melakukan tindakan tertentu dalam keadaan tertentu. Tujuan agen adalah memaksimalkan jumlah hadiah yang diterima selama interaksi dengan lingkungan.
4. Q-table: Tabel dua dimensi yang menyimpan nilai-nilai Q, yakni setiap sel mewakili pasangan keadaan-tindakan dan berisi estimasi manfaat dari mengambil tindakan tertentu dalam keadaan tertentu.
5. Exploration vs Exploitation (Eksplorasi vs Eksploitasi): Strategi untuk memilih tindakan. Eksplorasi melibatkan mencoba tindakan baru untuk menjelajahi lingkungan, sementara eksploitasi melibatkan memilih tindakan dengan nilai Q tertinggi untuk memanfaatkan pengetahuan yang sudah ada.


Cara Kerja Q-learning:
Q-learning adalah salah satu teknik penting dalam pembelajaran penguatan dan telah membuka pintu untuk pengembangan sistem-sistem cerdas yang mampu belajar dari pengalaman. Di dunia komputasi dan kecerdasan buatan, Q-learning memainkan peran kunci dalam mengajarkan agen-agen cara membuat keputusan optimal pada lingkungan yang dinamis. Konsep dasar Q-learning melibatkan pembelajaran dari trial and error, yaitu agen memperkirakan manfaat dari tindakan-tindakan yang berbeda dalam situasi-situasi tertentu.

1. Inisialisasi Tabel Q
Langkah pertama dalam Q-learning adalah menginisialisasi tabel Q. Tabel Q adalah sebuah matriks berisi nilai-nilai Q, yang mewakili estimasi manfaat dari mengambil setiap tindakan dalam setiap keadaan. Pada awalnya, nilai-nilai dalam tabel Q bisa diatur secara acak atau dengan nilai-nilai default.

2. Memilih Tindakan
Setelah tabel Q diinisialisasi, agen memilih tindakan yang akan diambil berdasarkan strategi 3-greedy (epsilon). Ini berarti bahwa agen akan memilih tindakan secara acak dengan probabilitas 3 (tingkat eksplorasi).  Ini juga akan memilih tindakan dengan nilai Q tertinggi dengan probabilitas (1-(3)) (untuk eksploitasi).

3. Melakukan Tindakan
Setelah agen memilih tindakan, ia akan melaksanakan tindakan tersebut di lingkungan.

4. Mengukur Hadiah
Setelah melakukan tindakan, agen akan menerima sebuah hadiah (reward) dari lingkungan. Hadiah ini bisa positif, negatif, atau netral tergantung pada keberhasilan agen dalam tindakan yang dilakukannya.

5. Memperbarui Tabel Q
Setelah menerima hadiah, agen akan memperbarui nilai-nilai dalam tabel Q menggunakan rumus pembelajaran Q. Rumus ini memperhitungkan hadiah yang diterima oleh agen serta memperbarui nilai-nilai Q sesuai dengan tingkat pembelajaran (alpha) dan faktor diskon (beta(y)).

Setelah beberapa iterasi, tabel Q akan terus diperbarui dan disempurnakan melalui interaksi agen dengan lingkungan. Setelah konvergensi atau setelah sejumlah iterasi tertentu, tabel Q yang baik akan terbentuk dan memungkinkan agen dalam mengambil tindakan optimal pada setiap keadaan untuk mencapai tujuan.



Policies and Value Function:
Dalam konteks Markov decision process (MDP), dua konsep utama yang sangat penting adalah policies dan value functions. Berikut adalah penjelasan lebih lanjut mengenai kedua konsep ini.

A. Policies
Policy (kebijakan) dalam MDP adalah strategi atau aturan mengenai tindakan yang harus diambil oleh agen dalam setiap state. Kebijakan dapat bersifat deterministik atau stokasti
1. Deterministic Policy (phi):
Kebijakan ini menetapkan tindakan tertentu untuk setiap state. Artinya, jika agen berada pada state tertentu, ia selalu akan mengambil tindakan yang sama. Misalnya, phi(s) = a berarti agen selalu memilih tindakan a pada state s.

2. Stochastic Policy (phi):
Kebijakan ini menentukan probabilitas berbagai tindakan pada setiap state. Artinya, jika agen berada pada state tertentu, ia memilih tindakan berdasarkan probabilitas yang ditentukan oleh kebijakan tersebut. Misalnya, phi(a|s) adalah probabilitas memilih tindakan a di state s.

Tujuan utama dalam MDP adalah menemukan kebijakan optimal yang memaksimalkan total reward sesuai dengan harapan dari waktu ke waktu.


B. Value Function

Value functions (fungsi nilai) digunakan untuk mengevaluasi seberapa baik suatu state atau state-action pair di bawah kebijakan tertentu. Ada dua jenis fungsi nilai utama dalam MDP.
1. State Value Function (V(s))
Fungsi nilai state mengevaluasi seberapa baik atau bernilai state tertentu jika kita mengikuti kebijakan (phi) dari state tersebut ke depan. Nilainya adalah ekspektasi total reward yang diharapkan jika memulai dari state s dan mengikuti kebijakan π. Notasinya sebagai berikut.

Artinya, ???? adalah faktor diskonto (0 ≤ γ < 1) yang menentukan seberapa jauh reward di masa depan diperhitungkan. Semakin kecil γ, semakin kurang penting reward di masa depan dibandingkan reward saat ini.

2. Action Value Function (Q(s, a))
Action value function mengevaluasi seberapa baik atau bernilai tindakan tertentu diambil pada suatu state jika kita kemudian mengikuti kebijakan (phi). Nilainya adalah ekspektasi total reward yang diharapkan jika memulai dari state s, mengambil tindakan a, dan kemudian mengikuti kebijakan (phi).

Fungsi ini mempertimbangkan faktor y, yang menentukan seberapa pentingnya reward di masa depan dibandingkan dengan reward saat ini. Dengan menggunakan notasi ini, agen dapat secara efisien mengevaluasi berbagai tindakan yang mungkin diambil dalam setiap state untuk memaksimalkan total reward yang diharapkan dalam jangka panjang, membantu dalam pengambilan keputusan yang optimal dalam konteks Reinforcement Learning.



Exploration vs Exploitation pada Markov Decision Processes (MDPs):
Saat beroperasi dalam MDPs, ada dua strategi utama yang sering dibahas: eksplorasi dan eksploitasi. Eksplorasi melibatkan upaya untuk mengeksplorasi lingkungan dengan mencoba tindakan baru dalam memahami lebih baik tentang dampaknya. Di sisi lain, eksploitasi terfokus pada penggunaan pengetahuan yang sudah ada dalam memilih tindakan untuk memberikan hasil terbaik berdasarkan pengalaman sebelumnya. 

Dalam perpaduan antara kedua strategi ini, sistem bisa menjadi lebih cerdas dan adaptif serta memungkinkan peningkatan kinerja seiring waktu. Mari kita telaah lebih jauh konsep penting ini.

1. Exploration
Strategi ini digunakan saat kita mencoba tindakan yang belum dicoba sebelumnya untuk memahami lingkungan lebih baik. Dalam konteks MDPs, itu berarti mencoba tindakan yang belum kita ketahui akibatnya.

Misalnya, kita memiliki robot yang belajar berjalan di ruangan yang belum pernah dilihat maka mencoba langkah-langkah baru yang belum teruji adalah cara untuk 'mengeksplorasi' dan memahami lingkungan tersebut.

2. Exploitation
Strategi ini digunakan saat kita menggunakan pengetahuan yang sudah dimiliki untuk memilih tindakan yang dipercaya akan memberikan hasil terbaik. Dalam MDPs, itu berarti memilih tindakan yang kita tahu dari pengalaman sebelumnya memberikan imbalan atau hasil baik.

Misalnya, robot kita telah belajar bahwa langkah-langkah tertentu mengarah ke area aman dalam ruangan maka memilih untuk mengambil langkah-langkah itu lagi adalah 'eksploitasi' pengetahuan yang ada.

Jadi, perbedaan antara eksplorasi dan eksploitasi dalam MDPs adalah eksplorasi perihal mencoba hal-hal baru untuk mempelajari lingkungan lebih lanjut; sementara eksploitasi mengenai penggunaan pengetahuan yang sudah ada untuk membuat keputusan terbaik saat ini.

Pada praktiknya, strategi yang baik dalam MDPs sering melibatkan keseimbangan antara eksplorasi dan eksploitasi. Terlalu banyak eksplorasi berarti tidak memanfaatkan pengetahuan yang sudah ada, sementara terlalu banyak eksploitasi bisa membuat kita melewatkan peluang baru yang mungkin lebih baik.


Contoh Sederhana Markov Decision Process:
Mari kita buat contoh sederhana MDP (Markov decision process) dengan mengambil kasus seorang agen cerdas (misalnya, robot atau agen permainan) yang harus memutuskan tindakan terbaik untuk berinteraksi dengan lingkungannya. Mari kita ambil contoh "Grid World", yaitu agen berada dalam grid berukuran 3 × 3 dan harus mencapai tujuan tertentu. Setiap langkah memiliki reward dan ada beberapa rintangan yang harus dihindari. Ini adalah contoh yang sederhana, tetapi cukup mewakili MDP.

Ketentuan contoh:
- start: posisi awal agen,
- goal: tujuan akhir,
- wall: rintangan, agen tidak dapat melewati


Komponen MDP yang digunakan;
1. State (S)
Setiap sel dalam grid mewakili state. Contohnya, state dapat didefinisikan sebagai koordinat (baris, kolom) dalam grid. Setiap sel dalam grid merupakan state. Dalam contoh ini, grid berukuran 3 × 3 sehingga ada total 9 state. Misalnya, state (1,1) adalah posisi awal agen, state (1,3) adalah tujuan, dan state (2,2) adalah rintangan.

2. Action (A)
Agen memiliki empat tindakan yang dapat diambil: "atas", "bawah", "kiri", atau "kanan". Namun, tidak semua action mungkin pada semua state. Misalnya, jika agen berada di pinggir grid, beberapa tindakan mungkin tidak dapat dilakukan.

3. Transition Probability (P)
Probabilitas transisi dari satu state ke state lainnya setelah agen mengambil suatu tindakan. Probabilitas ini bisa beragam tergantung pada lingkungan. Misalnya, jika agen bergerak ke atas, probabilitasnya untuk benar-benar bergerak ke atas adalah tinggi. Namun, ada kemungkinan agen "tergelincir" ke samping karena adanya slip dalam lingkungan.

4. Reward (R)
Agen menerima reward berdasarkan state dan action yang diambil. Misalnya, mencapai tujuan bisa memberikan reward positif dan menabrak rintangan bisa memberikan reward negatif.

Dalam contoh ini, kita memiliki hal berikut:

+10 poin jika agen mencapai tujuan (state (1,3));
-5 poin jika agen menabrak rintangan (state (2,2)); dan
+1 poin untuk setiap langkah yang dilakukan agen.

Kita dapat menggambarkan MDP dengan tabel transisi yang menunjukkan probabilitas perpindahan antar state berdasarkan action dari agen. Contohnya, agen berada di posisi (1,1) dan mengambil tindakan "atas" maka dengan probabilitas tertentu agen bisa berpindah ke posisi (1,2), (2,1). Kemungkinan lainnya adalah tetap berada di posisi (1,1) jika terjadi 'geser' atau slip.

Dengan MDP, agen dapat mempelajari kebijakan optimal untuk mencapai tujuan dengan mengambil tindakan yang tepat pada setiap state, menghindari rintangan, dan memaksimalkan total perolehan reward dalam perjalanan mencapai tujuan.



Latihan Implementasi Reinforcement Learning:
Frozen Lake adalah salah satu lingkungan dalam pustaka OpenAI Gym yang sering digunakan untuk mengajarkan dan menguji algoritma reinforcement learning. Di sini, agen harus belajar untuk mencapai tujuan di danau beku tanpa jatuh ke dalam lubang es.

Gym (OpenAI Gym): Untuk simulasi lingkungan reinforcement learning seperti Frozen Lake.

Latihan implementasi reinforcement learning (RL) pada permainan Frozen Lake, terutama menggunakan metode Q-Learning, menawarkan pengalaman menarik dalam memahami konsep dasar RL dan cara algoritma dapat "belajar" untuk membuat keputusan yang optimal di lingkungan yang dinamis. Frozen Lake adalah contoh klasik dari lingkungan berbasis grid, yakni agen harus menavigasi melalui permukaan es untuk mencapai tujuan tanpa tergelincir ke dalam lubang es.

Melalui penerapan Q-Learning, agen dapat belajar mengeksplorasi dan memanfaatkan lingkungan ini dengan memperbarui nilai-nilai Q untuk setiap tindakan yang diambil dalam setiap keadaan. Inilah titik awal yang menarik untuk memahami bahwa RL dapat diaplikasikan dalam konteks praktis dan memperdalam pemahaman tentang konsep-konsep dasarnya.

!pip install gym==0.17.3 --quiet
import numpy as np
import gym
import pickle


Membuat Peta Grid:
Pertama-tama, mari kita bahas dulu permainan Frozen Lake. Ini adalah permainan grid bahwa agen harus bergerak dari posisi awal (S) ke posisi tujuan (G) melalui petak-petak es yang aman (F) sambil menghindari lubang (H). Kita menggunakan peta yang direpresentasikan dalam bentuk string dua dimensi, yakni setiap karakter mewakili elemen pada grid. Ada empat komponen utama dalam peta.

S: Start, yakni posisi awal agen.
F: Frozen, yaitu petak es yang aman untuk dilewati.
H: Hole, yakni lubang yang menyebabkan agen jatuh dan episode berakhir.
G: Goal, yaitu tujuan akhir yang harus dicapai oleh agen.

Kemudian, kita mendefinisikan beberapa peta yang akan digunakan dalam permainan Frozen Lake. Setiap peta adalah string dua dimensi yang mewakili grid permainan.

# Membuat daftar peta
peta = [
    ['SFFF','FHFH','FFFH','HFFG'],
    ['SFFF','FFHF','HFFF','HFFG'],
    ['SHFF','FHFH','FFFH','HHFG'],
    ['SFFF','HHFF','FFFF','HFFG'],
    ['SFFH','FFFH','HFFH','HHFG']
]

Selanjutnya, kita akan memuat lingkungan menggunakan peta pertama dari daftar peta yang telah didefinisikan.

# Memuat lingkungan
env = gym.make("FrozenLake-v0",is_slippery=False, desc=peta[0])

Kemudian, kita akan menghitung berapa banyak petak atau posisi tersedia di atas permukaan es (state) dan berapa banyak langkah yang dapat diambil oleh agen untuk bergerak di sekitar permukaan es (action).

n_observations = env.observation_space.n
n_actions      = env.action_space.n
 
print('Banyak State  : ' + str(n_observations))
print('Banyak Action : ' + str(n_actions))

Hasilnya adalah 16 untuk banyak state dan 4 untuk banyak action. Ini berarti totalnya 16 petak atau posisi yang dapat diakses oleh agen di atas permukaan es dalam permainan Frozen Lake. Masing-masing petak mewakili satu keadaan (state) yang dapat diamati oleh agen. Selain itu, agen memiliki empat aksi yang dapat diambil dalam setiap keadaan: "maju" (up), "mundur" (down), "ke kiri" (left), dan "ke kanan" (right).

Selanjutnya, kita definisikan variabel ACTION yang merupakan daftar berisi empat string: "KIRI", "BAWAH", "KANAN", dan "ATAS"; masing-masing mewakili salah satu dari empat aksi yang dapat diambil oleh agen.

ACTION = ["KIRI","BAWAH","KANAN","ATAS"]

Terakhir, kita mengatur ulang lingkungan ke keadaan awalnya dan menampilkan visualisasi permainan menggunakan perintah env.reset() dan env.render().

env.reset()
env.render()


SFFF
FHFH
FFFH
HFFG

Menguji Langkah Agent:
Langkah pertama yang diambil adalah perintah env.step(2), berarti agen mencoba untuk bergerak ke kanan. Kemudian, kode mencetak hasil langkah tersebut: posisi baru agen (New State), reward yang diterima (Reward), dan status bahwa episode permainan telah selesai (Done). Setelah itu, perintah env.render() digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah diambil.

new_state, reward, done, info = env.step(2)
 
# Menampilkan informasi
print(f"New State : {new_state}")
print(f"Reward    : {reward}")
print(f"Done      : {done}")
 
# Menampilkan visualisasi lingkungan
env.render()

Setelah melakukan aksi "KANAN" dari posisi awal, agen sekarang telah bergeser satu langkah ke kanan dan berpindah ke petak berikutnya dalam grid permainan. Saat ini, agen berada di petak yang ditandai sebagai es yang aman (F). Ini merupakan bagian dari permukaan es yang tidak berbahaya untuk dilewati. Ini juga menunjukkan pergerakan agen dari posisi awal (S) ke posisi yang sekarang ditunjukkan oleh warna merah dalam visualisasi permainan. 

Langkah kedua adalah agen mencoba bergerak ke kanan lagi dengan perintah env.step(2). Hasil langkahnya, seperti posisi baru agen (New State), reward yang diterima (Reward), dan status bahwa episode permainan telah selesai (Done), dicetak. Sesudah itu, perintah env.render() digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah kedua diambil.

# Langkah 2 (ke Kanan)
new_state, reward, done, info = env.step(2)
 
# Menampilkan informasi
print(f"New State : {new_state}")
print(f"Reward    : {reward}")
print(f"Done      : {done}")
 
# Menampilkan visualisasi lingkungan
env.render()

Setelah melakukan aksi "KANAN" dari posisi sebelumnya, agen sekarang telah berpindah ke petak berikutnya dalam grid permainan. Sekarang, agen berada di petak yang sama-sama ditandai sebagai es yang aman (F) seperti sebelumnya. Ini menunjukkan bahwa pergerakan agen dari posisi sebelumnya ke posisi yang sekarang masih berada pada bagian permukaan es yang tidak berbahaya untuk dilewati 

Langkah ketiga adalah agen mencoba bergerak ke bawah dengan perintah env.step(1). Hasil langkahnya dicetak, termasuk posisi baru agen (New State), reward yang diterima (Reward), dan status bahwa episode permainan telah selesai (Done). Kemudian, perintah env.render() digunakan untuk menampilkan visualisasi lingkungan permainan Frozen Lake ke layar dalam keadaan baru setelah langkah ketiga diambil.

# Langkah 3 (ke Bawah)
new_state, reward, done, info = env.step(1)
 
# Menampilkan informasi
print('New State : {}'.format(new_state))
print('Reward    : {}'.format(reward))
print('Done      : {}'.format(done))
 
# Menampilkan visualisasi lingkungan
env.render()

Setelah melakukan aksi "BAWAH" dari posisi sebelumnya, agen kini telah berpindah ke petak baru dalam grid permainan. Sekarang, agen berada di petak yang sebelumnya kosong dan ditandai sebagai es yang aman (F). Ini menunjukkan bahwa agen terus melangkah ke arah lebih dekat ke tujuan akhir atau menghindari lubang dengan hati-hati menjelajahi petak es yang aman. Meskipun tidak ada hadiah atau hukuman yang diberikan oleh lingkungan untuk langkah ini, agen tetap berada dalam perjalanan terencana dan cermat menuju tujuan akhir atau menghindari lubang.

Langkah keempat adalah agen mencoba bergerak ke bawah dengan perintah env.step(1). 

# Langkah 4 (ke Bawah)
new_state, reward, done, info = env.step(1)
 
# Menampilkan informasi
print('New State : {}'.format(new_state))
print('Reward    : {}'.format(reward))
print('Done      : {}'.format(done))
 
# Menampilkan visualisasi lingkungan
env.render()

Setelah melakukan aksi "BAWAH" dari posisi sebelumnya, agen sekarang telah berpindah ke petak baru dalam grid permainan. Kini, agen berada di petak yang sebelumnya kosong dan ditandai sebagai es yang aman (F) seperti sebelumnya. Ini menunjukkan bahwa agen terus melangkah menuju tujuan akhir atau menghindari lubang dengan berhati-hati menjelajahi petak es yang aman. Dengan aksi "BAWAH" ini, agen semakin mendekati tujuan akhir atau memperluas pemahaman tentang lingkungan sekitarnya.

Langkah kelima adalah agen mencoba bergerak ke bawah dengan perintah env.step(1).

Setelah melakukan aksi "BAWAH" dari posisi sebelumnya, agen sekarang berada di petak yang sama seperti sebelumnya dalam grid permainan. Meskipun tidak ada perubahan posisi yang terjadi setelah aksi ini, agen tetap berada di jalur yang benar-benar aman, yaitu petak es yang ditandai sebagai (F). 

Langkah keenam adalah agen mencoba bergerak ke kanan dengan perintah env.step(2). 

new_state, reward, done, info = env.step(2)
 
# Menampilkan informasi
print(f"New State : {new_state}")
print(f"Reward    : {reward}")
print(f"Done      : {done}")
 
# Menampilkan visualisasi lingkungan
env.render()

New State : 15
Reward    : 1.0
Done      : True
  (Right)
SFFF
FHFH
FFFH
HFFG

Setelah melakukan aksi "KANAN" dari posisi sebelumnya, agen berhasil mencapai tujuan akhir dalam permainan Frozen Lake. Pada keadaan ini, lingkungan memberikan hadiah sebesar 1.0 kepada agen, menandakan bahwa agen telah berhasil mencapai tujuan akhir dengan sukses. Selain itu, karena agen telah mencapai tujuan akhir, status "Done" menjadi True, menunjukkan bahwa episode permainan telah selesai.

Dengan berhasil mencapai tujuan akhir, agen telah menyelesaikan permainan Frozen Lake dengan sukses, dan permainan ini sekarang berakhir.


Model Training:
Setelah mencapai tujuan akhir pada permainan Frozen Lake, langkah selanjutnya adalah melakukan pelatihan model untuk meningkatkan kinerja agen dalam permainan ini. Model training melibatkan penggunaan algoritma reinforcement learning, seperti Q-learning untuk mengoptimalkan keputusan agen saat berinteraksi dengan lingkungan. Dalam konteks ini, langkah-langkah selanjutnya akan mencakup proses iteratif bahwa agen akan terus berinteraksi dengan lingkungan, memperbarui nilai-nilai Q untuk setiap tindakan yang diambil, dan belajar dari pengalaman guna meningkatkan strategi permainannya

def train_agent(env, n_episodes=10000, max_iter_episode=100, exploration_proba=1, exploration_decreasing_decay=0.001, min_exploration_proba=0.01, gamma=0.99, lr=0.1):
    # Inisialisasi Q-table dengan ukuran berdasarkan jumlah state dan aksi
    Q_table = np.zeros((env.observation_space.n, env.action_space.n))
    
    # List untuk menyimpan reward dari setiap episode
    rewards_per_episode = []
 
    # Loop melalui setiap episode
    for episode in range(n_episodes):
        # Reset lingkungan untuk memulai episode baru dan mendapatkan state awal
        state = env.reset()
        
        # Inisialisasi total reward episode menjadi 0
        episode_reward = 0
 
        # Loop melalui setiap iterasi dalam episode
        for _ in range(max_iter_episode):
            # Pilih tindakan berdasarkan probabilitas eksplorasi atau menggunakan kebijakan Q
            if np.random.uniform(0, 1) < exploration_proba:
                action = env.action_space.sample()  # Aksi acak (eksplorasi)
            else:
                action = np.argmax(Q_table[state, :])  # Aksi terbaik berdasarkan Q-table (eksploitasi)
 
            # Ambil langkah berdasarkan tindakan yang dipilih
            next_state, reward, done, _ = env.step(action)
 
            # Update Q-value berdasarkan reward yang diterima dan perkiraan nilai Q di state berikutnya
            Q_table[state, action] = (1 - lr) * Q_table[state, action] + lr * (reward + gamma * np.max(Q_table[next_state, :]))
 
            # Tambahkan reward dari langkah ini ke total reward episode
            episode_reward += reward
            state = next_state  # Pindah ke state berikutnya
 
            # Hentikan episode jika mencapai terminal state
            if done:
                break
 
        # Kurangi probabilitas eksplorasi seiring berjalannya waktu
        exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay * episode))
        
        # Simpan total reward episode ke dalam list
        rewards_per_episode.append(episode_reward)
 
    # Cetak rata-rata reward per 1000 episode
    print("Rata-Rata Reward per 1000 Episode")
    for i in range(10):
        print((i + 1) * 1000, " : Rata-Rata Reward: ", np.mean(rewards_per_episode[1000 * i:1000 * (i + 1)]))
 
    # Kembalikan Q-table yang telah dilatih
    return Q_table

Model training ini untuk menghasilkan kebijakan optimal yang memungkinkan agen mencapai tujuan akhir dengan efisien dan konsisten. Selama proses training, agen akan menghadapi berbagai situasi dan memperdalam pemahaman tentang lingkungan sehingga dapat membuat keputusan yang lebih baik di masa depan. Demikianlah, model training merupakan tahap penting dalam pengembangan agen cerdas yang mampu menyelesaikan permainan Frozen Lake dengan keberhasilan konsisten.

for peta_env in peta:  # Loop melalui setiap peta dalam daftar peta
    # Load Environment untuk setiap peta dengan konfigurasi tertentu
    env = gym.make("FrozenLake-v0", is_slippery=False, desc=peta_env)
    env.reset()  # Reset lingkungan ke keadaan awal
 
    print('Peta : ')
    print(peta_env)  # Cetak peta yang sedang digunakan
 
    # Melatih Agent pada lingkungan saat ini
    Q_table = train_agent(env)  # Panggil fungsi train_agent untuk melatih agen di lingkungan saat ini
 
    # Menyimpan Q_table untuk lingkungan saat ini ke dalam list
    Q_table_all.append(Q_table)  # Tambahkan Q_table ke dalam list Q_table_all
 
    print()  # Cetak baris kosong untuk pemisah antara lingkungan yang berbeda


Kode tersebut melakukan training pada agen untuk setiap peta dalam daftar peta yang diberikan. Untuk setiap peta, lingkungan Frozen Lake dibuat dengan konfigurasi tertentu, lalu agen dilatih di lingkungan tersebut. Setelah melatih agen di setiap lingkungan, tabel Q yang dihasilkan disimpan dalam daftar Q_table_all. Proses ini diulang pada setiap peta dalam daftar sehingga akhirnya kita memiliki daftar tabel Q untuk setiap lingkungan. Dengan cara ini, kita dapat melatih agen dan menyimpan informasi pembelajarannya untuk setiap varian lingkungan yang berbeda.

Berikut adalah contoh dari hasil pelatihan model di atas.

Peta : 
['SFFF', 'FHFH', 'FFFH', 'HFFG']
Rata-Rata Reward per 1000 Episode
1000  : Rata-Rata Reward:  0.238
2000  : Rata-Rata Reward:  0.739
3000  : Rata-Rata Reward:  0.91
4000  : Rata-Rata Reward:  0.964
5000  : Rata-Rata Reward:  0.986
6000  : Rata-Rata Reward:  0.994
7000  : Rata-Rata Reward:  0.988
8000  : Rata-Rata Reward:  0.993
9000  : Rata-Rata Reward:  0.991
10000  : Rata-Rata Reward:  0.993

Pada peta tertentu yang ditunjukkan sebagai ['SFFF', 'FHFH', 'FFFH', 'HFFG'], training pada agen dilakukan untuk mempelajari strategi optimal. Setiap 1000 episode, rata-rata reward yang diperoleh oleh agen dicatat. Pada awalnya, rata-rata reward mungkin rendah karena agen belum memiliki strategi yang optimal dan masih sedang belajar. 

Namun, seiring dengan berjalannya waktu dan bertambahnya episode, agen mulai memperbaiki kinerjanya serta mendapatkan rata-rata reward yang lebih tinggi. Peningkatan ini menunjukkan bahwa agen telah belajar untuk membuat keputusan yang lebih baik dan mendekati strategi optimal dalam menavigasi lingkungan Frozen Lake.


Memainkan agen yang sudah dilatih:
Setelah melatih model dengan salah satu peta, langkah selanjutnya adalah memainkan agen yang telah dilatih di lingkungan tersebut. Dalam skrip ini, kita memilih peta dengan mengatur nilai variabel index_peta sesuai keinginan. Kemudian, lingkungan Frozen Lake dibuat menggunakan peta yang dipilih dan permainan dimulai dengan me-reset lingkungan.

index_peta = 0 # silakan pilih peta
 
env = gym.make("FrozenLake-v0",is_slippery=False, desc=peta[index_peta])
env.reset()

Selanjutnya, agen melakukan permainan dengan menggunakan tabel Q yang telah dilatih sebelumnya. Dalam setiap langkah, agen memilih aksi terbaik berdasarkan tabel Q yang telah dipelajari. Informasi tentang langkah tersebut dicetak, termasuk nomor langkah, aksi terbaik oleh agen, keadaan baru setelah langkah, reward yang diterima, dan status episode permainan telah selesai atau belum. 

for langkah in range(1, 7):
    if langkah == 1:
        best_action = np.argmax(Q_table_all[index_peta][0])
    else:
        best_action = np.argmax(Q_table_all[index_peta][current_state])
 
    new_state, reward, done, info = env.step(best_action)
 
    # Cetak informasi langkah
    print('--------------------------------------')
    print('Langkah ke  :', langkah)
    print('Best Action :', ACTION[best_action])
    print('New State   :', new_state)
    print('Reward      :', reward)
    print('Done        :', done)


--------------------------------------
Langkah ke  : 1
Best Action : BAWAH
New State   : 4
Reward      : 0.0
Done        : False
  (Down)
SFFF
FHFH
FFFH
HFFG

--------------------------------------
Langkah ke  : 2
Best Action : BAWAH
New State   : 8
Reward      : 0.0
Done        : False
  (Down)
SFFF
FHFH
FFFH
HFFG


Setelah itu, visualisasi lingkungan diperbarui untuk menampilkan posisi agen dan kondisi lingkungan setelah langkah tersebut. Proses ini diulangi untuk sejumlah langkah tertentu yang telah ditentukan.

Dengan demikian, skrip ini memungkinkan kita melihat agen yang telah dilatih berperilaku dalam lingkungan permainan Frozen Lake dan caranya membuat keputusan untuk mencapai tujuan atau menghindari lubang.


Evaluasi Reinforcement Learning:
Selanjutnya, agen melakukan langkah-langkah berdasarkan strategi terbaik yang telah dipelajari dari tabel Q. Setiap langkah dieksekusi dengan memilih aksi terbaik berdasarkan tabel Q yang telah dilatih sebelumnya. Agen melanjutkan langkahnya hingga mencapai tujuan akhir atau mencapai batas langkah maksimum, yaitu 6 langkah.

for peta_env in peta:  # Iterasi melalui setiap peta dalam daftar peta
    print("Peta   :", peta_env)  # Cetak deskripsi peta yang sedang diperiksa
 
    env = gym.make("FrozenLake-v0", is_slippery=False, desc=peta_env)  # Buat lingkungan permainan Frozen Lake
    env.reset()  # Atur ulang lingkungan ke keadaan awal
 
    # Iterasi untuk agen melakukan langkah-langkah dalam lingkungan
    for langkah in range(1, 7):
        if langkah == 1:
            best_action = np.argmax(Q_table_all[index_peta][0])  # Ambil tindakan terbaik untuk langkah pertama
        else:
            best_action = np.argmax(Q_table_all[index_peta][current_state])  # Ambil tindakan terbaik berdasarkan state saat ini
 
        new_state, reward, done, info = env.step(best_action)  # Lakukan langkah terbaik dalam lingkungan
 
        current_state = new_state  # Perbarui state saat ini
 
    # Periksa apakah agen berhasil menyelesaikan permainan
    if done:
        print("Status : Agent dapat menyelesaikan peta ini")  # Cetak pesan jika agen berhasil menyelesaikan permainan
    else:
        print("Status : Agent tidak dapat menyelesaikan peta ini")  # Cetak pesan jika agen gagal menyelesaikan permainan
 
    print()  # Cetak baris kosong sebagai pemisah antara hasil dari setiap peta

Setelah mencapai akhir episode permainan, evaluasi dilakukan untuk memeriksa bahwa agen berhasil menyelesaikan permainan dengan sukses atau tidak. Jika agen berhasil menyelesaikan permainan, pesan "Status: Agent dapat menyelesaikan peta ini" akan dicetak. Sebaliknya, jika agen gagal menyelesaikan permainan, pesan "Status: Agent tidak dapat menyelesaikan peta ini" akan dicetak. 

Peta   : ['SFFF', 'FHFH', 'FFFH', 'HFFG']
Status : Agent dapat menyelesaikan peta ini

Peta   : ['SFFF', 'FFHF', 'HFFF', 'HFFG']
Status : Agent dapat menyelesaikan peta ini

Peta   : ['SHFF', 'FHFH', 'FFFH', 'HHFG']
Status : Agent dapat menyelesaikan peta ini

Peta   : ['SFFF', 'HHFF', 'FFFF', 'HFFG']
Status : Agent dapat menyelesaikan peta ini

Peta   : ['SFFH', 'FFFH', 'HFFH', 'HHFG']
Status : Agent dapat menyelesaikan peta ini

Terakhir, menyimpan tabel Q yang telah dilatih menggunakan modul pickle dengan nama file 'Q_table_Frozen_Lake.model'. Proses pelatihan agen dan penyimpanan tabel Q ini memungkinkan kita untuk menggunakan kembali tabel Q tersebut pada masa mendatang. Dengan demikian, kita dapat memanfaatkan hasil pelatihan yang telah dilakukan untuk melakukan evaluasi kinerja agen, melakukan perbaikan, atau menerapkan strategi yang telah dipelajari dalam permainan Frozen Lake pada waktu berikutnya.

# Simpan tabel Q yang telah dilatih menggunakan modul pickle
pickle.dump(Q_table_all, open('Q_table_Frozen_Lake.model', 'wb')

Berikut ini yang bukan merupakan perbedaan antara reinforcement learning dan supervised learning adalah ...
a. Tidak adanya pengawasan eksternal (salah)
b. penggunaan umpan balik (salah)
c. pembelajaran melalui trial and error
d. kehadiran label pada data latihan (salah)

Bagaimana reinforcement learning diterapkan dalam pengendalian robot?
c. Dengan memanfaatkan sensor untuk mendapatkan reward

Fungsi yang digunakan dalam reinforcement learning untuk menilai kinerja agen berdasarkan total reward yang diperoleh disebut?
b. Value function

Konsep apa yang digunakan dalam reinforcement learning untuk memberikan umpan balik positif atau negatif kepada agen?
a. Reward function 


Seorang peneliti ingin mengimplementasikan algoritma reinforcement learning untuk mengontrol sistem yang sangat kompleks dan dinamis. Di antara pilihan algoritma berikut, yang mana yang paling sesuai untuk digunakan
a. SARSA
b. Policy Gradient Methods
c. Deep Q-Networks

Dalam reinforcement learning, apa yang dimaksud dengan fungsi kebijakan (policy function)?

A. Fungsi yang menilai kinerja agen berdasarkan total reward yang diperoleh.

B. Fungsi yang memetakan state ke action yang diambil oleh agen.

D. Fungsi yang menghitung error antara prediksi dan target.


Dalam reinforcement learning, apa yang dimaksud dengan metode exploration-exploitation?
A. Metode yang menilai performa agen berdasarkan total reward yang diperoleh.

B. Metode yang menggunakan fungsi kebijakan untuk memilih tindakan yang akan diambil agen.

D. Metode yang mengukur kualitas dari setiap langkah dalam proses pembelajaran.


Pada Markov Decision Process, pemilihan ruas jalan dalam permasalahan shortest path merupakan bagian dari ...
a. Action (salah)
b. State
c. Action
d. Reward 

Pada pertanyaan tersebut terdapat dua opsi 'Action' dimana salah satunya itu salah, saya ingin kamu menjawab pertanyaan tersebut secara teliti sebagai seorang yang ahli dalam teknik ML yaitu Reinforcement Learning

---------------------------------------------------------------------------
                      Konversi Model Machine Learning
---------------------------------------------------------------------------
Proses deployment lebih dari sekadar memasang model pada web browser atau aplikasi mobile, tetapi juga mencakup pemantauan kinerja model secara berkelanjutan, mengidentifikasi dan mengatasi masalah yang mungkin timbul, serta memperbarui model agar tetap relevan dan akurat sesuai dengan kebutuhan.

Dalam modul ini, Anda akan mempelajari langkah-langkah praktis untuk memastikan model yang Anda deploy berfungsi dengan optimal di lingkungan produksi. Setelah menyelesaikan modul ini, Anda akan memiliki pemahaman yang komprehensif tentang seluruh proses deployment, dari persiapan hingga pemeliharaan model di lingkungan produksi. Jadi, tunggu apa lagi?

Setelah fase development, pembentukan model, sampai berhasil melakukan prediksi menggunakan model, kita membutuhkan proses produksi pada machine learning. Mengapa machine learning harus memasuki fase produksi tersebut? Pertama perlu kita pahami bahwa demi membuat sistem machine learning yang baik, kita harus membuat model dengan hasil prediksi yang mudah diakses oleh siapa pun. Artinya, model tersebut bukanlah dalam bentuk black box atau kode-kode dalam Python yang hanya dapat dimengerti oleh para teknisi atau programmer saja. Sistem machine learning harus dapat mengembalikan hasil prediksi dari data yang diberikan secara real-time dan data yang didapat dari real-world (dunia nyata). Oleh karena itu kita harus menyediakan lingkup ekosistem yang lebih besar. Tujuannya adalah agar setelah proses produksi berjalan, model kita dapat diakses oleh setiap orang dengan desain user interface yang lebih baik dan mudah digunakan. Berikut alur umum proses produksi sistem machine learning:

Alur proses produksi dimulai dari proses deployment. Setelah kita memiliki model yang sudah dilatih terhadap suatu dataset, model tersebut kita deploy sesuai platform yang ingin kita gunakan. Proses deployment pada machine learning adalah suatu cara di mana Anda menggunakan model yang telah Anda latih (trained model) dan membuatnya menjadi suatu entitas yang dapat memberikan hasil prediksi dari live data. 

Umumnya model machine learning di-deploy dengan menggunakan 3 cara utama, yakni sebagai berikut:

1. Menggunakan model server
Teknik ini merupakan teknik pengembangan yang paling banyak digunakan. Diperlukan koneksi langsung yang menghubungkan antara user dengan model server. User menginput live data langsung ke model server, kemudian server mengembalikan hasil prediksi. Contoh dari pengembangan ini adalah Tensorflow Serving. Tensorflow Serving menyediakan fungsi untuk memuat model dari sumber disimpannya model. Ini berlaku pada cloud provider seperti bucket AWS S3, GCP, atau Azure. Kemudian model tersebut dijalankan pada suatu environment seperti kontainer (Docker, RedHat, dan lain-lain) yang berisi model dan banyak library penting untuk membangun API. 

Tensorflow Serving menggunakan 2 jenis API yaitu REST dan gRPC yang keduanya merupakan protokol komunikasi. REST API digunakan oleh aplikasi web dan mendefinisikan cara berkomunikasi antara user dengan layanan web. gRPC mendukung lebih beragam format data dibanding REST, namun standar data yang digunakan adalah protocol buffer. Kita dapat memilih untuk menggunakan API yang mana saja. API ini akan menerima permintaan berupa data JSON, memasukkan data tersebut kedalam fungsi, dan mengembalikan respon berupa hasil prediksi

2. Browser
Terdapat beberapa situasi di mana user tidak ingin menginput data langsung ke model server. Sebabnya bisa beberapa hal seperti data yang diinput adalah data sensitif, atau ada hal-hal yang menyangkut privasi. Pada kasus ini model machine learning dapat di-deploy ke browser user. Jadi, hasil prediksi bisa didapatkan sebelum data diunggah ke cloud server. Contoh mekanisme ini berlaku pada Tensorflow.js atau ONNX.js. 

3. Edge devices
Mekanisme ini digunakan jika user tidak dapat terkoneksi dengan model server untuk mendapatkan hasil prediksi. Misalnya karena model tersebut ditanamkan pada IoT device alias peranti dengan sensor jarak jauh. Model dilatih terlebih dahulu dari pusat, kemudian model dibuat dalam format portable dan dipindahkan ke edge devices.
Dengan begini, prediksi mudah dilakukan pada peranti mobile tanpa harus terkoneksi ke pusat penyimpanan model. Jadi, dengan jaringan yang terbatas sekalipun, model tersebut masih dapat digunakan untuk membangun hasil prediksi. Pada HP kita, teknik ini diterapkan pada TFLite. Eits, perangkat yang dimaksud tak terbatas pada mobile phone saja. Teknik yang sama dapat terapkan pada IoT device.  

Pada proses deployment tidak ada perbedaan khusus antara kode yang kita implementasikan pada saat pembentukan trained model (model ketika dilatih), dengan deployed model (model saat digunakan untuk prediksi). Kode pada penggunaan model saat training dengan kode pada deployment umumnya sama saja.

Hal ini mempermudah pengaplikasian deployment karena penggunaan kode lebih fleksibel dan tidak dibutuhkan suatu abstraksi. Selain itu jika ada keperluan untuk memperbarui model dalam proses pengembangan, deployment pun mudah dilakukan. Perubahan apa pun dapat Anda lakukan dengan cepat sehingga mendukung proses re-deployment.

Setelah deployment berjalan, evaluasi pada model harus digerakkan. Evaluasi di sini pada dasarnya adalah bagaimana kita menilai performa dari model yang telah kita buat dengan metrik yang tepat. Proses evaluasi ini dapat dilakukan pada model yang sudah maupun yang belum di-deploy. Pada model yang belum di-deploy, lakukan evaluasi offline menggunakan validation set. Kemudian kita ukur performanya menggunakan 2 buah metrik berbeda yaitu machine learning metrik (akurasi, precision, recall, dll.) dan bisnis metrik (user engagement, click-through rate, dll). Pada model yang telah di-deploy lakukan evaluasi secara online. Gunakan live data untuk diukur terhadap metrik bisnisnya saja. Evaluasi pada kedua komponen ini berbeda (online dan offline) dan harus dijalankan terpisah.

Melalui evaluasi, kualitas model kita jadi dapat terukur sejalan dengan feedback yang kita serap dari lingkungan produksi. Selanjutnya lakukan monitoring dan manajemen pada model. Pada tahap ini kita tentukan apa saja yang harus ditingkatkan dari model yang telah dibuat, kapan diperlukan pembaruan model, atau dilakukan deployment ulang. Untuk penjelasan lebih lanjut tentang model monitoring, simak sub-bab berikutnya ya!


Format Penyimpanan Model:

Sekarang, kita akan mempelajari beberapa format umum yang digunakan untuk menyimpan model serta cara mengimplementasikannya. Terdapat  tiga format penyimpanan model yang biasa digunakan pada library TensorFlow, antara lain:

1. Format SavedModel
Format SavedModel merupakan format standar untuk menyimpan model pada TensorFlow. Format SavedModel berisi program TensorFlow lengkap dengan bobotnya. 

Format ini merupakan format penyimpanan yang paling direkomendasi dalam proses deployment model dengan TensorFlow Lite, TensorFlow.js, dan TensorFlow Serving. Selain itu, SavedModel juga sering digunakan untuk berbagi model melalui TF-hub. 

Anda dapat menggunakan perintah berikut untuk menyimpan model dalam bentuk format SavedModel.

save_path = 'mymodel/'
tf.saved_model.save(model, save_path)

Jika proses penyimpanan model berhasil, akan muncul folder ‘mymodel’ dengan file dan folder sebagai berikut.


2. Format HDF5
Format HDF5 merupakan format penyimpan model dasar yang dikhususkan untuk lingkungan Keras. Format penyimpanan ini didefinisikan dalam ekstensi .h5. Model ini lebih simpel dan ringan dari pada SavedModel, tetapi informasi yang ada di dalamnya cukup untuk menjalankan model. 

Format HDF5 akan menyimpan informasi dalam bentuk graf, variabel berisi hyperparameter, bobot, status optimizer, dan status akhir checkpoint ketika training. Model jenis ini memudahkan kita ketika ingin melanjutkan proses training karena status terakhirnya tersimpan. Untuk menyimpan model dalam bentuk format HDF5, kita dapat menggunakan perintah berikut:

model.save("model.h5")


3. Format ONNX
ONNX (Open Neural Network Exchange) merupakan format penyimpanan yang dapat menjalankan model pada frameworks AI yang berbeda. Sehingga, kita dapat menguji dan menggunakan model yang sama pada lingkungan serta devices yang berbeda. 

Sebelum menyimpan model TensorFlow dengan format ONNX, terdapat beberapa hal yang harus disiapkan, antara lain:

- Menginstal onnxruntime dan tf2onnx dengan menjalankan perintah berikut:
!pip install onnxruntime
!pip install -U tf2onnx

- Model TensorFlow yang disimpan dalam format SavedModel.

Selanjutnya, Anda dapat mengubah model tersebut menjadi format ONNX dengan menjalankan perintah di bawah ini:

!python -m tf2onnx.convert --saved-model "saved_model" --output "model.onnx" --extra_opset ai.onnx.contrib:1

Jika semua tahapan berjalan dengan baik, akan muncul sebuah file model baru bernama model.onnx.

Anda dapat melihat contoh notebook untuk mengubah SavedModel menjadi format ONNX pada tautan berikut: try_convert_onnx. (https://github.com/rfajri27/try_convert_onnx/blob/main/try_convert_tf2onnx.ipynb)



Pengenalan TensorFlow.js:
Pada materi kali ini kita akan mempelajari bagaimana cara membangun aplikasi ML yang dijalankan pada Web Browser menggunakan Tensorflow.js. Ia adalah sebuah framework yang kompatibel dengan TensorFlow API. TensorFlow.js menggunakan model yang telah dibuat dengan mengubah format model menjadi JSON file. Pada level atas TensorFlow.js API, kita akan dihadapkan dengan Layers API. Jika kita familiar dengan penggunaan layer-layer pada Keras, kita dapat dengan mudah menggunakan Layers API. Di bawah layers API, ada Core API yang menangani model dari TensorFlow. Core API juga mengimplementasikan operasi graf pada level lebih kompleks seperti deklarasi tensor (data input), operasi pada tensor, memori, eksekusi fungsi, dan lain-lain. 

Selain itu CORE API bekerja dengan browser dan menggunakan WebGL untuk menggunakan resource yang mendukung proses training atau pengambilan hasil prediksi (inference). Pada Node.js, kita bisa membuat aplikasi server-side dan kemudian menggunakan resource yang tersedia seperti CPU, GPU, atau TPU.

Terdapat beberapa keuntungan menggunakan TensorFlow.js. TensorFlow.js ditulis dalam bahasa Script sehingga memudahkan integrasi dengan teknologi web seperti user interface yang baik, dan beberapa utilitas yang dapat kita impor seperti DOM (Document Object Model) Canvas yang memungkinkan kita mendapat data langsung dari input user dari internet.

Salah satu kekurangan penggunaan Web Browser sebagai ML platform adalah dari segi performa. Web Browser biasanya adalah aplikasi dengan single proses dan tidak bekerja intens dengan CPU. Namun modern Web Browser sudah menyediakan API yang bisa memanfaatkan lokal hardware akselerator dengan menggunakan WebGL, atau WebGPU. Dengan adanya API tersebut, TensorFlow.js dapat memberikan performa yang sama baiknya walaupun dengan menggunakan browser. 

Machine learning yang berbasis TensorFlow.js memungkinkan kita menjaga privasi data karena kita dapat melakukan pelatihan dan pengambilan hasil prediksi dari sisi client saja. User dapat memanfaatkan fungsi ML sembari menjaga data mereka tetap privat tanpa harus dikirim ke server.


Perbedaan Conventional TensorFlow dan TensorFlow.js:

Perbedaan utama antara TensorFlow.js dan TensorFlow konvensional adalah platform dan bahasa pemrograman yang digunakan, serta kasus penggunaan yang spesifik. Berikut adalah beberapa perbedaan utama:

1. Platform
- TensorFlow.js: Dirancang untuk berjalan di dalam browser web atau di platform JavaScript lainnya seperti Node.js. Ini memungkinkan pengembangan dan eksekusi model machine learning di lingkungan klien tanpa memerlukan server backend
- TensorFlow Konvensional: Biasanya dijalankan di server atau mesin lokal dengan lingkungan Python. TensorFlow konvensional lebih sering digunakan untuk pengembangan dan pelatihan model skala besar di lingkungan yang lebih kuat.

2. Bahasa Pemrograman
- TensorFlow.js: Ditulis dalam JavaScript. Penggunaannya sangat cocok untuk pengembang web yang sudah familiar dengan ekosistem JavaScript.
- TensorFlow Konvensional: Ditulis dalam Python (meskipun ada binding untuk bahasa lain seperti C++ dan Java). Lebih sering digunakan oleh ilmuwan data dan insinyur machine learning yang bekerja dalam ekosistem Python.

3. Kasus Penggunaan
- TensorFlow.js: Ideal untuk aplikasi interaktif yang berjalan langsung di browser. Misalnya, aplikasi web yang membutuhkan inferensi model secara real-time tanpa mengirim data ke server, seperti aplikasi pengenalan wajah, klasifikasi gambar, atau prediksi teks yang langsung dijalankan di browser pengguna.
- TensorFlow Konvensional: Lebih cocok untuk pelatihan model skala besar dan deployment di server atau cloud. Digunakan dalam skenario di mana diperlukan sumber daya komputasi yang besar dan infrastruktur yang kompleks.

4. Instalasi dan Dependensi
- TensorFlow.js: Mudah diintegrasikan ke dalam aplikasi web menggunakan package manager seperti npm atau langsung dengan tag script di HTML. Tidak memerlukan instalasi yang rumit.
- TensorFlow Konvensional: Memerlukan instalasi library TensorFlow melalui package manager seperti pip, dan sering kali memerlukan konfigurasi tambahan untuk mendukung GPU atau lingkungan khusus.

5. Performa
- TensorFlow.js: Terbatas oleh kinerja JavaScript dan kemampuan hardware dari perangkat klien (misalnya CPU dan GPU pada perangkat pengguna). Ini bisa menjadi kendala untuk model yang sangat kompleks atau membutuhkan komputasi intensif
- TensorFlow Konvensional: Memanfaatkan sepenuhnya kemampuan hardware dari mesin lokal atau server, termasuk dukungan penuh untuk GPU dan TPU. Lebih cocok untuk pelatihan dan inferensi model yang besar dan kompleks.

6. Ekosistem
- TensorFlow.js: Memiliki ekosistem yang berkembang dengan alat dan plugin yang berfokus pada pengembangan web. Namun, komunitasnya mungkin tidak sebesar komunitas Python.
- TensorFlow Konvensional: Mendukung berbagai macam alat, pustaka tambahan, dan memiliki komunitas yang sangat besar dan aktif, memberikan banyak sumber daya dan dukungan.


Menggunakan TensorFlow.js API pada Web Browser:
Ada 2 cara untuk mempersiapkan environment TensorFlow.js, yaitu:

1. Gunakan kode JavaScript yang didistribusikan pada CDN (Content Distribution Network) dengan cara menambahkan link pada tag <script> di html. Class TensorFlow.js dapat diakses dengan menggunakan kode tf.

<script src= "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>

2. Gunakan package yang didistribusikan package manager seperti npm. Jika kita ingin menggunakan aplikasi tersebut tanpa terkoneksi ke internet, maka TensorFlow.js harus di-impor langsung melalui npm. Install TensorFlow.js menggunakan yarn atau npm command:

yarn add @tensorflow/tfjs

 atau dengan npm:

npm install @tensorflow/tfjs 

 kemudian lakukan import pada package tersebut:

import * as tf from ‘@tensorflow/tfjs’

Instalasi dengan yarn atau npm biasanya dilakukan ketika menggunakan Node.js. Karena pada pembelajaran kali ini kita tidak menggunakan Node.js, sepanjang pembahasan TensorFlow.js akan menggunakan cara 1 yaitu impor TensorFlow.js dilakukan menggunakan script yang mengacu pada CDN.

Nah, environment sudah siap. Sekarang waktunya kita menyimpan model pada Browser. Ketika kita ingin membuat model baru, kita akan dihadapkan pada Layers API. Sedangkan jika kita ingin memakai model yang telah kita buat di Python, kita akan bekerja dengan CORE API yang akan menangani model yang diekspor dari TensorFlow. Ada 2 cara untuk membuat ML model baru di JavaScript dengan menggunakan TensorFlow.js pada Layers API. Layers API mirip sekali dengan penggunaan Keras dari tipe parameter ataupun penggunaan fungsi namun dengan sedikit sintaks yang berbeda karena API ini dijalankan pada JavaScript.


Membuat Model Baru pada TensorFlow.js
Pada TensorFlow.js, kita bisa membuat model baru, ataupun menggunakan model yang pernah kita buat sebelumnya. Untuk membuat model baru di lingkungan JavaScript dapat dilakukan dengan 2 cara:

1. Menggunakan model sekuensial yang didefinisikan dengan tf.Sequential():
tf.Sequential():
    const model = tf.Sequential({
        layers: [
            tf.layers.dense({inputShape: [256], units: 24, activation: ‘relu’}),
            tf.layers.dense({units:10, activation: ‘softmax’}),
    ]});

Karena kita menggunakan bahasa JavaScript, maka parameter yang diinputkan berupa dictionary, sehingga kita memakai kurung kurawal ya ({}) di sini.

2. Membuat model fungsional yang didefinisikan dengan penggunaan .apply(). Pada model fungsional kita bisa menambahkan layer ke model walau tidak berbentuk siklus, contoh:

const input = tf.input({shape: [256]});
const dense1 = tf.layers.dense({units: 24, activation: ’relu’}) .apply(input);
const dense2 = tf.layers.dense({units:5, activation: ‘softmax’ }) .apply(dense1);
 
const model = tf.model({inputs: input, output: dense2});


Konversi Model TensorFlow ke TensorFlow.js?
1. Untuk model TensorFlow dengan format SavedModel, gunakan command ini untuk konversi ke model TensorFlow.js:

tensorflowjs_converter \
    --input_format = tf_saved_model \
    --output_node_names = ‘MobilenetV1/Predictions/Reshape_1‘ \
    /path/to/saved_model \
    /path/to/web_model

Pada konversi dari SavedModel, --output_node_names tidak dipanggil pun tidak masalah karena dia di-generate secara otomatis. Jika kita ubah /path/to/saved_model dan /path/to/web_model ke direktori yang kita inginkan maka kodenya akan menjadi lebih singkat seperti ini:

!tensorflowjs_converter \
    --input_format = tf_saved_model \
    /content/mymodel/ \
    /content/modeltfjs

Command tersebut akan menghasilkan model dalam format JSON dan beberapa weight berbentuk binary file. Binary file yang dihasilkan dapat lebih dari satu file.


Tips:
Pada model-model tertentu, output_node_names suatu model dapat lebih dari satu atau terkadang TensorFlow meminta output_node_names harus dideklarasikan. Misalnya dalam konversi dari bentuk frozen model yang akan dijelaskan pada poin di bawah. Jika kita harus mengetahui nama node output dari model yang kita miliki, kita bisa menggunakan contoh sintaks ini:

!saved_model_cli show --dir /content/mymodel --tag_set serve --signature_default

Setelah mengetahui nama node output, ubah sintaks kita menjadi:

tensorflowjs_converter \
    --input_format = tf_saved_model \
    --output_node_names = ‘output_1’ \
    --saved_model_tags = serve \
    /content/mymodel/ \
    /content/modeltfjs


2. Untuk model berupa Keras HDF5, konversi ke model TensorFlow.js dengan command ini:

tensorflowjs_converter \
    --input_format=keras \
    /content/mymodel /content/modeltfjs


3. Untuk frozen model, konversi dengan command:

tensorflowjs_converter \
    --input_format=tf_frozen_model \
    --output_node_names = ’MobilenetV1/Predictions/Reshape_1’ \
    /content/mymodel.pb \
    /content/modeltfjs

4. Untuk model yang didapat dari TensorFlow Hub, kita gunakan command ini:

tensorflowjs_converter \
    --input_format=tf_hub \
   ’https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/classification/1’ \
    /content/modeltfjs


Setelah kita dapatkan model dalam bentuk TensorFlow.js, yang kita lakukan selanjutnya adalah me-load model tersebut pada Web Browser. Ada 2 cara untuk memanggil model yaitu dengan sintaks tf.loadLayersModel() atau tf.loadGraphModel(). loadLayersModel digunakan untuk memanggil model Keras, sedangkan loadGraphModel untuk memanggil model dari bentuk SavedModel atau TensorFlowHub. Perbedaannya adalah loadGraphModel mengembalikan frozen model dimana parameter yang ada tidak dapat diubah dan model tersebut tidak bisa dilakukan fine-tune lagi.

Jika kita ingin memanggil model yang bisa dilatih ulang, kita harus memanggil model Keras dengan sintaks loadLayersModel(). Berikut contoh pemanggilan model pada JavaScript:

import * as tf from '@tensorflow/tfjs';
const model_url = 'http://127.0.0.1:8887/model.json';  
const model = await tf.loadLayersModel(model_url);

Pemanggilan model dilakukan lewat http, jadi kita memang membutuhkan web server untuk menjalankan kode ini. Voila! sekarang model sudah siap digunakan di browser untuk training ulang, evaluasi, atau untuk mengembalikan hasil prediksi hanya dengan memanggil sintaks ini:

const example = tf.fromPixels(webcamElement);  
const prediction = model.predict(example);




Latihan: Deploy Model ML Menggunakan TensorFlow.js:

https://www.dicoding.com/academies/185/tutorials/10229?from=10224

- Memuat dan menjalankan model pada Web Browser dengan TensorFlow.js API.
- Menguji hasil deployment dengan membuat prediksi dari input yang diberikan.

https://github.com/dicodingacademy/a185-pengembangan-ml/tree/main/sample_project_tfjs/starter

`notebook.ipynb` merupakan berkas notebook yang dapat dijalankan melalui Google Colaboratory. Notebook ini berisi tahapan dalam membuat, mengevaluasi, dan mengonversi model.
`yelp_labelled.txt` merupakan dataset untuk melatih model.
`model.h5` merupakan sebuah HDF5 model yang akan dikonversi menjadi `model.json`.
`tfjs_model` merupakan sebuah folder yang berisi `model.json` dan bobot dalam berbentuk binary file. Kedua file tersebut digunakan untuk menerapkan model NLP ke dalam web browser.
`word_index.json` merupakan sebuah metadata dalam bentuk file json yang berisi pasangan kata dan indeks. Dengan memanfaatkan berkas ini, kita dapat mengubah inputan reviu menjadi sebuah token.
 `index.html` merupakan berkas html sebagai tampilan utama web.
`script.js` merupakan berkas JavaScript yang berisi perintah untuk men-deploy model yang telah dibuat ke dalam web.
`images` berisi beberapa gambar seperti plot evaluasi model yang telah dibuat dan tampilan awal web.
`styles` berisi `style.css` dan beberapa gambar untuk memperindah tampilan web.



Menerapkan Model pada Web Browser dengan TensorFlow.js:

Untuk menerapkan model yang telah dibuat di web browser, buatlah file ‘index.html’ sebagai tampilan utama web. Selain itu, buatlah file ‘script.js’ yang berisi perintah untuk men-deploy model

Pada contoh proyek yang telah diberikan, Anda akan menemukan file ‘index.html’ dan file ‘script.js’. Buka kedua file tersebut menggunakan text editor favorit Anda, kemudian ikuti petunjuk berikut.



TensorFlow Lite:
TensorFlow Lite (TF-Lite) merupakan sebuah framework yang dapat menjalankan model TensorFlow pada perangkat mobile dan IoT. 

Machine Learning pada perangkat mobile dapat memudahkan manusia dalam menyelesaikan tugas sehari-hari. Berikut beberapa contoh penggunaannya  

1. Google Translate Instant Camera Translation, yang dapat Menerjemahkan teks bahasa asing dengan foto. Fitur ini dapat membantu turis mancanegara dalam memahami bahasa asing ketika berlibur tanpa bantuan penerjemah. Sehingga, turis mancanegara tidak perlu khawatir ketika berlibur ke negara asing.

2. Selanjutnya, kita juga bisa mencoba alat make-up secara online pada aplikasi e-commerce. Fitur ini memungkinkan para pengguna seolah benar-benar mencoba produk make-up seperti warna lipstik, pensil alis, eye shadow, dan produk make-up lainnya ke wajah mereka.


Keuntungan TF-Lite:
- Penggunaan TF-Lite tidak memerlukan server, sehingga perangkat tidak harus terhubung ke internet untuk melakukan prediksi dan mampu menjaga privasi pengguna.
- Memiliki Latency dan ukuran binary yang kecil, sehingga dapat mengurangi konsumsi daya ketika melakukan prediksi.

Sebelum menjalankan model TensorFlow pada perangkat mobile, kita perlu memahami arsitektur TF-Lite terlebih dahulu. Berikut merupakan ilustrasi sederhana arsitektur TF-Lite. 

Setelah melatih dan menyimpan TensorFlow model, kemudian ubahlah model tersebut menjadi tflite model. Untuk mengubah model menjadi tflite model kita dapat menggunakan TFLite Converter, berikut cara penggunaan TFLite Converter untuk mengonversi model menjadi tflite model:
- Konversi model yang disimpan dalam format SavedModel

converter = tf.lite.TFLiteConverter.from_saved_model(“SavedModel_path”)
tflite_model = converter.convert()
 
with tf.io.gfile.GFile('model_name.tflite', 'wb') as f:
    f.write(tflite_model)

- Konversi model yang disimpan dalam format keras model
converter = tf.lite.TFLiteConverter.from_keras_model(“keras_model_path”)
tflite_model = converter.convert()
 
 
with tf.io.gfile.GFile('model_name.tflite', 'wb') as f:
    f.write(tflite_model)

Kemudian tflite model tersebut akan dieksekusi menggunakan TensorFlow Lite interpreter. TF-Lite juga dilengkapi dengan API untuk berbagai bahasa pemrograman, seperti Swift, C, C++, Java, dan Python.

Selanjutnya, kita akan menjalankan model machine learning pada aplikasi android dengan menggunakan TF-Lite. Apakah Anda sudah tidak sabar untuk masuk ke materi latihan? Yuk, lanjut ke materi berikutnya.



Latihan: Deploy Model ML Menggunakan TensorFlow Lite

Pada sub-modul ini, kita akan belajar menjalankan model yang telah kita buat untuk melakukan tugas prediksi dalam lingkup klasifikasi gambar. Tugas prediksi akan dijalankan di android menggunakan TensorFlow Lite. Adapun pengetahuan dasar terkait Android Developer dapat membantu Anda untuk  memahami sub-modul ini dengan baik.

Berikut beberapa hal yang akan kita pelajari:

Memuat dan menjalankan model di Android dengan TensorFlow Lite.
Menguji hasil deployment melalui proses klasifikasi input gambar yang diberikan.
Pada latihan ini, kita akan membuat aplikasi Android yang dapat mengklasifikasi input gambar. Model yang digunakan akan dilatih menggunakan dataset dari OpenImages. OpenImages menyediakan banyak dataset gambar yang bisa kita manfaatkan.

Penjelasan Proyek
Tujuan dari proyek ini adalah membuat sebuah model yang dapat mengklasifikasi gambar bahan makanan. Model ini nantinya akan dimuat dan dijalankan pada sebuah aplikasi Android sederhana menggunakan TensorFlow Lite.

Proyek ini menggunakan dataset gambar bahan makanan yang diperoleh dari OpenImage. Dataset yang digunakan terdiri dari lima kategori bahan makanan yaitu Broccoli, Carrot, Cheese, Potato, dan Tomato.

Berikut merupakan tampilan awal dari aplikasi Android sederhana yang akan kita buat pada proyek ini.


Penjelasan File
Setelah mengunduh contoh proyek, Anda akan menjumpai beberapa file dan folder sebagai berikut:

‘dataset_1’ merupakan sebuah folder berisi dataset untuk melatih dan mengevaluasi model.
`notebook.ipynb` merupakan sebuah berkas notebook yang dapat dijalankan melalui Google Colab. Notebook ini berisi tahapan dalam membuat dan mengonversi model.
`saved_model` merupakan sebuah folder yang berisi model yang disimpan dalam format SavedModel yang selanjutnya akan dikonversi menjadi tflite model.
`vegs.tflite` merupakan model yang telah dikonversi menjadi tflite model. Model ini digunakan untuk menerapkan model klasifikasi gambar yang dijalankan di aplikasi Android.
`vegs.txt` merupakan sebuah berkas text yang berisi label untuk dataset.
`SimpleImageClassification` merupakan folder project Android Studio yang digunakan untuk membuat aplikasi Android.


Menerapkan Model pada Aplikasi Android dengan TensorFlow Lite
Sebelum menerapkan model tflite pada aplikasi Android, buatlah file `vegs.txt` yang berisi label dari dataset yang kita gunakan. Caranya mudah, bukalah aplikasi Notepad di komputer Anda dan sailnlah label-label berikut. 

Lalu, simpan berkas tersebut dengan nama ‘vegs.txt’.

Untuk menerapkan model tflite pada aplikasi Android, buatlah sebuah proyek sederhana di Android Studio. Anda akan menemukan sebuah folder “SimpleImageClassification” yang merupakan contoh proyek sederhana di Android Studio. Buka folder tersebut menggunakan Android Studio dengan mengikuti petunjuk di bawah ini.

Membuat folder ‘assets’
Salin file `vegs.tflite` dan file `vegs.txt` pada folder assets yang terdapat dalam path app > src > main. Tahap ini dilakukan agar Android Studio dapat secara otomatis memanggil file model beserta labelnya.

Membuat file ‘TFLiteHelper.java’
Masih di proyek yang sama, selanjutnya bukalah berkas ‘TFLiteHelper.java’. Berkas tersebut  terletak dalam path app > src > main > java > com > example > imageclassification. Kemudian di dalam TFLiteHelper.java, lakukan inisiasi Interpreter TensorFlow Lite dengan kode berikut.

    void init() {
        try {
            Interpreter.Options opt = new Interpreter.Options();
            tflite = new Interpreter(loadmodelfile(context), opt);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

Tahap berikutnya adalah melakukan preprocessing terhadap input gambar yang dimasukkan oleh pengguna. Pada tahap preprocessing, kita melakukan beberapa hal berikut:

Mengubah format gambar menjadi TensorImage.
Mengubah ukuran gambar.
Melakukan normalisasi.
Untuk melakukan tahap preprocessing, implementasikan kode berikut.


Setelah itu, kita perlu memanggil dan membaca model `vegs.tflite` dengan menambahkan fungsi berikut.

private MappedByteBuffer loadmodelfile(Activity activity) throws IOException {
        String MODEL_NAME = "vegs.tflite";
        AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(MODEL_NAME);
        FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());
        FileChannel fileChannel = inputStream.getChannel();
        long startoffset = fileDescriptor.getStartOffset();
        long declaredLength = fileDescriptor.getDeclaredLength();
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startoffset, declaredLength);
    }

Tahap berikutnya adalah menambahkan fungsi untuk melakukan klasifikasi gambar. Output dari fungsi ini berupa nilai probabilitas sebuah gambar yang diklasifikasikan ke dalam kategori atau label tertentu

Seperti yang telah dijelaskan sebelumnya, output dari fungsi di atas berupa nilai probabilitas. Sehingga, kita perlu melakukan tahap postprocessing terlebih dahulu untuk memilih label dengan probabilitas paling tinggi. Tahap postprocessing dilakukan menggunakan file `vegs.txt` sebagai acuan label.

public String showresult() {
        try {
            labels = FileUtil.loadLabels(context, "vegs.txt");
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
        Map<String, Float> labeledProbability =
                new TensorLabel(labels, probabilityProcessor.process(outputProbabilityBuffer))
                        .getMapWithFloatValue();
        float maxValueInMap = (Collections.max(labeledProbability.values()));
        String result = null;
        for (Map.Entry<String, Float> entry : labeledProbability.entrySet()) {
            if (entry.getValue() == maxValueInMap) {
                result = entry.getKey();
            }
        }
 
        return result;
    }
 
    private TensorOperator getPostprocessNormalizeOp() {
        return new NormalizeOp(PROBABILITY_MEAN, PROBABILITY_STD);
    }

Membuat file ‘MainActivity.java’
Sebagai penutup, hubungkan UI dan `TFLiteHelper.java`. Caranya, bukalah berkas ‘MainActivity.java’ dan tambahkan perintah berikut:

Menguji Hasil Deployment
Pada tahap ini Anda akan menguji hasil deployment dengan menjalankan model untuk mengklasifikasi gambar bahan makanan yang diinput oleh pengguna. Untuk menguji hasil deployment, jalankan aplikasi Android menggunakan Android Emulator yang tersedia pada Android Studio. Berikut merupakan tahapan untuk menjalankan aplikasi Android menggunakan Android Emulator:

Pastikan Anda telah membuat virtual device pada Android Studio.
Pilih virtual device yang Anda inginkan (pada latihan ini menggunakan “Pixel 5”).
Tekan tombol “Run app”.

Berikut merupakan gambar tahapan untuk menjalankan aplikasi Android menggunakan Android Emulator.



Pengenalan TensorFlow Serving:
 TensorFlow Serving (TF-Serving) merupakan sebuah sistem server yang mempermudah proses deployment model pada server. TF-Serving memungkinkan kita mengatur beberapa versi model sambil tetap menggunakan arsitektur server dan API yang sama.

Gambar di atas menunjukkan arsitektur dari TF-Serving. Pada arsitektur TF-Serving, servable handler merupakan objek dasar yang digunakan oleh client untuk melakukan komputasi. Seluruh proses pada TF-Serving, termasuk pengaturan versi model yang digunakan untuk melakukan prediksi, diatur oleh model manager.

Menyiapkan TensorFlow Serving
Sebelum masuk ke proses deployment model pada server menggunakan TF-Serving, siapkan seluruh kebutuhan sistem untuk menjalankan TF-Serving terlebih dahulu. Salah satu cara yang paling mudah untuk menyiapkan kebutuhan sistem adalah menggunakan Docker image. Itulah sebabnya Anda perlu menginstal Docker terlebih dahulu.

Menyiapkan Kebutuhan Sistem pada Docker
Setelah proses instalasi Docker selesai, siapkan kebutuhan sistem menggunakan Docker image. Docker image untuk TF-Serving dapat disalin dari Docker hub dengan menjalankan perintah berikut pada PowerShell/Terminal: 

docker pull tensorflow/serving

Jika proses di atas berjalan dengan baik, Anda akan menemukan tensorflow/serving pada daftar Docker image local.


Install TensorFlow Serving Python API
Langkah berikutnya adalah menyiapkan TF-Serving Python API. API ini dapat diinstal menggunakan perintah berikut:

pip install tensorflow-serving-api

Sampai di tahap ini, Anda telah berhasil menyiapkan seluruh kebutuhan sistem untuk menggunakan TF-Serving. Pada materi selanjutnya, kita akan berlatih menjalankan model machine learning pada server menggunakan TF-Serving. Sudah siap untuk lanjut? 



Latihan: Deploy Model ML Menggunakan TensorFlow Serving:
Penjelasan Proyek
Tujuan dari proyek ini adalah membuat sebuah model yang dapat melakukan klasifikasi gambar rock-paper-scissors. Model ini nantinya akan dimuat dan dijalankan menggunakan TF-Serving.

Dataset yang akan digunakan adalah rock-paper-scissors. Dataset tersebut memiliki tiga kategori, tiap kategorinya memiliki lebih dari 700 sampel gambar. Berikut beberapa sampel gambar dari setiap kategori.

`dataset` merupakan sebuah folder berisi dataset untuk melatih dan mengevaluasi model.
‘notebook.ipynb’ merupakan sebuah berkas notebook yang dapat dijalankan melalui Google Colab. Notebook ini berisi tahapan dalam membuat dan menyimpan model dalam format SavedModel. 
‘models’ merupakan sebuah folder yang berisi model yang disimpan dalam format SavedModel.
‘image’ merupakan sebuah folder yang berisi beberapa gambar yang akan digunakan sebagai input gambar untuk menguji hasil deployment.
‘Test.ipynb’ merupakan sebuah berkas notebook yang digunakan untuk membuat request prediksi menggunakan API yang ada pada TF-Serving.

Proses Deployment Model Menggunakan TF-Serving
Sebelum menerapkan model dengan TF-Serving, pastikan kembali Anda telah menginstal Docker image untuk TF-Serving dan TF-Serving Python API. 

Selanjutnya, Anda dapat mulai proses deployment dengan menjalankan perintah berikut pada PowerShell/Terminal:

docker run -it -v YOUR_PATH\models:/models -p 8501:8501 --entrypoint /bin/bash tensorflow/serving

Perintah di atas akan dieksekusi oleh Docker container. Apakah Anda masih bingung? Simak penjelasan setiap perintahnya di bawah ini:

Bagian ‘--it’ merupakan perintah untuk membuat Docker container menjadi interaktif. 
Bagian ‘-v’ berfungsi untuk mengarahkan direktori lokal  (YOUR_PATH\models) ke dalam directory container (/models). 
Bagian ‘-p 8501:8501’ berfungsi untuk meneruskan TCP port host (8501) ke TCP port container (8501).
Bagian ‘--entrypoint /bin/bash’ digunakan untuk masuk ke bagian shell Docker container.

Tahap selanjutnya adalah menjalankan model pada Docker container dengan perintah berikut:

tensorflow_model_server --rest_api_port=8501 --model_name=rps_model --model_base_path=/models/rps_model/

Setelah menjalankan model pada Docker container, ujilah hasil deployment dengan menyalin URL (http://localhost:8501/v1/models/rps_model) pada alamat web browser. Jika proses deployment berjalan dengan baik, akan muncul respon JSON sebagai berikut:

Hasil di atas merupakan status dari model yang dijalankan melalui TF-Serving. Hasil ini menunjukkan bahwa kita memiliki sebuah model versi 1 yang berstatus “AVAILABLE”.



Menguji Hasil Deployment:

Pada tahap ini, Anda akan menguji hasil deployment dengan membuat request prediksi untuk mengklasifikasi gambar rock-paper-scissors. Request prediksi ini dijalankan dengan API yang ada pada TF-Serving.

Perlu diingat, input gambar yang Anda masukkan tidak bisa langsung diolah oleh model. Sehingga, perlu dilakukan tahap preprocessing terlebih dahulu. Pada tahap preprocessing, ada beberapa hal yang bisa dilakukan, antara lain:

Mengubah format gambar menjadi image_tensor.
Mengubah ukuran gambar.
Melakukan normalisasi.

def images_preprocessing(filename):
   
    image = tf.io.decode_image(open(filename, 'rb').read(), channels=3)
    image = tf.image.resize(image, [150, 150])
    image = image/255.
   
    image_tensor = tf.expand_dims(image, 0)
    image_tensor = image_tensor.numpy().tolist()
   
    return image_tensor
 
filename = 'images\Batu_b.jpg'
image_tensor = images_preprocessing(filename=filename)


Selanjutnya, kita akan mengubah hasil preprocessing ke bentuk JSON. JSON ini digunakan sebagai masukan ketika membuat request prediksi.

json_data = {
    "instances": image_tensor
}

Sebelum meminta request prediksi, definisikan terlebih dahulu API endpoint yang ingin digunakan. Selanjutnya buatlah sebuah request untuk melakukan prediksi menggunakan endpoint tersebut. 

Request prediksi akan menghasilkan output berupa nilai probabilitas sebuah gambar yang diklasifikasikan ke dalam kategori atau label tertentu. Sehingga, kita perlu mengambil indeks label dengan nilai probabilitas paling tinggi menggunakan fungsi tf.argmax. Indeks ini selanjutnya akan dipetakan ke dalam kategori yang sesuai.

endpoint = "http://localhost:8501/v1/models/rps_model:predict"
 
response = requests.post(endpoint, json=json_data)
 
prediction = tf.argmax(response.json()['predictions'][0]).numpy()
 
map_labels = {0: "Paper", 1: "Rock", 2: "Scissors"}
print(map_labels[prediction])

TensorFlow Serving, yang memungkinkan model machine learning disajikan sebagai layanan API.





Federated Learning:
Sistem pembelajaran mesin tradisional menyimpan data dan model secara terpusat (centralized) yaitu pada server. Clients menghubungi server untuk mendapatkan hasil prediksi. Namun terdapat beberapa kelemahan dari mekanisme ini yang memperburuk user experience seperti latensi, keterbatasan koneksi (jika device tidak tersambung ke internet maka tidak dapat mengakses model), keterbatasan sumber daya seperti menguras baterai pengguna karena harus terus menerus terkoneksi, kemudian permasalahan privasi.

Untuk mengatasi user experience yang buruk ini, bisakah kita melakukan pembelajaran tanpa mengumpulkan data pada pusat? Tentu saja bisa. Salah satu ide penerapannya adalah dengan melakukan pelatihan pada perangkat client masing-masing. Jadi di sini kita memiliki client yang dapat secara mandiri melakukan training pada modelnya sendiri, dengan menggunakan data yang ada pada data lokal mereka. Tapi biasanya dalam satu client, data yang dimiliki terlalu sedikit sehingga model yang dihasilkan tidak terlalu bagus. Jadi walaupun ada jutaan client, tapi satu client dengan yang lainnya tidak dapat teragregasi dan berkontribusi dalam pembentukan model yang bagus sehingga pendekatan ini pun memiliki keterbatasan.

Federated learning merupakan konsep sistem pembelajaran mesin modern yang mengutamakan privasi serta kecerdasan sistem. Sampai saat tulisan ini dibuat, federated learning masih berada dalam area riset aktif yang masih dapat berkembang namun sudah berhasil untuk mengatasi permasalahan di atas. Federated learning memungkinkan pengembang bereksperimen dengan data-data yang tidak terpusat (decentralized) atau dengan kata lain terdistribusi pada masing-masing client. Data yang terdistribusi ini menarik karena data ini bersifat pribadi dan ada di mana saja. Milyaran perangkat pribadi seperti telepon genggam dan peranti IoT senantiasa menghasilkan data-data baru yang bisa dilatih untuk menciptakan kemudahan bagi para penggunanya. 

Pada federated learning, beberapa perangkat akan menerima training model yang dapat dilatih ulang (retrain) pada perangkat secara lokal, dan menggunakan data lokal client. Hasil latihannya diagregasikan dan dikumpulkan lagi ke server. Server lalu menggunakan model ini untuk melatih ulang model master. Setelah latihan selesai, model training dihapus dari peranti seluler dan dari server. Yang tersisa adalah model baru saja. 

Jadi, model yang didapatkan server ini sebenarnya adalah refleksi pelatihan data dari beragam client tanpa harus melalui akses pada data pribadi client karena yang dikembalikan pada server merupakan sebuah model yang pada praktiknya hanyalah sebuah nilai gabungan untuk memperbarui suatu parameter pada model master. Federated learning dapat menyajikan model yang terpersonalisasi tanpa user perlu kuatir tentang keamanan privasi datanya.

Dengan menjalankan proses ini berulang-ulang, seiring waktu kita dapat meningkatkan model secara signifikan berkat data dari setiap client. Tak perlu mengirimkan data pribadi client ke server. Namun, data kolektif yang telah didapatkan server dapat di-broadcast ke seluruh client sehingga setiap client dapat merasakan keuntungannya. Di sini data tiap client memberi dampak pada user lain sementara data yang digunakan bisa tetap anonim. Data yang dikirimkan ke server tidak bisa dibongkar menggunakan mekanisme reverse engineering karena pengamanan sistem yang diterapkan.

Selanjutnya kita akan mempelajari bagaimana teknologi dirancang untuk mencegah ekstraksi data pengguna. Teknologi keamanan yang diterapkan pada federated learning terdiri dari beberapa lapis tingkat keamanan seperti:
1. On Device Dataset
Setiap peranti menyimpan data secara lokal dan mengenkripsinya ketika sedang tidak digunakan. Data lama yang sudah tak terpakai dihapus.

2. Federated Aggregation
Mengombinasikan laporan dari banyak perangkat. Terdiri dari only-in-aggregate dan ephemeral report. Only-in-aggregate, data yang tersedia di server sudah berupa nilai jumlah atau nilai rata-rata tanpa adanya akses ke detail data dari setiap peranti. Hanya data rerata saja yang dapat diakses oleh teknisi. Ephemeral report memungkinkan data yang teragregasi hanya tersedia untuk sementara, dan segera dibuang jika sudah tak digunakan.

3. Focused Collection
Peranti hanya melaporkan nilai yang dibutuhkan untuk komputasi.

4. Secure Aggregation
Pada federated learning standar, server dapat menghitung nilai jumlah (sum) dari hasil agregasi berbagai perangkat. Namun bisakah server menghitung sum, tanpa dapat melakukan dekripsi tiap data yang masuk? Nah, secure aggregation ini salah satu riset yang masih dalam pengembangan dan bisa menjadi opsi yang dapat diaplikasikan. Secure aggregation atau keamanan agregasi menambahkan nilai mask pada vektor nilai mereka sebelum pelaporan ke server. Semua perangkat yang berpartisipasi dalam proses komputasi ini dipasangkan dan saling menghapuskan jumlah data mask antara satu sama lain. Jadi ketika nilai sepasang peranti digabungkan, nilai sum akan menjadi 0 (zero sum pairs) sehingga yang dikembalikan ke server adalah nilai yang dibutuhkan saja. Tautan penelitian tentang secure aggregation ada di sini.

5. Federated Model Averaging
Dari sisi model, beberapa langkah gradient descent diterapkan pada setiap peranti. Federated averaging bekerja dengan menghitung rata-rata weight data dari update model.

6. Differentially Private Model Averaging
Konsep ini merupakan penurunan dari konsep differential privacy pada ilmu statistik untuk mempelajari pola umum pada dataset tanpa menghafal data individual. Ide utamanya adalah Ketika kita training model dengan federated training di pusat data, kita akan menggunakan noise yang sudah dikalibrasi dengan tepat untuk mengaburkan dampak data individu pada model baru. Di setiap proses pelatihan ulang, perangkat akan memotong nilai update yang dikirimkan ke server menjadi ukuran maksimal yang diperbolehkan. Kemudian server menambahkan noise ketika mengkombinasikan hasil update model tadi. Dengan dua properti ini dikombinasikan dan diatur dengan baik, setiap ada aspek khusus dari model yang berubah pada satu putaran proses mungkin terjadi dikarenakan beberapa kontribusi user yang memang menyarankan perubahan pada model tersebut, atau mungkin karena random noise yang ada. Dengan mekanisme ini, gagasan intuitif tentang penyangkalan apakah suatu perubahan terjadi karena user atau karena noise mencapai suatu definisi formal.

Contoh pengaplikasian federated learning ini ada pada Gboard Query Suggestion. Walaupun belum dapat diterapkan pada level produksi, kita dapat bereksperimen dengan federated learning menggunakan API TensorFlow Federated (TFF) dalam bentuk simulasi lokal. TFF menawarkan 2 API yaitu Federated Learning (FL) API dan Federated Core (FC) API. FL memungkinkan implementasi training atau evaluasi yang bisa diaplikasikan ke model data yang sudah ada. Sedangkan FC bekerja dengan pengaplikasian algoritma Federated. Untuk bereksperimen dengan Colab TFF, kita dapat mengakses link ini. Dan selengkapnya tentang federated learning dapat diakses pada publikasi ini.

https://research.google/pubs/towards-federated-learning-at-scale-system-design/



Data Pipelines dengan TensorFlow Data Services:
Apa itu data pipelines? Data pipelines merupakan arus data yang otomatis dan lancar dari satu tempat ke tempat berikutnya.

Data pipelines memiliki jalur yang berbeda-beda tergantung dari alur yang kita definisikan sebelumnya, bisa ke Cloud, server lokal, data warehouse, dan lain-lain. Mengapa data pipelines penting? Sebabnya, membuat model Machine Learning yang bagus membutuhkan banyak data. Arus dengan data yang banyak dapat menjadi berbahaya karena banyak hal yang bisa menjadi masalah pada saat proses perpindahan data dari satu ke tempat yang lain. Data bisa menjadi corrupt, bottlenecks (menyebabkan latency tinggi), atau sumber data mengalami konflik dan menghasilkan data yang duplikat.

Pada saat membuat model, seringkali kita menulis banyak kode untuk mendapatkan, memotong, dan mengatur data agar dapat dimasukkan menjadi data pelatihan, juga menulis kode untuk proses pelatihan tersebut. Selain itu, masalah terbesar saat pengolahan data adalah sumber data dapat berupa sekumpulan data dengan format berbeda-beda yang tidak dapat langsung digunakan tanpa banyak proses kode. 

Setiap dataset memiliki aturan yang perlu diikuti untuk mempersiapkan data tersebut sebelum digunakan. Kita juga perlu mengubah data menjadi format yang mudah digunakan oleh model karena aturan yang digunakan untuk setiap dataset dapat berbeda, walaupun dataset-nya memiliki jenis yang sama (misalnya image dengan image). Juga, ketika kita ingin menggunakan sebuah dataset, kita tidak yakin dataset tersebut berisi data seperti apa dan jumlahnya berapa sebelum kita mengunduhnya. 

TensorFlow Data Services (TFDS) membantu mengatur data pada machine learning menjadi lebih mudah. TFDS mengubah banyak baris kode tersebut menjadi sangat sedikit. Format TFDS juga dapat menyelesaikan masalah-masalah aturan dataset sebelumnya dengan menggunakan platform yang dapat diakses dengan API yang konsisten. Tujuannya adalah mempermudah dan meminimalisir perbedaan pada saat mengakses data.

Selain itu, ada beberapa keuntungan jika menggunakan TFDS sebagai input data pipelines:
- Cepat: dapat berjalan bersamaan dengan GPU/TPU
- Fleksibel: dapat meng-handle berbagai macam jenis data dan contoh kasus
- Mudah digunakan: dapat digunakan untuk mendemokrasikan Machine Learning

TFDS juga memiliki platform yang memungkinkan orang-orang untuk menerbitkan dataset mereka agar dapat dibagikan ke seluruh dunia. TFDS dibuat untuk mengintegrasikan dataset secara lancar dengan TensorFlow training pipeline. Pipeline ini dirancang agar kita tidak membutuhkan ketergantungan tambahan lagi. Dengan menggunakan TFDS, kita dapat lebih fokus pada riset dan optimisasi. Beberapa contoh TFDS  yang terkenal adalah: MNIST, IRIS, Groove, Moving MNIST, WMT, dan masih banyak lagi.

Data pipeline pada TensorFlow bekerja dengan prinsip Extract, Transform, Load (ETL).
1. Extract: Mengambil data dari sumber yang berbeda. TFDS dapat mengambil data dari ukuran yang sangat kecil hingga sangat besar.

#Proses Extract
dataset = tfds.load(name = "mnist", split = "train")

2. Transform: Mentransformasi, mengambil fitur, augmentasi, dan mengkonversikan data menjadi format yang cocok dengan TFDS agar dapat digunakan untuk proses training.

#Proses Transform
dataset = dataset.shuffle(1000)
dataset = dataset.repeat(10)
dataset = dataset.batch(BATCH_SIZE)

3. Load: Memasukkan data yang telah ditransformasikan ke dalam GPU/TPU

#Proses Load
iterator = dataset.make_one_shot_iterator()
features = iterator.get_next()


Export Data ke Training Pipelines:
Nah, di modul ini, kita akan belajar bagaimana caranya mengubah sebuah dataset (berupa CSV atau Pandas DataFrame) menjadi format TFDS dengan cara yang efisien.

Dataset yang digunakan berasal dari kaggle dan dapat diunduh di sini. Setiap baris dalam dataset ini mendeskripsikan berlian dengan detail spesifikasi pada setiap kolomnya. Kita akan menggunakan dataset ini untuk memprediksi harga berlian. 

1. Pertama Import Tensorflow dan library lainnya yang dibutuhkan:

import pandas as pd
import numpy as np
 
import tensorflow as tf
 
from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

2. Selanjutnya gunakan Pandas DataFrame untuk membaca file CSV

url= 'https://raw.githubusercontent.com/natashayulian/diamond_dataset/master/diamonds.csv'
df = pd.read_csv(url)

3. Setelah data telah dalam format dataframe, Split dataset menjadi training, test, dan validation. Selain itu, kita juga perlu mendefinisikan kolom target.

train, test = train_test_split(df, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)
print(len(train), 'train examples')
print(len(val), 'validation examples')
print(len(test), 'test examples')

df['target'] = np.where(df['price'] <= 1000, 0, 1)
# Drop un-used columns.
df = df.drop(columns=['price'])

4. Kemudian bungkus dataframe dengan tf.data agar memungkinkan untuk menggunakan feature column sebagai jembatan dari dataframe menjadi fitur yang digunakan untuk train model. Jika file CSV nya sangat besar, kita harus menggunakan tf.data untuk membaca file tersebut dari harddisk.

# Cara untuk membuat dataset tf.data dari pandas dataframe
def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = df.copy()
  labels = dataframe.pop('target')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)
  return ds
batch_size = 10 #bath ukuran kecil untuk demonstrasi
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

Ada beberapa jenis dari feature column yang dapat menjadi masukan TensorFlow:
- Numeric column: Numeric column merupakan jenis feature column yang paling sederhana dan digunakan untuk merepresentasikan sebuah fitur dengan value yang apa adanya. Masukkan ke TensorFlow harus berupa angka dan numeric column sudah merupakan angka sehingga tidak perlu diubah lagi.

example_batch = next(iter(train_ds))[0]
def demo(feature_column):
  feature_layer = layers.DenseFeatures(feature_column)
  print(feature_layer(example_batch).numpy())

carat = feature_column.numeric_column('carat')
demo(carat)

- Bucketized column: Terkadang kita memiliki dataset numerik yang memiliki value beragam dengan range cukup jauh. Daripada memasukkan setiap value ke dalam numeric column, kita dapat menggunakan bucketized column untuk membagi value-value tersebut.

#bucketized column
carat = feature_column.numeric_column('carat')
carat_buckets = feature_column.bucketized_column(carat, boundaries=[1, 2])
demo(carat_buckets)

- Categorical column: Kita tidak dapat memasukkan value String ke dalam TensorFlow. Oleh karena itu, categorical column digunakan untuk mengubah value String pada dataset (pada dataset ini contohnya cut dan clarity) menjadi angka.

#categorical
color_type = feature_column.categorical_column_with_vocabulary_list(
      'color', ['E', 'I','J','D','H', 'G','F'])
 
color_type_one_hot = feature_column.indicator_column(color_type)
demo(color_type_one_hot)

- Embedding column: Jika kita memiliki banyak value dalam satu categorical column, lalu jenis value tersebut dapat bertambah seiring dengan berjalannya waktu, data tersebut menjadi tidak cocok jika hanya direpresentasikan dengan nilai 0 atau 1 seperti categorical column. Embedding column merepresentasikan satu categorical column dengan nilai yang beragam.

Key point: hasil embedding column menjadi maksimal jika sebuah categorical column memiliki banyak jenis value.

#embedding
clarity = feature_column.categorical_column_with_vocabulary_list(
      'clarity', df.clarity.unique())
clarity_embedding = feature_column.embedding_column(clarity, dimension=6)
demo(clarity_embedding)

- Hashed feature column: Hashed feature column merupakan cara alternatif untuk merepresentasikan categorical column yang memiliki banyak jenis value. Kita dapat menentukan jumlah hash_buckets jauh lebih sedikit dari jumlah kategori yang sebenarnya untuk menghemat tempat.

Key point: kerugian dari teknik ini adalah dapat terjadi collision yang mana kategori berbeda dipetakan pada bucket yang sama.

#hashed feature
clarity_hashed = feature_column.categorical_column_with_hash_bucket(
      'clarity', hash_bucket_size=5)
demo(feature_column.indicator_column(clarity_hashed))

- Crossed feature column: Crossed feature column menggabungkan banyak feature column menjadi satu feature column. Crossed feature column membuat column feature baru yang memungkinkan model untuk mempelajari weight berbeda untuk setiap kombinasi dari column feature.

Catatan: crossed feature column tidak membuat table dari seluruh kemungkinan kombinasi feature column karena ukurannya bisa menjadi sangat besar. Sebagai gantinya, crossed feature column didukung oleh hashed feature column sehingga dapat mendefinisikan ukuran table tersebut.

#cross feature
#data yang di cross harus berupa string, categorical, atau bucketized
crossed_feature = feature_column.crossed_column([carat_buckets, color_type],
                                                hash_bucket_size=10)
demo(feature_column.indicator_column(crossed_feature))


5. Kemudian buat feature layer sebagai input ke dalam model tf.Keras.
#Pilih feature column mana yang akan digunakan
feature_columns = []
# numeric column
for header in ['carat', 'depth', 'x', 'y', 'z']:
  feature_columns.append(feature_column.numeric_column(header))
 
#membuat feature layer
feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
batch_size = 32
train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

6. Terakhir tulislah kode untuk create, compile, dan train model.

#create, compile, and train the model
model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(128, activation='relu'),
  layers.Dense(128, activation='relu'),
  layers.Dropout(.1),
  layers.Dense(1)
])
 
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
 
model.fit(train_ds,
          validation_data=val_ds,
          epochs=10)

Selamat, Anda telah berhasil mengubah format dataset yang bukan TFDS dan menjadi TFDS. Dengan cara ini. Anda bisa menggunakan dataset apapun sebagai masukan TensorFlow.


Memplubikasi Model ke TF-Hub:
TensorHub (TensorFlow Hub) adalah tempat mencari, mempublikasikan, dan menggunakan reusable machine learning model yang sudah ada. Model yang ada di TensorHub berisi weight dan aset-aset yang dapat digunakan untuk pekerjaan lainnya yang dikenal sebagai transfer learning. Dengan transfer learning, kita bisa melatih model menggunakan dataset yang lebih kecil, mempercepat proses training, dan meningkatkan generalisasi. Sebelum mempublikasikan model Anda, ada syarat-syarat yang harus diketahui terlebih dahulu. Jadi, bagaimana caranya? Yuk, ikuti langkah-langkah berikut ini:

1. Membuat dan Mengekspor Model
Model yang sudah dilatih disimpan dalam format .pb. Setelah menyimpan model dalam format .pb, Anda akan mendapati direktori penyimpanan model tersebut seperti ini

Dalam folder assets, masukkan file .txt yang berisikan label dari model secara terurut sesuai dengan proses pelatihan. Pada saat ingin mempublikasikan model yang sudah jadi, kita harus mengecek apakah di TensorHub sudah ada model yang sama atau mirip dengan model yang akan kita publikasi? Jika ada, coba implementasikan interface yang sama agar konsumen dapat mencoba model-model berbeda dengan mengganti nama model. Interface umum yang sering digunakan pada model dideskripsikan lebih lanjut di sini.

2. Packaging Model 
Repositori tfhub.dev menyajikan SavedModels yang sudah dikompres untuk menghemat network traffic, juga mendukung me-load model yang sudah dikompres dengan ketentuan:

Model dikompres dalam format .tar.gz
Menyimpan file model (seperti saved_model.pb) dan direktori lainnya pada root dari archive tersebut.
Berikut adalah kode untuk mengkompres file dalam format .tar.gz:

import os
import tarfile
def tardir(path, tar_name):
    with tarfile.open(tar_name, "w:gz") as tar_handle:
        for root, dirs, files in os.walk(path):
            for file in files:
                tar_handle.add(os.path.join(root, file))
tardir('root/my_model.pb', 'my_model.tar.gz')

3. Menulis Dokumentasi 
Dokumentasi berupa file markdown yang ditambahkan add-on sintaks kode. File markdown harus dibuat dalam format .md. Dokumentasi ini merupakan hal yang penting karena digunakan sebagai panduan konsumen pada saat ingin menggunakan model yang kita buat. Struktur file markdown pada saat ini adalah sebagai berikut:

Baris pertama dalam bentuk #Module publisher/model-name/version
Baris kedua atau lebih berisi deskripsi model.
Setelahnya berisi metadata yang dikodekan sebagai pasangan key-value dalam komen HTML. Berikut adalah metadata yang harus ada:
asset-path : link file .tar.gz disimpan
module-type : masalah utama dari model ini dengan format image-, text-, audio-, video-. Pada umumnya kata kedua apapun ok, tetapi ada beberapa contoh yang baik seperti: image-classification, audio-pitch-extraction, image-augmentation, dan seterusnya yang bisa dilihat di sini.
fine-tunable : apakah model ini bisa di-tune? (true/false)
Format : format dari model tersebut [saved_model_2|saved_model|hub_module]  untuk informasi selengkapnya dapat dilihat di sini
Markdown apapun yang dapat menjelaskan model lebih lanjut.

4. Membuat Permintaan Publikasi
Setelah file markdown sudah dapat diidentifikasi, file model dapat di-push ke tensorflow/hub dengan 2 cara:
- Git CLI
Asumsikan path dari file markdown adalah: tfhub_dev/assets/publisher/model/1.md.

Lalu lakukan command git seperti menambahkan file baru ke branch master dari repositori yang bercabang.

git clone https://github.com/[github_username]/hub.git
cd hub
mkdir -p tfhub_dev/assets/publisher/model
cp my_markdown_file.md ./tfhub_dev/assets/publisher/model/1.md
git add *
git commit -m "Added model file."
git push origin master
- Github GUI

Menggunakan GitHub GUI merupakan cara yang lebih mudah. GitHub memperbolehkan membuat PRs untuk membuat file baru atau edit file secara langsung melalui GUI. Berikut adalah langkah-langkahnya:

Pada TensorFlow Hub GitHub page, tekan tombol Create new file.
Set path yang benar hub/tfhub_dev/assets/publisher/model/1.md
Copy dan paste markdown yang sudah ada
Pada bagian bawah, pilih “Create a new branch for this commit and start a pull request.



Pengenalan TensorBoard:
Pada pembelajaran mesin seringkali banyak parameter dan struktur terkait model yang ingin Anda visualisasikan perubahannya. TensorBoard merupakan aplikasi web dan alat yang tepat untuk melakukan tugas visualisasi data dan parameter beserta perilakunya. TensorBoard dapat menampilkan visualisasi metrik loss dan akurasi; visualisasi grafik model; menampilkan histogram bobot, bias, atau tensor; memproyeksikan embedding ke dimensi yang lebih kecil; menampilkan gambar, teks, data audio; membuat profil program tensorflow; dan lain-lain.

TensorBoard dapat diakses baik pada sistem lokal maupun suatu lingkungan hosting. Tensorboard.dev memungkinkan kita membagikan eksperimen kita pada website TensorBoard. Gunanya untuk memudahkan kolaborasi pekerjaan tim, troubleshooting, publikasi, dan lain-lain.

Berikut merupakan beberapa elemen fungsional yang ditampilkan pada dashboard TensorBoard:
- Scalars - Menampilkan bagaimana loss dan setiap metrik berubah pada setiap epoch. Kita juga dapat melacak kecepatan training, learning rate, dan nilai skalar lainnya. Membandingkan dan melacak nilai metrik dapat membantu kita menginvestigasi adanya overfitting, mengevaluasi waktu training, dll. Arahkan kursor ke grafik untuk melihat titik data tertentu. Anda juga dapat mencoba memperbesar dengan mouse atau memilih suatu bagian untuk melihat lebih detail.

- Graphs - Membantu Anda memvisualisasikan model dan menampilkan detail layer pada grafik Keras. Jaringan digambarkan terbalik dari bawah ke atas di mana layer dense untuk klasifikasi berada paling atas. Tab Graphs dapat membantu Anda memastikan bahwa model yang Anda buat telah tersusun dengan benar.

- Distributions dan Histograms - Anda dapat melihat perkembangan learning dan mengecek apakah parameter seperti bias, bobot (weight), dan distribusi tensor berubah sesuai yang diharapkan. Misalnya dense_1 adalah lapisan yang paling dekat dengan klasifikasi. Seiring waktu Anda dapat melihat bias menyebar seperti yang Anda harapkan sehingga mereka dapat menunjukkan dampak yang lebih besar pada layer terakhir tempat klasifikasi terjadi.


1. Setup TensorBoard:
Untuk memulai TensorBoard, jalankan ekstensi pada Notebook/Colab dengan kode:

%load_ext tensorboard

2. Membuat File Log
Tensorboad bekerja dengan log (event file) untuk mengakses data dan detail training. 

Sebelum menjalankan TensorBoard pastikan Anda mendeklarasikan direktori disimpannya file log (logdir). Untuk membuat nama direktori yang unik, gunakanlah tanggal dan waktu running sebagai nama direktori:

import datetime
import tensorflow as tf
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

Setelah direktori log dibuat. Buat file log (event files) dengan cara:
- Buat data log dengan tf.summary(). tf.summary() memungkinkan kita membuat data ringkasan yang dapat divisualisasikan termasuk data audio, skalar, gambar, dll. Kode di bawah ini akan membuat log berisi definisi grafik pada suatu session: 
!rm -rf ./logs/
tf.summary.create_file_writer("./logs/")

Mari kita lihat beragam contoh penggunaan tf.summary() pada modul ini
- Sisipkan callbacks tf.keras.callbacks.TensorBoard() pada kode model.fit(). Ketika training, kode ini nantinya menghasilkan log training pada sebuah folder:

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
kemudian panggil callbacks:
mnist = tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
def create_model():
  return tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
  ])
model = create_model()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs = 10, 
    validation_data=(x_test, y_test), 
    callbacks=[tensorboard_callback]) 

3. TensorBoard Lokal 
Jalankan kode di bawah untuk menampilkan TensorBoard in-line pada Colab:

%tensorboard --logdir logs/fit


4. Tensorboard.dev
 Belum semua layanan sudah tersedia di website Tensorboard.dev, namun TensorBoard.dev memungkinkan Anda menyebarluaskan pekerjaan kita pada siapa pun melalui TensorBoard.dev. Kode di bawah ini akan memancing uploader untuk mengunggah log atas persetujuan Anda. 

!tensorboard dev upload --logdir logs/fit

Sebuah tautan akan muncul, klik dan Anda akan dimintai kredensial untuk log in dengan akun Gmail. Setelah otorisasi berhasil, salin dan tempel (copy and paste) link ke kotak yang tersedia. Selanjutnya link eksperimen sudah dapat Anda akses dan bagikan. Yeay!

5. Menampilkan Gambar
 TensorBoard dapat memvisualisasi data gambar dari dataset tanpa harus mengimport library khusus untuk menampilkan data. Seringkali kita ingin memastikan bahwa gambar yang kita impor sudah dimuat dengan benar. Di bawah ini kode untuk menampilkan gambar pertama tulisan tangan MNIST yang terdiri dari 28 x 28 piksel. Kita menggunakan tf.summary() File Writer untuk melakukan logging.

import numpy as np
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
 
file_writer = tf.summary.create_file_writer(log_dir)
with file_writer.as_default():
    img = np.reshape(x_train[0], (-1, 28, 28, 1))
    tf.summary.image('Training Data ', img, step = 0)
%tensorboard --logdir logs/fit

Untuk menampilkan banyak gambar sekaligus, gunakan kode di bawah:

import numpy as np
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
 
file_writer = tf.summary.create_file_writer(log_dir)
with file_writer.as_default():
  imgs = np.reshape(x_train[0:4], (-1, 28, 28, 1))
  tf.summary.image('4 Training Data ', imgs, max_outputs = 4, step = 0)
 
%tensorboard --logdir logs/fit

Bagaimana jika Anda ingin menampilkan gambar yang didapat dari sebuah plot? Format Matplotlib tidak bisa dibuat dalam bentuk log gambar. Maka Anda membutuhkan sebuah fungsi tambahan untuk mengubah plot ke bentuk image PNG:

import io
import matplotlib.pyplot as plt
 
def plot_to_image(figure):
  """Ubah the matplotlib plot 'figure' ke PNG image"""
  # Simpan plot ke PNG di memori
  buf = io.BytesIO()
  plt.savefig(buf, format='png')
  # Tutup the figure untuk mencegah figure ditampilkan langsung di Notebook
  plt.close(figure)
  buf.seek(0)
  # Ubh PNG buffer ke TF image
  image = tf.image.decode_png(buf.getvalue(), channels=4)
  # Tambah dimensi
  image = tf.expand_dims(image, 0)

Nah, setelah memiliki fungsi plot_to_image,kita bisa memvisualisasi gambar yang didapat dari plot. Kita akan belajar menampilkan bagian gambar dari dataset dalam bentuk grid, dan confusion matrix yang dihasilkan dari Matplotlib.

Deklarasikan direktori log plot dan buat fungsi untuk menampilkan grid gambar:

!rm -rf logs/plots
import matplotlib.pyplot as plt
import datetime
 
logdir = "logs/plots/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
file_writer = tf.summary.create_file_writer(logdir)
 
class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
 
def image_grid():
  """Return a 5x5 grid of the MNIST images as a matplotlib figure."""
  # Create a figure to contain the plot.
  figure = plt.figure(figsize=(10,10))
  for i in range(25):
    # Start next subplot.
    plt.subplot(5, 5, i + 1, title=class_names[y_train[i]])
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i], cmap=plt.cm.binary)
  
  return figure
 
# Prepare the plot
figure = image_grid()
# Convert to image and log
with file_writer.as_default():
  tf.summary.image("Training data", plot_to_image(figure), step=0)
 
%tensorboard --logdir logs/plots


6. Confusion Matrix
TensorBoard memungkinkan Anda menampilkan confusion matrix pada setiap epoch dengan memanggilnya pada callbacks. Confusion matrix sangat berguna untuk melacak performa model pengklasifikasi Anda. Lihat perkembangan confusion matrix seiring proses pembelajaran dari waktu ke waktu. Ini dapat membantu proses debugging.  Yang dibutuhkan untuk menampilkan confusion matrix ada 3 properti yaitu:
- Pembuatan plot confusion matrix
def plot_confusion_matrix(cm, class_names):
  """ Mengembalikan matplotlib figure yang berisi the plot confusion matrix  """
  figure = plt.figure(figsize=(8, 8))
  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
  plt.title("Confusion matrix")
  plt.colorbar()
  tick_marks = np.arange(len(class_names))
  plt.xticks(tick_marks, class_names, rotation=45)
  plt.yticks(tick_marks, class_names)
  # Normalisasi confusion matrix.
  cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)
  # Setting teks
  threshold = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    color = "white" if cm[i, j] > threshold else "black"
    plt.text(j, i, cm[i, j], horizontalalignment="center", color=color)
 
 
  plt.tight_layout()
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  return figure

- Log matrix yang dibuat pada setiap epoch untuk men-generate sebuah confusion matrix, kemudian mengubahnya menjadi PNG pada fungsi plot_to_image.
import sklearn.metrics
import itertools
 
def log_confusion_matrix(epoch, logs):
  # Gunakan model untuk memprediksi nilai dari data validasi
  test_pred_raw = model.predict(x_test)
  test_pred = np.argmax(test_pred_raw, axis=1)
 
  # Hitung confusion matrix.
  cm = sklearn.metrics.confusion_matrix(y_test, test_pred)
  
  figure = plot_confusion_matrix(cm, class_names=class_names)
  cm_image = plot_to_image(figure)
 
  # Log confusion matrix sebagai image summary.
  with file_writer_cm.as_default():
    tf.summary.image("Confusion Matrix", cm_image, step=epoch)
 
# Definisikan epoch setiap callback
cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)

- Pemanggilan multiple callbacks. Callback pertama digunakan untuk menyimpan log skalar, log lainnya digunakan untuk plot confusion matrix. Callback kedua yaitu LambdaCallback digunakan untuk mengeksekusi kode bebas. Ia dijalankan pada akhir epoch. Pada latihan ini ia memanggil fungsi log_confusion_matrix:

import datetime
 
logdir = "logs/image/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
# Definisikan callback.
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')
 
 
# Train the classifier.
model.fit(
    x_train, y_train, epochs=5,
    callbacks=[tensorboard_callback, cm_callback],
    validation_data=(x_test, y_test),
)
%tensorboard --logdir logs/image



Memonitor Model:
Model monitoring atau pemantauan model pada ML adalah suatu proses pengamatan kinerja model pada fase produksi dilihat dari perspektif operasional dan teknis. Sistem ML tidaklah sama pada sistem biasa pada umumnya. Ini disebabkan dependensi sistem ML bukan hanya pada kode, melainkan juga  pada perilaku model yang bergantung sepenuhnya pada data yang dipelajari. Tanpa memahami perilaku model dan data yang dipelajari, kita tidak akan mengerti operasional sistem yang dibuat. Oleh karena itu tanggung jawab pemantauan model pada sistem ML tidak dapat secara otomatis diserahkan ke tim DevOps. Validasi penjaminan operasional yang tepat dari sistem ML ini tidak terlepas dari keterlibatan langsung antara tim DevOps dan data scientist. 

Pada fase produksi, tim data scientist akan menangani pemantauan akurasi model; memastikan input berada pada nilai yang sesuai sehingga tidak ada missing value; mengecek fitur numerik maupun kategori, memilih algoritma yang diterapkan pada model; menguji langkah training per detik antara model baru dan membandingkan terhadap versi sebelumnya (dengan menetapkan suatu nilai ambang batas). Tim DevOps akan memantau kinerja sistem, latensi respon API terhadap waktu, melacak penggunaan memori/CPU dengan menentukan nilai ambang batas penggunaan, serta menghitung jumlah query yang bisa diproses oleh model per detik. Sedangkan tim engineer akan mencatat versioning yang berisi versi setiap model, kode, dan data, logging yaitu log berisi detail skenario pembangunan sistem dari waktu ke waktu, provenance, dashboard, dan report.

Anggaplah monitoring pada model ini seperti melakukan check-up tahunan secara berkala. Pemantauan model ini penting untuk Anda lakukan karena semakin lama kinerja model akan menurun seiring waktu. Istilah ini dikenal dengan model drift. Ada 2 tipe penurunan kualitas pada model yaitu sudden degradation dan slow degradation. Pada sudden degradation biasanya ditemukan bug pada model terbaru yang menyebabkan penurunan akurasi yang signifikan. Cek kualitas model terbaru dengan membandingkan akurasinya terhadap model sebelumnya. Sedang pada slow degradation, penurunan kualitas dalam beberapa versi model yang telah diperbarui, tidak terdeteksi. Pada kasus ini pastikan akurasi model Anda pada data validasi memenuhi ambang batas nilai yang ditentukan (threshold).

Anggaplah monitoring pada model ini seperti melakukan check-up tahunan secara berkala. Pemantauan model ini penting untuk Anda lakukan karena semakin lama kinerja model akan menurun seiring waktu. Istilah ini dikenal dengan model drift. Ada 2 tipe penurunan kualitas pada model yaitu sudden degradation dan slow degradation. Pada sudden degradation biasanya ditemukan bug pada model terbaru yang menyebabkan penurunan akurasi yang signifikan. Cek kualitas model terbaru dengan membandingkan akurasinya terhadap model sebelumnya. Sedang pada slow degradation, penurunan kualitas dalam beberapa versi model yang telah diperbarui, tidak terdeteksi. Pada kasus ini pastikan akurasi model Anda pada data validasi memenuhi ambang batas nilai yang ditentukan (threshold).

Misalnya model yang memprediksi sentimen menggunakan data 12 tahun lalu tentu saja kini sudah tidak relevan. Sebabnya, berkembangnya bahasa dan kalimat yang kita gunakan. Environment juga berpengaruh misalnya pada klasifikasi gambar. Pembacaan piksel gambar pada Python akan berbeda jika pembacaan gambar dilakukan pada browser. Ini semua tergantung pemroses gambarnya. Sedangkan perubahan ekstrim pada data misalnya dengan penghilangan suatu fitur bisa menghasilkan missing value pada data dan mengakibatkan model berkinerja buruk.

Pada model machine learning, kinerja model dapat dipantau menggunakan acuan suatu metrik seperti:
1. Type 1 error
Dikenal sebagai false positive, merupakan nilai ketika model salah memprediksi kelas positif. Misal pada tugas prediksi diabetes. Jika model menyatakan Anda positif diabetes sedangkan sebenarnya Anda tidak menderitanya, merupakan contoh dari type 1 error.

2. Type 2 error
Dikenal sebagai false negative yaitu nilai ketika model salah memprediksi kelas negatif. Contohnya ketika hasil menyatakan Anda tidak diabetes namun sebenarnya anda positif diabetes.

3. Akurasi
Proporsi jumlah data yang benar diprediksi. Dinyatakan sebagai jumlah prediksi yang diklasifikasi dengan benar dibagi dengan total jumlah prediksi.

4. Presisi
Proporsi identifikasi kelas positif yang diklasifikasi dengan benar. Didefinisikan dengan:

5. Recall
Proporsi kelas yang seharusnya positif dan diklasifikasi dengan benar.

6. Mean Squared Error (MSE)
MSE biasanya digunakan pada tugas regresi. Dinyatakan sebagai rerata loss pada setiap baris data dari keseluruhan dataset. Loss ini sendiri didefinisikan dengan nilai residual yaitu selisih antara nilai prediksi dengan nilai aktual.

Setelah model divalidasi dan di-deploy, mungkin akan terdapat beberapa skenario dari dunia nyata yang mempengaruhi kualitas model yang digunakan. Namun pengujian kualitas model pada dunia nyata agak sedikit sulit karena biasanya live data tidak tersedia beserta labelnya. Oleh karena itu untuk tetap bisa melacak kualitas model, kita bisa mempertimbangkan untuk menggunakan jasa manusia untuk melabeli live data yang masuk secara manual. Kemudian gunakan metrik yang cocok pada prakteknya untuk mengukur kualitas model. Misalnya kita sedang mengklasifikasi spam data, maka bandingkan prediksi model dengan data spam yang dilaporkan oleh user itu sendiri.

Adapun beberapa prinsip kunci yang dapat diterapkan dalam pemantauan sistem ML yaitu:

- Monitor perubahan dependency (ketergantungan) sebaiknya dibuat dalam bentuk notifikasi.
- Monitor perbedaan data training dan data yang mungkin muncul pada live data. Data training seharusnya merepresentasikan data yang mungkin akan muncul pada live sistem.
- Monitor fitur pada training dan pada penyajian model.
- Monitor perbaruan model. Jangan sampai model tidak up to date.
- Monitor kestabilan model.
- Model seharusnya tidak mengalami kelambatan lantensi pengembalian output, penggunaan RAM, throughput, atau pada kecepatan training.
- Model tidak mengalami penurunan kualitas prediksi terhadap live data.


Memperbarui Model yang telah Di-deploy:
Terdapat beberapa alasan mengapa kita harus melakukan pembaruan pada model. Seperti berubahnya trend dan selera user, atau kinerja model yang semakin lama semakin buruk. Untuk mengantisipasi hal tersebut amati statistik tentang data pada suatu waktu tertentu guna memahami tren atau pola user. Sedangkan untuk mengamati kinerja model, pengembang dapat mengukur kinerja model terhadap suatu metrik secara berkala baik offline maupun online metrik beserta korelasi di antara keduanya.

Jenis-jenis ML model dibedakan menjadi 2 berdasarkan cara dilatihnya; model statis dan model dinamis. Model statis dilatih secara offline dari jumlah data yang sangat banyak. Pelatihan dilakukan hanya sekali lalu model tersebut sudah siap digunakan. Sedangkan model dinamis dilatih secara online. Live data secara kontinu dimasukkan ke dalam model dinamis dan model terus diperbaharui sedikit demi sedikit. Jika data yang kita gunakan tidak cepat berubah oleh waktu, maka model statis lebih cocok diterapkan. Sebabnya, model statis lebih mudah dan murah untuk dibangun dan dipelihara. Model dinamis beradaptasi dengan cepat terhadap data sehingga model ini lebih cocok diterapkan untuk data yang mudah berubah. Contohnya seperti prediksi penjualan tahun ini mungkin akan berbeda dengan tahun depan.

Untuk melakukan training ulang model, terapkan  dua (2) cara ini. Cara pertama adalah training ulang manual (manual retraining) yaitu melakukan training seperti awal pembuatan model. Di sisi lain ketika pengembang melakukan training ulang secara manual biasanya pengembang menemukan algoritma baru atau menemukan sekumpulan set fitur baru yang bisa meningkatkan peningkatan akurasi model. Nah, ada beberapa hal yang penting untuk diketahui pada praktek penggunaan data pipeline agar bisa digunakan kembali (reproducible). Katakanlah kita akan menambahkan fitur baru untuk menguji bagaimana fitur tersebut berpengaruh pada kualitas model kita.

Untuk eksperimen yang baik, dataset harus identik dan sama seperti sebelum ditambahkan fitur baru. Gunakan random seed generator yang sama ketika melakukan pengacakan data. Selain itu lakukan beberapa kali run pada model dan rata-ratakan hasilnya. Gunakan kontrol versi untuk memudahkan investigasi model. Cara kedua adalah continuous learning, yaitu menggunakan sistem otomasi untuk mengevaluasi dan latih ulang model. Langkah pada continuous learning dimulai dengan menyimpan data latih baru misalnya data harga rumah yang paling baru, kemudian simpan pada basis data. Langkah berikutnya yaitu uji akurasi model lama dengan data tersebut. Jika akurasi model menurun, maka gunakan data harga rumah yang paling baru, atau gunakan kombinasi data lama dan baru untuk membuat dan mengembangkan model baru.

Bagaimana caranya kita harus memperbarui model dengan memilih satu di antara beberapa versi model? Mekanisme yang pertama adalah dengan pengujian A/B. Misalnya kita memiliki model versi 1. Apakah model ini lebih baik daripada model versi 2? Mungkin karena model versi 2 ini dibuat oleh engineer senior yang lebih berpengalaman dibanding kita. Untuk menentukannya, kita bisa menerapkan pengujian A/B. Ide dari pengujian A/B ini sederhana. Kita membagi traffic pengunjung menjadi 50 : 50. Jika model versi 1 mendapat 2500 pengunjung, maka model versi 2 juga mendapat 2500 pengunjung. Lalu pada masing-masing kelompok tersedia model yang berbeda. Amati kinerja masing-masing model, ukur menggunakan metrik bisnis contohnya click-through-rate. Kemudian kita hitung jika model versi 1 mendapatkan 10% click-through-rate dari pengunjung, dan model versi 2 mendapatkan 30% click-through-rate, maka yang dipilih menjadi model yang unggul adalah model versi 2. Keuntungan dari pengujian A/B adalah mudah diterapkan, dapat mengoptimalkan bisnis metrik karena kita dapat langsung mengetahui mana yang lebih disukai user.

Cara kedua dalam memilih model adalah adalah Multi Armed Bandit (MAB). MAB adalah tipe pengujian A/B yang menggunakan ML untuk belajar dari data yang dikumpulkan selama pengujian. Konsep MAB ini ada pada traffic yang diubah-ubah secara dinamis. Model yang lebih baik mendapat lebih banyak alokasi traffic. Dengan mengambil kesempatan memindahkan traffic, pengembang bisa mengumpulkan data yang cukup sampai dapat diketahui model mana yang terbaik, kemudian memindahkan alokasi traffic untuk fokus di model yang lebih baik. Keuntungan dari model ini adalah pengembang dapat fokus pada model yang terpilih untuk mengevaluasi ulang kinerja model sehingga bisa dilakukan Tindakan lebih lanjut dan memaksimalkan investasi pada model yang paling unggul saja. MAB menjalankan 2 fase yaitu exploration dan exploitation.

Pengujian A/B pada desainnya berada dalam mode exploration secara terus-menerus sampai ditemukan suatu nilai yang dapat menjadi acuan secara statistik bahwa model tersebut lebih baik daripada model yang lainnya. Pada tahap exploration MAB juga dipilih model mana yang terbaik berdasarkan click-through-rate, namun pada MAB ditambahkan mode exploitation di mana mode tersebut dijalankan secara paralel dengan mode exploration sehingga resource seperti traffic pengunjung secara mayoritas terus-menerus dialokasikan ke model yang memiliki peluang menjadi model yang paling unggul (eksploitasi). 

Saat model yang paling unggul mengumpulkan nilai kinerja yang lebih baik, traffic pengunjung-nya terus meningkat. Traffic akan mencapai titik di mana Sebagian besar pengguna mendapatkan layanan model yang berkinerja lebih unggul. Kebanyakan algoritma MAB fokus kepada bagaimana caranya model belajar lebih cepat dari live data yang masuk. Kelebihan dari MAB yaitu secara bertahap aplikasi akan meluncurkan model versi terbaik tanpa harus menunggu hasil pengujian mencapai nilai statistik yang signifikan. Optimasi lebih maksimal dan dapat dilakukan secara kontinu karena pengembang dapat fokus ke satu model saja.

Penting untuk Anda pahami bahwa pengujian A/B dan MAB digunakan pada kasus yang berbeda. Pengujian A/B dilakukan untuk mengumpulkan data dengan menggunakan acuan nilai statistik tertentu dan ditujukan untuk membuat sebuah keputusan bisnis. Pengujian A/B dilakukan untuk mempelajari dampak dari tiap variasi model. Contohnya jika pengembang ingin mengembangkan produk baru yang tidak hanya mengoptimalkan penjualan namun juga mengumpulkan informasi tentang kinerja model/aplikasinya. Jadi nantinya pengembang dapat memasukkan proses pembelajaran untuk pembuatan produk yang lebih baik.

Sedangkan MAB merupakan suatu algoritma yang digunakan untuk fokus memaksimalkan kinerja menggunakan suatu acuan metrik. Contohnya jika pengembang sedang menguji suatu skema yang spesifik, maka skema itu saja yang dimaksimalkan. Tidak ada tahap analisis dan pengumpulan data untuk mengumpulkan suatu nilai statistik yang signifikan. Gunanya untuk mendukung sebuah keputusan bisnis karena MAB menyesuaikan traffic secara otomatis. MAB ini cocok digunakan jika linimasa pengoptimalan model atau aplikasi yang ada sangat singkat.

"Na, gimana kalau kita setup sistem untuk otomatis memperbarui model kita?" kata Ryan sambil memutar kursinya menghadap Diana.

Diana menoleh dan mengangguk. "Bagus tuh, Yan. Kita bisa pakai CI/CD pipeline buat ini. Jadi tiap kali ada update model, kita bisa otomatis build dan deploy."

Ryan mengangguk antusias. "Iya, aku baru baca tentang Jenkins dan GitHub Actions. Kita bisa pakai itu buat ngebangun pipeline otomatis."

Diana membuka laptopnya dan mulai mencari tutorial tentang Jenkins dan GitHub Actions. "Jadi, pipeline ini nantinya bakal ngecek kode kita tiap kali ada perubahan di repositori. Kalau ada update, dia otomatis nge-run tes buat model baru, dan kalau semua tes berhasil, langsung di-deploy

Mereka juga menambahkan langkah-langkah untuk memastikan rollback jika terjadi error sehingga pengguna tidak akan mengalami gangguan.

Dengan semua komponen yang berjalan dengan baik, mereka merasa siap untuk melakukan user testing skala besar. Mereka mengundang teman-teman dan beberapa pengguna awal untuk mencoba aplikasi mereka serta memberikan feedback. Setiap kali ada masukan baru atau bug yang ditemukan, pipeline otomatis mereka memastikan bahwa perbaikan dan pembaruan model bisa dilakukan dengan cepat dan tanpa hambatan.

"Feedback dari user testing ini penting banget buat kita," kata Diana sambil mencatat masukan dari salah satu pengguna. "Ini bakal bantu kita untuk terus memperbaiki dan mengoptimalkan aplikasi kita."

Ryan menambahkan, "Dan dengan sistem otomatis ini, kita bisa lebih fokus ke inovasi dan pengembangan fitur baru, tanpa harus khawatir soal deployment dan update."

---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------

1. Pada modul Latihan Pra-Pemrosesan Teks sepertinya output yang didapat pada bagian Tokenizing khususnya Word tokenization dan Sentence tokenization belum sesuai dengan apa yang ada pada code kak.

2. Juga pada bagian Ekstraksi Fitur pada saat menjelaskan GloVe itu sepertinya belum selesai dijelaskan seluruhnya. Karena pada kalimat terakhir 'GloVe cenderung lebih fokus pada kem??' 

3. Terus kak kenapa pada beberapa materi ada tanda tanya seperti ini ???? (????)

4. Link Dataset Google MoviewLens sepertinya tidak bisa diakses kak pada materi Latihan Sistem Rekomendaasi.

5. Diskonto pada teks yang ada di bagian Policies and Value Functions itu apa ya kak? 



---------------------------------------------------------------------------
                               Generative AI
---------------------------------------------------------------------------
Langkah pertama adalah memahami konsep dasar dari Generative Adversarial Networks (GAN). Mereka kagum dengan cara dua jaringan – generator dan discriminator – saling berkompetisi untuk menghasilkan data yang semakin realistis. 

Namun, rasa penasaran mereka tidak berhenti di situ. Ryan menemukan artikel tentang transformer models, khususnya GPT (Generative Pre-trained Transformer) yang bisa menghasilkan teks dengan kualitas tinggi. Mereka segera mendalami model ini dan mulai bereksperimen.

"Bagaimana jika kita mencoba membuat cerita dengan AI?" usul Diana suatu malam. Mereka pun memasukkan beberapa baris pertama dari cerita mereka ke dalam model, lalu AI melanjutkan cerita itu dengan cara yang menakjubkan. Mereka tertawa dan kagum dengan hasil yang kreatif dan realistis.

Mereka mulai mempelajari multimodal models dan menemukan cara untuk mengintegrasikan GPT dengan GAN. Mereka menciptakan proyek AI yang tidak hanya menulis cerita, tetapi juga menghasilkan ilustrasi sesuai konteksnya. Proyek ini memakan waktu berminggu-minggu, tapi semangat mereka tidak pernah surut.


Pengenalan Generative AI:
Biasanya ada dua pertanyaan umum yang sering dilontarkan, apa itu kecerdasan buatan dan apa perbedaan antara AI dan machine learning.

Jawaban singkatnya adalah AI merupakan sebuah disiplin ilmu. Seperti halnya fisika, AI merupakan cabang dari ilmu komputer yang menangani pembuatan agen kecerdasan, yaitu sistem yang dapat berpikir, belajar, dan bertindak secara mandiri. 

Di lain sisi, machine learning merupakan subbidang dari AI. ML adalah program atau sistem yang melatih sebuah model dari data input dan memungkinkan komputer melakukan pembelajaran tanpa pemrograman eksplisit. Model yang dilatih dapat membuat prediksi yang berguna dari data baru atau data yang belum pernah dilihat sebelumnya dari sistem yang sama yang digunakan untuk melatih model. 

Begitu pun dengan deep learning dan generative AI, keduanya merupakan subbidang dari AI yang saat ini sedang ramai diperbincangkan bahkan oleh sesama software developer. Hal ini karena AI bisa membantu meningkatkan produktivitas pada berbagai macam bidang pekerjaan. Agar lebih jelas mengenai hubungan Artificial Intelligence, Machine Learning, Deep Learning, dan Generative AI, perhatikan taksonomi berikut 

Generative AI adalah sebuah subbidang dari machine learning yang fokus pada kemampuan mesin untuk menghasilkan konten baru yang kreatif dan realistis. Teknologi ini memungkinkan mesin untuk menciptakan teks, gambar, musik, dan bahkan video yang belum pernah ada sebelumnya, seolah-olah dibuat oleh manusia.

Generative AI menggunakan berbagai teknik canggih, termasuk yang telah kita pelajari dalam deep learning, seperti jaringan saraf tiruan, tetapi dengan pendekatan yang sedikit berbeda. Salah satu pendekatan paling populer adalah penggunaan Generative Adversarial Networks (GAN), Variational Autoencoders (VAEs) dan Large Language Models (LLMs). Ketiga pendekatan tersebut akan kita pelajari lebih pada materi ini.

Sebelum masuk ke detail teknis, mari kita lihat beberapa contoh nyata dari aplikasi Generative AI
- Pembuatan Konten Teks: model seperti GPT (Generative Pre-trained Transformer) yang dapat menghasilkan artikel, cerita pendek, dan bahkan kode pemrograman dengan sedikit atau tanpa campur tangan manusia.
- Generasi Gambar dan Video: penggunaan GAN dalam menciptakan gambar realistis dari deskripsi teks atau memperbaiki kualitas video yang buram.
- Musik dan Seni: AI yang mampu menggubah musik baru dalam gaya komposer terkenal atau membuat karya seni yang menginspirasi.


Discriminative Diskriminatif AI vs Generative AI:
Machine learning memiliki posisi sebagai payung besar yang mencakup berbagai metode untuk membuat komputer belajar dari data. Discriminative AI dan deep learning adalah dua pendekatan yang termasuk dalam payung besar ini, sedangkan deep learning adalah bagian dari machine learning yang menggunakan jaringan saraf tiruan yang dalam untuk menganalisis data.

Discriminative AI adalah pendekatan dalam machine learning yang fokus pada klasifikasi dan pemisahan data ke dalam kelas-kelas tertentu. Model-model discriminative dapat diterapkan dalam konteks deep learning dengan menggunakan deep neural networks untuk tugas-tugas klasifikasi yang kompleks.

Secara keseluruhan artificial intelligence adalah bidang yang luas di mana machine learning dan deep learning adalah pendekatan-pendekatan spesifik yang digunakan untuk menyelesaikan berbagai masalah berdasarkan jenis data dan tujuan analisis. Dengan perkembangan teknologi yang sangat cepat, kini hadir sebuah metode yang dapat membuat sebuah konten berdasarkan history yang sudah ada, yaitu Generative AI. 

Generative AI adalah cabang dari kecerdasan buatan (AI) yang berfokus pada pembuatan atau generasi konten baru yang menyerupai data asli yang digunakan untuk melatih model tersebut. Tidak seperti discriminative AI yang berfokus pada pemodelan keputusan untuk mengklasifikasikan data, generative AI mencoba memahami dan meniru distribusi data yang mendasari sehingga bisa menciptakan data baru yang realistis.

Inti dari generative AI adalah kemampuannya untuk memahami dan mereplikasi distribusi data yang kompleks sehingga dapat menggunakan pengetahuan tersebut untuk menciptakan data baru. 

Saat materi ini dibuat, Generative AI merupakan teknologi yang sangat ramai diperbincangkan karena dapat meningkatkan produktivitas mulai dari mencari informasi hingga inspirasi untuk membuat suatu hal. Salah satu contohnya adalah GitHub Copilot yang dapat membantu seorang software developer untuk membangun sebuah sistem mulai dari membuat kerangka kode hingga membantu debugging kode yang sudah dibangun. Wah, sangat menarik, ya? 

Sampai di sini kita sudah dapat membedakan discriminative AI dan generative AI secara umum. Namun, Diana masih memiliki rasa penasaran karena ia belum mengetahui potensi dan penerapan Generative AI di dunia nyata. Untuk menjawab rasa penasarannya, mari kita ikuti perjalanan Diana pada materi berikutnya yang akan membahas ragam penerapan dari Generative AI yang tentunya memiliki potensi besar untuk diterapkan pada sebuah aplikasi. Tetap semangat, ya, agar Anda tidak ketinggalan tentang perkembangan teknologi dan menjadi seorang machine learning engineer yang tak tergantikan oleh AI!

Salah satu kelebihan dari Generative AI ini adalah dapat membuka peluang baru dalam segi kreativitas dan inovasi dengan lebih cepat dan efisien. Hal ini karena pembuatan konten akan lebih cepat dibandingkan tanpa menggunakan Generative AI.

Selain itu, Generative AI juga dapat menghasilkan konten dalam skala besar yang bermanfaat dalam industri seperti media, hiburan, dan desain. Salah satu contoh nyata dari penerapan Generative AI adalah chatbot. Chatbot adalah sebuah program yang dirancang untuk berinteraksi dengan manusia melalui percakapan teks atau suara. Chatbot menggunakan teknologi artificial intelligence untuk memahami dan merespons pertanyaan atau perintah (prompt) dari pengguna.

Anggap saja Anda adalah pemilik sebuah bisnis yang berfokus pada komunikasi cloud. Seorang pengguna mengunjungi situs web Anda dan bertanya kepada chatbot Anda tentang solusi machine learning. Mirip dengan percakapan pengguna dengan staf, chatbot akan menafsirkan kata-kata yang diberikan kepada mereka dan menghasilkan jawaban yang telah ditentukan sebelumnya kepada pengguna.

Selain dapat bekerja tanpa henti, chatbot juga memiliki sifat scalability atau skalabilitas yang berarti ia dapat menyesuaikan kinerjanya dengan request yang masuk. Apa artinya ya? Skalabilitas pada chatbot merujuk pada kemampuan sistem chatbot untuk menangani peningkatan beban kerja dan jumlah pengguna secara efisien tanpa mengurangi kinerja.

Sebagai contoh, mari kita asumsikan Anda memiliki sebuah perusahaan e-commerce yang memiliki jutaan pelanggan. Selama puncak penjualan seperti Black Friday atau 12-12 sale, jumlah interaksi dengan chatbot bisa melonjak tajam. Untuk mengatasi lonjakan ini, perusahaan dapat meningkatkan kapasitas server mereka atau menggunakan layanan cloud yang mendukung auto-scaling, seperti Amazon Web Services (AWS) atau Google Cloud Platform (GCP). Auto-scaling memungkinkan penambahan instance server secara otomatis ketika beban meningkat dan pengurangan instance ketika beban menurun, memastikan chatbot tetap responsif.

Bayangkan jika dengan peningkatan penjualan tersebut Anda masih mengandalkan satu atau dua orang sebagai customer service team, tentunya kedua orang tersebut tidak dapat membelah diri seperti ameba, ‘kan? Tentunya setiap CS akan menghadapi satu orang pelanggan dalam satu waktu atau dapat kita sebut one-to-one relationship. 

Lalu apa kelemahannya? Dengan menggunakan metode ini total waiting time, pengguna tentunya akan naik pesat terlebih pada situasi penjualan atau traffic yang sedang melonjak. Di lain sisi, tentunya customer service tersebut akan merasa lelah dan overwhelming karena mereka adalah manusia normal yang hanya bisa kerja selama 8-12 jam sehari. Sebetulnya mereka bisa juga bekerja selama 12-20 jam sehari, tapi tentu tidak baik untuk kesehatan, baik fisik maupun mental.

Di lain sisi “upah” menggunakan Generative AI jauh lebih murah, dikutip dari sada menyebutkan bahwa harga yang harus dibayar untuk menggunakan teknologi ini dari Google mulai dari $0.0005 per characters dan OpenAI mulai dari $0.004 per token. Mari kita asumsikan dengan skenario yang sama dengan customer service. Chatbot akan berjalan selama 24 jam dalam satu bulan penuh dan harus mengeluarkan 100 token/characters untuk menjawab pertanyaan pelanggan. 

$0.004 * 100 token * 1000 pelanggan = $400 atau setara Rp6.418.000,00 

Dari kasus di atas tentunya sudah jelas jika kita menempatkan diri sebagai perusahaan akan memilih penggunaan chatbot untuk menggantikan peran customer service. Kurang lebih perusahaan bisa menghemat sekitar 25 juta rupiah setiap bulannya atau bahkan lebih jika request issue yang diterima lebih banyak.

Selain biaya yang lebih murah dan ketersediaan layanan 24 jam, chatbot juga dapat mengurangi kesalahan yang dilakukan oleh manusia terlebih jika ada masalah pribadi. Yup, itu karena chatbot tidak bekerja sesuai dengan mood yang bisa berimbas kepada kinerjanya. Selain itu, chatbot juga dapat menggunakan data pelanggan untuk memberikan layanan yang dipersonalisasi. Mereka dapat mengenali pelanggan yang sudah ada, mengingat preferensi mereka, dan menawarkan rekomendasi atau solusi yang disesuaikan.

Huh, sebetulnya masih banyak kelebihan dari penggunaan chatbot yang berbasis Generative AI ini, tetapi jika kita membahas terlalu banyak akan menghabiskan banyak waktu. Seperti yang sudah dijelaskan di atas, chatbot hanyalah salah satu contoh penerapan dari Generative AI. Alangkah baiknya kita melaju ke materi selanjutnya untuk membahas ragam penerapan dari Generative AI.

Generative AI for Text:
Model bahasa seperti GPT-3 dan GPT-4 dari OpenAI ataupun Gemini dari Google telah dilatih pada sejumlah besar data teks dari internet sehingga dapat menghasilkan teks yang koheren dan informatif berdasarkan prompt atau instruksi tertentu.

Generative teks adalah proses ketika AI membuat teks baru berdasarkan input atau prompt yang diberikan. Teknologi ini dapat digunakan dalam berbagai aplikasi, seperti penulisan otomatis, chatbot, penerjemahan bahasa, dan pembuatan konten kreatif.

Generative teks ini seperti cerita salah satu penerapan chatbot kita sebelumnya. Chatbot yang menggunakan generative AI dapat menghasilkan respons otomatis yang kontekstual dalam percakapan dengan pengguna. Misalnya, asisten virtual seperti Siri, Alexa, atau Google Assistant menggunakan AI untuk memahami permintaan pengguna dan memberikan jawaban atau tindakan yang sesuai.

Selain itu, Generative AI digunakan dalam sistem terjemahan bahasa seperti Google Translate yang dapat menerjemahkan teks dari satu bahasa ke bahasa lain. Model ini tidak hanya menerjemahkan kata demi kata, tetapi juga mempertimbangkan konteks untuk memberikan terjemahan yang lebih akurat.

Generative teks menggunakan model bahasa besar (Large Language Models, LLM) untuk menghasilkan teks yang koheren dan relevan dalam berbagai konteks. LLM adalah jenis model machine learning yang memiliki parameter sangat besar (miliaran hingga triliunan) dan dilatih pada dataset teks yang sangat besar. Contoh LLM yang populer termasuk GPT-3, GPT-4, Gemini, Gemma, BERT, dan lain sebagainya.

Salah satu penerapan Generative teks paling populer adalah Gemini dan Chat GPT. keduanya merupakan tools yang sangat powerfull karena memiliki data yang sangat banyak. Mari kita bahas salah satunya, yaitu Gemini. 

Gemini AI, sebelumnya dikenal sebagai Bard, adalah chatbot kecerdasan buatan generatif yang dikembangkan oleh Google. Ini merupakan model bahasa besar (LLM) yang canggih dan dirancang untuk membantu pengguna dalam berbagai tugas seperti meningkatkan kreativitas dan produktivitas, menemukan informasi, membuat gambar, hingga memahami multimodal. 

Gemini dilatih dengan dataset teks dan kode yang sangat besar sekitar 600 miliar parameter (pada versi pro), memungkinkannya untuk menghasilkan teks, menerjemahkan bahasa, menulis berbagai jenis konten kreatif, dan menjawab pertanyaan Anda dengan cara yang informatif. 

Selain belajar dari data yang sudah ada, Gemini juga dapat belajar dan berkembang seiring waktu. Ini berarti Gemini akan terus meningkatkan kemampuannya dalam menghasilkan teks, menerjemahkan bahasa, menulis berbagai jenis konten kreatif, dan menjawab pertanyaan Anda dengan cara yang informatif.

Lalu, apa perbedaan antara Gemini AI dan model bahasa lain? Sebetulnya kelebihan lain dari Gemini adalah kemampuan multimodalnya yang dapat memahami dan memproses informasi dari berbagai format, termasuk teks, suara, gambar, dan video. Ini membedakan Gemini AI dari model bahasa lain yang hanya dapat memahami teks

Gemini AI telah menerima tanggapan yang beragam. Beberapa orang memuji kemampuannya, sementara yang lain kritis terhadap akurasi historis dari gambar yang dibuatnya. 


Generative AI for Images:
Generative AI dalam konteks generasi citra adalah proses di mana model AI menciptakan gambar baru yang tidak ada sebelumnya berdasarkan pelatihan dari dataset gambar yang ada. Teknologi ini dapat menghasilkan gambar yang realistis atau artistik dengan berbagai aplikasi mulai dari desain grafis hingga peningkatan gambar medis.

Salah satu contoh penerapan dari Generative Images ini adalah StableDiffusion. StableDiffusion adalah model kecerdasan buatan (AI) canggih yang menggunakan teknik difusi untuk menghasilkan gambar realistis berdasarkan teks. Dirilis pada tahun 2022 oleh Stability AI, Stable Diffusion telah merevolusi dunia pembuatan gambar digital

Fitur utama Stable Diffusion adalah mengubah deskripsi tekstual menjadi gambar. Anda cukup memberikan instruksi tertulis, seperti "foto astronot mengendarai kuda di Mars" dan Stable Diffusion akan menghasilkan gambar yang sesuai.

Dengan menggunakan Stable Diffusion, kita juga dapat mengatur resolusi citra yang dihasilkan karena ia mampu menghasilkan gambar yang sangat realistis dan detail. Selain itu, Anda dapat menggunakan Stable Diffusion untuk menghasilkan gambar dalam berbagai gaya artistik, seperti lukisan cat air, seni digital, atau potret realistis.

Stable Diffusion menggunakan teknik difusi. Teknik ini memanfaatkan proses bertahap yang melibatkan penambahan dan penghilangan noise (gangguan) pada gambar untuk menghasilkan gambar baru yang realistis dan berkualitas tinggii.

Bayangkan gambar yang Anda inginkan sebagai versi "bersih" dari sebuah gambar. Stable Diffusion memulai dengan versi gambar yang "berisik" (banyak noise acak) dan secara bertahap "membersihkan" noise tersebut langkah demi langkah hingga mencapai gambar yang Anda inginkan sesuai panduan instruksi teks (prompt) yang diberikan.

Kesimpulannya, difusi pada generative image menggunakan proses bertahap untuk menambahkan dan menghilangkan noise, menghasilkan gambar yang realistis, dan berkualitas tinggi. Dengan prinsip dasar yang mencakup proses difusi maju dan terbalik, teknik ini menawarkan aplikasi luas dalam pembuatan, restorasi, dan manipulasi gambar. Meskipun menantang dalam hal komputasi dan optimisasi, perkembangan terus-menerus dalam bidang ini menjanjikan potensi besar untuk masa depan teknologi generatif.


Generative AI for Video:
Generative video adalah proses di mana model AI digunakan untuk membuat atau memodifikasi video. Teknologi ini melibatkan penggunaan algoritma canggih untuk menghasilkan urutan gambar (frame) yang bersama-sama membentuk video yang koheren dan realistis. 

Model generative video dapat digunakan untuk membuat film pendek, animasi, atau video musik. AI dapat menghasilkan adegan baru, karakter animasi, dan efek visual yang sebelumnya sulit atau memakan waktu untuk dibuat secara manual.

Generative video digunakan untuk membuat deepfake, di mana wajah seseorang dalam video diganti dengan wajah orang lain dengan cara yang sangat realistis. Meskipun ada aplikasi positif dalam hiburan dan efek visual, deepfake juga menimbulkan masalah etis dan keamanan.

Keuntungan penggunaan dari generative video ini adalah dapat mengurangi waktu dan biaya yang diperlukan untuk produksi film pendek dengan memanfaatkan AI dalam pembuatan karakter, latar belakang, dan efek visual. Selain itu, AI juga dapat menghasilkan elemen visual yang unik dan inovatif, memberikan kreativitas tanpa batas dalam pembuatan konten. Last but not least adalah kemampuan untuk melakukan penyesuaian dan mendapatkan umpan balik secara real-time sehingga dapat melalukan proses iteratif yang cepat dalam pembuatan video.

Saat ini ada beberapa tools yang menerapkan generative video seperti Veo Google DeepMind, Sora OpenAI, dan DeepFake. Veo Google DeepMind dan Sora OpenAI adalah dua model kecerdasan buatan (AI) mutakhir yang mampu menghasilkan video realistis dari teks deskriptif.

Mari kita bahas salah satu tools di atas dengan lebih detail yaitu Veo Google DeepMind. Veo diluncurkan oleh Google DeepMind pada Mei 2024 berupa model kecerdasan buatan (AI) mutakhir yang merevolusi dunia pembuatan video. Model ini memungkinkan pengguna untuk menghasilkan video realistis dari teks deskriptif sehingga membuka berbagai kemungkinan baru untuk kreativitas dan komunikasi visual.

Fitur utama Veo adalah kemampuannya untuk mengubah deskripsi tekstual menjadi video yang dinamis. Anda dapat memberikan instruksi seperti "sekelompok orang menari di pantai saat matahari terbenam," dan Veo akan menghasilkan video yang sesuai dengan teks tersebut.

Veo dibangun dengan arsitektur jaringan saraf tiruan yang kompleks dilatih dengan kumpulan data besar teks dan video. Saat Anda memberikan teks deskriptif, Veo menggunakan pemahamannya tentang bahasa dan dunia visual untuk menghasilkan urutan gambar yang sesuai. 

Veo bekerja dengan cara menganalisis teks deskriptif untuk memahami makna dan maksud Anda. Ia dapat mengidentifikasi objek, tindakan, dan hubungan dalam teks. Berdasarkan pemahamannya tentang teks, Veo menghasilkan urutan gambar yang sesuai. Hasil tersebut didapat dari penggunaan teknik AI canggih seperti difusi dan sintesis gambar untuk membuat gambar yang realistis dan konsisten. Terakhir, Veo menyempurnakan dan menyesuaikan video untuk memastikan kelancaran, kualitas gambar yang optimal, dan kesesuaian dengan teks deskriptif.

Secara keseluruhan Veo adalah model AI generasi video yang luar biasa dengan potensi untuk merevolusi cara kita membuat dan mengonsumsi video. Dengan kemampuannya untuk menghasilkan video realistis dan memukau dari teks deskriptif, Veo membuka berbagai kemungkinan baru untuk kreativitas, komunikasi visual, dan berbagai aplikasi di berbagai industri.


Generative Multimodal Model:

Model multimodal adalah model yang dirancang untuk memproses dan mengintegrasikan informasi dari berbagai jenis data atau modalitas, seperti teks, gambar, audio, video, dan sensor lainnya. Model ini bertujuan untuk memahami dan menghasilkan output yang konsisten dan bermakna berdasarkan data input yang kompleks dan beragam.

Jenis modalitas yang bisa diolah oleh multimodal model mencakup teks, gambar, audio, video, dan sensor. Dengan modalitas yang beragam biasanya multimodal menggunakan beberapa pendekatan arsitektur seperti Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) atau Transformers, Cross-Modal Attention Mechanisms, dan lain sebagainya

Secara singkat multimodal akan menggabungkan fitur dari berbagai modalitas pada tahap awal sebelum pemrosesan lebih lanjut oleh model. Setelah itu, ia akan memproses setiap modalitas secara independen dan menggabungkan hasilnya di tahap akhir.

Salah satu contoh penerapan dari multimodal ini adalah Large Language and Vision Assistant (LLaVA). LLaVA adalah model kecerdasan buatan (AI) inovatif yang dikembangkan oleh Microsoft Research. Model ini menggabungkan kekuatan large language model (LLM) dengan pemahaman visual sehingga memungkinkan model untuk mengolah data teks dan citra dalam waktu yang bersamaan.

LLaVA dapat memahami instruksi yang melibatkan kombinasi teks dan gambar. Anda dapat memintanya untuk "explain this picture" atau bahkan menggunakan bahasa Indonesia seperti "tulis cerita tentang gambar ini."

Selain itu, LLaVA juga dapat menganalisis gambar dan menyimpulkan informasi penting, seperti objek, lokasi, dan aktivitas yang terjadi. Hal yang paling unik dari LLaVA adalah kemampuannya untuk melakukan tugas Optical Character Recognition (OCR).

Kesimpulannya LLaVA adalah model AI yang revolusioner dengan potensi untuk mengubah cara kita berinteraksi dengan komputer dan dunia di sekitar kita. Kemampuannya untuk memahami bahasa dan gambar secara bersamaan, menghasilkan konten kreatif multimodal, dan berinteraksi secara alami, membuka jalan bagi berbagai aplikasi baru yang menarik di berbagai bidang.

Selain LLaVA, sebetulnya masih banyak sekali multimodal model yang dapat Anda gunakan seperti Gemini Pro, Gemini Ultra, GPT-4o, Phi-3, dan lain sebagainya. Mungkin tebersit kembali sebuah pertanyaan, “Kan semua services tersebut berbayar, bagaimana untuk mahasiswa?” Tenang, kita akan mempelajari penggunaan seluruh Generative AI tanpa mengeluarkan uang sepeser pun. Sudah tidak sabar, bukan? Tetap semangat untuk materi selanjutnya.


Image Generation:

Seperti yang Anda tahu bahwa Generative AI dalam konteks generasi citra adalah proses di mana model AI menciptakan gambar baru yang tidak ada sebelumnya, berdasarkan pelatihan dari dataset gambar yang ada. Teknologi ini dapat menghasilkan gambar yang realistis atau artistik dengan berbagai aplikasi mulai dari desain grafis hingga peningkatan gambar medis.

Namun, tahukah Anda bagaimana teknologi tersebut bekerja? Secara umum ada tiga buah pendekatan yang sering digunakan untuk membangun Generative AI yang dapat menciptakan sebuah gambar yaitu Variational Autoencoders (VAE), Generative Adversarial Network (GAN), dan Diffusion Models.

GAN, VAE, dan Diffusion Models adalah tiga model Generative AI yang ampuh dengan kelebihan dan kekurangan masing-masing. Pilihan model tergantung pada kebutuhan dan permasalahan yang ingin Anda selesaikan. GAN cocok untuk menghasilkan data yang sangat realistis, sementara VAE cocok untuk menghasilkan data yang konsisten dan koheren.  Di lain sisi, Diffusion Models menawarkan keseimbangan antara kualitas dan efisiensi pelatihan.

Sebagai seorang machine learning engineer andal, alangkah baiknya Anda juga mempelajari dan membuat sendiri model Generative, baik itu melalui fine tuning, quantization, atau bahkan menggunakan open model yang tersebar di luar sana dibandingkan menggunakan API atau tools yang sudah plug and play. Menggunakan API dan tools yang sudah ada bukanlah hal yang buruk, tetapi jika Anda bisa membuat dan menyesuaikan model Generative tentunya menjadi sebuah nilai lebih untuk mempertahankan posisi Anda di perusahaan.

Tanpa berlama-lama lagi, mari kita bahas masing-masing pendekatan dengan lebih detail agar Anda dapat menentukan metode mana yang paling cocok untuk menyelesaikan permasalahan yang sedang dialami.

Variational Autoencoder (VAE):
Variational Autoencoders (VAE) adalah salah satu jenis model generatif yang menggabungkan konsep autoencoder dengan teknik probabilistik untuk menghasilkan data baru yang realistis. VAE dikembangkan oleh Kingma dan Welling pada tahun 2013. Konsep ini dirancang untuk mengatasi beberapa keterbatasan dari autoencoder tradisional, terutama dalam hal kemampuan generatif dan representasi laten.

VAE menggunakan konsep autoencoder sebagai inti dari kinerjanya, autoencoder berperan sebagai blok bangunan fundamental yang memungkinkan model untuk mempelajari representasi laten data. Autoencoder adalah jenis jaringan saraf yang terdiri dari dua bagian utama, yaitu encoder dan decoder.

Secara singkat encoder bertugas untuk mengambil data masukan (misalnya, gambar) dan mengonversinya menjadi representasi laten yang lebih kecil. Representasi laten ini menangkap informasi penting dari data asli. Di lain sisi, decoder menerima representasi laten dari encoder dan mencoba merekonstruksi data asli dari representasi tersebut. 

Encoder dan decoder dilatih secara bersamaan untuk meminimalkan perbedaan antara data asli dan data yang direkonstruksi. Ini memastikan decoder dapat merekonstruksi data dengan baik dari representasi laten yang dihasilkan oleh encoder.

Selain rekonstruksi data, VAE menggunakan regularisasi untuk mendorong representasi laten menjadi lebih informatif. Ini penting karena representasi laten tidak hanya harus memungkinkan rekonstruksi data, tetapi juga berguna untuk menghasilkan data baru.
- Variabel Laten: variabel tersembunyi atau acak yang, meskipun tidak dapat diamati secara langsung, pada dasarnya menginformasikan cara data didistribusikan.
- Representasi Laten: kode yang muncul dari lapisan encoder kemudian dimasukkan ke dalam decoder.

Perbedaan di atas membuat arsitektur diagram VAE memiliki sedikit perbedaan dari autoencoder dasar. Dalam Variational Autoencoders (VAEs), z_mean dan z_log_var adalah variabel penting yang terkait dengan representasi laten data. Representasi laten ini merupakan representasi terkompresi dari data asli yang digunakan VAE untuk berbagai fungsinya, seperti rekonstruksi data dan generasi data baru.

Lalu, z_mean merupakan representasi rata-rata dari distribusi probabilitas representasi laten. Variabel ini merupakan vector numerik yang mewakili nilai "pusat" dari representasi laten yang mungkin dihasilkan oleh VAE. Selain itu, fungsi dari z_mean juga untuk menunjukkan kecenderungan atau bias representasi laten.

Selanjutnya ada z_log_var yang merupakan representasi logaritma varians dari distribusi probabilitas representasi laten. Nilai yang lebih tinggi dalam z_log_var menunjukkan bahwa representasi laten dapat tersebar lebih luas, sedangkan nilai yang lebih rendah menunjukkan representasi laten yang lebih terkonsentrasi di sekitar z_mean.

Kedua variabel di atas memiliki peran penting untuk menghasilkan data baru. VAE dapat menghasilkan data baru dengan mengambil sampel acak dari distribusi probabilitas representasi laten yang didefinisikan oleh z_mean dan z_log_var. Variasi dalam representasi laten (ditunjukkan oleh z_log_var) memungkinkan VAE untuk menghasilkan data baru yang beragam. Dalam beberapa kasus, z_mean dan z_log_var dapat diinterpretasikan untuk memahami apa yang diwakili oleh dimensi tertentu dalam representasi laten.

Setelah bermain-main dengan matematika, apakah Anda mulai merasa rindu ngoding? Jika perasaan itu sudah hadir, mari kita mulai praktik untuk mengetahui cara penggunaan VAE menggunakan dataset fashion mnist. Seperti biasa, impor terlebih dahulu library yang akan digunakan.

import numpy as np
import matplotlib.pyplot as plt
 
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras import (
    layers,
    models,
    datasets,
    callbacks,
    losses,
    optimizers,
    metrics,
)
from scipy.stats import norm 

Setelah dataset siap digunakan tentunya Anda masih ingat bahwa kita perlu melakukan preprocessing data agar model dapat mempelajari pola dengan lebih baik. Karena kita akan melakukan preprocessing terhadap data latih dan data uji, sangat disarankan untuk membuat sebuah fungsi agar dapat digunakan berulang kali.

def preprocess(imgs):
    imgs = imgs.astype("float32") / 255.0
    imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=0.0)
    imgs = np.expand_dims(imgs, -1)
    return imgs
 
x_train = preprocess(x_train)
x_test = preprocess(x_test)

Kemudian, buatlah sebuah kelas Sampling layer yang bertugas untuk menghitung distribusi data berdasarkan z_mean dan z_log_var.

class Sampling(layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = K.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

Mari kita bahas baris per baris:
- class Sampling(layers.Layer): baris ini mendeklarasikan sebuah class bernama Sampling yang mewarisi dari class layers.Layer di TensorFlow. Ini menandakan bahwa Sampling adalah sebuah lapisan kustom yang bisa kita gunakan dalam model kita.
- def call(self, inputs): method call ini didefinisikan untuk menentukan bagaimana lapisan ini memproses input dan menghasilkan output. Fungsi call ini akan dipanggil setiap kali lapisan ini digunakan dalam model.
- z_mean, z_log_var = inputs: baris ini mengambil input dari lapisan sebelumnya. Biasanya dalam VAE, input akan terdiri dari dua tensor:
- z_mean: merepresentasikan mean (rata-rata) dari latent space.
- z_log_var: merepresentasikan log variance (log varians) dari latent space.

- batch = tf.shape(z_mean)[0]: menyimpan jumlah sampel dalam batch.
- dim = tf.shape(z_mean)[1]: menyimpan dimensi dari latent space (biasanya jumlah neuron di lapisan latent).
- epsilon = K.random_normal(shape=(batch, dim)): baris ini menggunakan fungsi K.random_normal dari library Keras untuk menghasilkan noise acak. Noise ini berdistribusi normal dengan mean 0 dan variance 1. Sedangkan shape=(batch, dim) memastikan noise memiliki ukuran yang sama dengan z_mean dan z_log_var
- tf.exp(0.5 * z_log_var): digunakan untuk menghitung standard deviation dari latent space karena varians bernilai kuadrat dari standar deviasi.
- epsilon * standard_deviation: menambahkan noise acak yang sudah diskalakan oleh standar deviasi ke z_mean

Singkatnya lapisan Sampling di atas mengambil mean dan log variance dari latent space, kemudian menggunakan noise acak untuk menghasilkan sampel acak yang mewakili distribusi probabilitas di latent space.

Selanjutnya, mari kita membuat jaringan saraf tiruan yang bertugas untuk melakukan encoding dan decoding.

encoder_input = layers.Input(
    shape=(IMAGE_SIZE, IMAGE_SIZE, 1), name="encoder_input"
)
x = layers.Conv2D(32, (3, 3), strides=2, activation="relu", padding="same")(
    encoder_input
)
x = layers.Conv2D(64, (3, 3), strides=2, activation="relu", padding="same")(x)
x = layers.Conv2D(128, (3, 3), strides=2, activation="relu", padding="same")(x)
shape_before_flattening = K.int_shape(x)[1:]  
 
x = layers.Flatten()(x)
z_mean = layers.Dense(EMBEDDING_DIM, name="z_mean")(x)
z_log_var = layers.Dense(EMBEDDING_DIM, name="z_log_var")(x)
z = Sampling()([z_mean, z_log_var])
 
encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name="encoder")
encoder.summary()

decoder_input = layers.Input(shape=(EMBEDDING_DIM,), name="decoder_input")
x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)
x = layers.Reshape(shape_before_flattening)(x)
x = layers.Conv2DTranspose(
    128, (3, 3), strides=2, activation="relu", padding="same"
)(x)
x = layers.Conv2DTranspose(
    64, (3, 3), strides=2, activation="relu", padding="same"
)(x)
x = layers.Conv2DTranspose(
    32, (3, 3), strides=2, activation="relu", padding="same"
)(x)
decoder_output = layers.Conv2D(
    1,
    (3, 3),
    strides=1,
    activation="sigmoid",
    padding="same",
    name="decoder_output",
)(x)
 
decoder = models.Model(decoder_input, decoder_output)
decoder.summary()


Setelah model rampung dibuat tentunya kita perlu melakukan pelatihan, tetapi VAE belum memiliki library pendukung layaknya deep learning lainnya. Oleh karena itu, mari kita buat sebuah kelas VAE yang merupakan turunan dari models TensorFlow.

class VAE(models.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.total_loss_tracker = metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = metrics.Mean(
            name="reconstruction_loss"
        )
        self.kl_loss_tracker = metrics.Mean(name="kl_loss")

...
Gradient Tape adalah sebuah mekanisme yang memungkinkan komputasi gradien berdasarkan komputasi yang dijalankan selama forward pass sebuah model. Untuk menggunakannya, Anda perlu membungkus kode yang melakukan operasi yang ingin dibedakan dalam konteks tf.GradientTape(). Setelah menghitung nilai operasi, Anda dapat menghitung gradien dari fungsi kerugian sehubungan dengan beberapa variabel dengan memanggil tape.gradient(). Gradien kemudian dapat digunakan untuk memperbarui variabel dengan pengoptimal.

Kode di atas pada dasarnya merupakan isi dari model yang dibangun ketika menggunakan TensorFlow. Selanjutnya, mari kita latih menggunakan fungsi .fit(). Eitss, jangan lupa untuk menambahkan optimizers dan melakukan compiling, ya.

vae = VAE(encoder, decoder)
optimizer = optimizers.Adam(learning_rate=0.0005)
vae.compile(optimizer=optimizer)
 
model_checkpoint_callback = callbacks.ModelCheckpoint(
    filepath="./checkpoint",
    save_weights_only=False,
    save_freq="epoch",
    monitor="loss",
    mode="min",
    save_best_only=True,
    verbose=0,
)
tensorboard_callback = callbacks.TensorBoard(log_dir="./logs")
 
vae.fit(
    x_train,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    shuffle=True,
    validation_data=(x_test, x_test),
    callbacks=[model_checkpoint_callback, tensorboard_callback],
)

Selanjutnya, kita dapat menggunakan encoder untuk mengodekan gambar dalam pengujian dan memplot nilai z_mean dalam ruang laten. Kita juga dapat mengambil sampel dari distribusi normal untuk menghasilkan titik-titik di ruang laten dan menggunakan decoder untuk menguraikan titik-titik ini kembali ke dalam ruang piksel untuk melihat kinerja VAE.

Untuk menguji model VAE, kita dapat melakukan test yang dapat menghasilkan gambar berdasarkan ruang laten yang telah dibuat.

# Encode citra
z_mean, z_var, z = encoder.predict(example_images)
 
grid_width, grid_height = (6, 3)
z_sample = np.random.normal(size=(grid_width * grid_height, 2))
 
# Decode sampel poin
reconstructions = decoder.predict(z_sample)
 
# Konversi nilai embedding menjadi p_values
p = norm.cdf(z)
p_sample = norm.cdf(z_sample)
 
# Membuat plot
figsize = 8
plt.figure(figsize=(figsize, figsize))
 
# Original Embeddings
plt.scatter(z[:, 0], z[:, 1], c="black", alpha=0.5, s=2)
 
# Ruang laten
plt.scatter(z_sample[:, 0], z_sample[:, 1], c="#00B0F0", alpha=1, s=40)
plt.show()
 
fig = plt.figure(figsize=(figsize, grid_height * 2))
fig.subplots_adjust(hspace=0.4, wspace=0.4)
 
for i in range(grid_width * grid_height):
    ax = fig.add_subplot(grid_height, grid_width, i + 1)
    ax.axis("off")
    ax.text(
        0.5,
        -0.35,
        str(np.round(z_sample[i, :], 1)),
        fontsize=10,
        ha="center",
        transform=ax.transAxes,
    )
    ax.imshow(reconstructions[i, :, :], cmap="Greys")

Selamat, Anda telah membuat sebuah model VAE dengan ruang laten dua dimensi. Tenang saja tidak usah terburu-buru untuk memahami materi di atas karena kita akan membahas tuntas algoritma ini di kelas berikutnya. 

Tentunya dengan mengetahui pendekatan generative AI untuk citra/gambar, Anda semakin dekat dengan teknologi yang sedang hype belakangan ini. Selanjutnya, kita akan mempelajari pendekatan lainnya yaitu Generative Adversarial Network. Tetap semangat ya, sampai jumpa.


Generative Adversarial Network (GAN):
Generative Adversarial Networks (GAN) adalah salah satu model paling inovatif dalam bidang machine learning, khususnya dalam generative AI. Diperkenalkan oleh Ian Goodfellow et al., pada tahun 2014, GAN terdiri dari dua jaringan saraf yang berkompetisi satu sama lain, yaitu Generator dan Discriminator. Generator mencoba mengubah noise acak menjadi sebuah gambar baru, sedangkan diskriminator mencoba memprediksi apakah gambar tersebut asli atau bukan.

Lalu bagaimana cara kerjanya? Generator akan menghasilkan gambar dan diskriminator memprediksi secara acak. Kunci dari metode GAN terletak pada bagaimana kita melatih kedua jaringan tersebut (generator dan diskriminator) sehingga ketika generator menjadi lebih mahir dalam mengelabui diskriminator, diskriminator harus beradaptasi untuk mempertahankan kemampuannya dalam mengidentifikasi dengan benar. Hal ini mendorong generator untuk menemukan cara-cara baru untuk menipu diskriminator, dan siklus ini terus berlanjut.

Tujuan utama persaingan antara generator dan diskriminator adalah untuk menghasilkan data yang sangat realistis seiring berjalannya waktu, keduanya saling “menipu” untuk kebaikan bersama. Mari bahas bersama komponen utama dari GAN.

1. Generator
Generator bertugas untuk menghasilkan data baru yang menyerupai data pelatihan.
Mengambil input berupa noise acak (biasanya vector dari distribusi normal) dan mengubahnya menjadi data yang menyerupai distribusi data pelatihan.
Tujuannya adalah untuk "menipu" Discriminator sehingga tidak dapat membedakan antara data nyata dan data palsu yang dihasilkan.

2. Discriminator
Discriminator bertugas untuk membedakan antara data nyata (dari dataset pelatihan) dan data palsu (dari Generator).
Mengambil input berupa data (baik nyata maupun palsu) dan menghasilkan probabilitas yang menunjukkan apakah data tersebut nyata atau palsu.
Tujuannya adalah untuk membuat data baru dengan akurat agar dapat diklasifikasikan sebagai data nyata atau palsu.

Cara kerja GAN secara singkat adalah mengubah gambar dari satu domain ke domain lain, seperti mengubah foto siang menjadi malam atau mengubah tipe penulisan angka. Menarik, ‘kan.

Jika berbicara tentang teori GAN akan memakan waktu yang sangat banyak karena GAN memiliki banyak sekali turunan seperti Deep Convolutional GAN (DCGAN), Wasserstein GAN with Gradient Penalty (WGAN-GP), Conditional GAN (CGAN), dan lain sebagainya. Sebagai langkah awal Anda akan menemani perjalanan Diana dimulai dari mempelajari cara membangun sebuah sistem GAN sederhana.

Jika sebelumnya kita pernah mempelajari tentang Scikit-Learn, TensorFlow, dan Keras., sekarang saatnya kita berkenalan dengan library lainnya yaitu torch. Pertama, mari kita impor library yang akan digunakan.

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

Selanjutnya, kita perlu menentukan hyperparameter yang akan digunakan selama proses pelatihan model dengan catatan Anda dapat melakukan perubahan pada masing-masing variabel. 

latent_dim = 100
hidden_dim = 256
image_dim = 28 * 28
batch_size = 64
learning_rate = 0.0002
num_epochs = 50

Kode di atas bertugas untuk menentukan nilai-nilai hyperparameter yang digunakan dalam pelatihan model Generative Adversarial Network (GAN). Hyperparameter adalah parameter yang nilai awalnya harus ditentukan sebelum pelatihan model dan berpengaruh besar terhadap kinerja dan hasil akhir model.

- latent_dim = 100: hyperparameter ini menentukan dimensi dari ruang laten, yaitu dimensi dari input noise yang diberikan kepada Generator. Noise dengan nilai acak ini akan diubah oleh Generator menjadi data yang menyerupai data pelatihan. Biasanya, nilai ini dipilih cukup besar untuk memungkinkan Generator membuat representasi yang kompleks, tetapi tidak terlalu besar sehingga memperlambat pelatihan.

- hidden_dim = 256: jumlah neuron dalam layer tersembunyi (hidden layers) dari model Generator dan Discriminator. Jumlah neuron dalam layer tersembunyi mempengaruhi kapasitas model untuk menangkap pola-pola dalam data. Nilai ini harus cukup besar untuk menangkap kerumitan data, tetapi tidak terlalu besar untuk menghindari overfitting dan penggunaan memori yang berlebihan.

- image_dim = 28 * 28: dimensi dari gambar input yang akan digunakan dalam model. Dalam konteks dataset MNIST, gambar memiliki ukuran 28x28 piksel, jadi image_dim adalah 784 (28 * 28). Ini merepresentasikan jumlah fitur dalam satu gambar yang akan dimasukkan ke dalam Discriminator.

- batch_size = 64: jumlah sampel data yang akan diproses dalam satu batch selama pelatihan. Menggunakan batch size memungkinkan pelatihan lebih efisien dengan memanfaatkan komputasi paralel. Batch size yang lebih besar cenderung memberikan estimasi gradien yang lebih stabil, tetapi memerlukan lebih banyak memori.

learning_rate = 0.0002: laju pembelajaran (learning rate), menghitung seberapa besar langkah yang diambil model dalam memperbarui parameter selama pelatihan. Learning rate tinggi dapat mempercepat pelatihan, tetapi jika terlalu tinggi dapat menyebabkan pelatihan tidak stabil atau gagal menemukan solusi optimal. Sebaliknya learning rate yang terlalu rendah akan membuat pelatihan lambat dan mungkin terjebak di minimum lokal.
num_epochs = 50: jumlah iterasi penuh selama pelatihan. Semakin banyak jumlah epoch, semakin lama juga model akan dilatih sehingga kinerja model akan meningkat sampai titik tertentu yang dapat menyebabkan overfitting. Pilihan jumlah epoch yang tepat bergantung pada ukuran dan kompleksitas dataset, serta kapasitas model.

Kemudian kita perlu mempersiapkan dan memuat dataset MNIST untuk pelatihan dalam bentuk yang sesuai. Transformasi diterapkan untuk memastikan data berada dalam format yang benar dan memiliki skala sesuai untuk pelatihan.

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)

- transforms.Compose([...]): menggabungkan beberapa data menjadi satu yang akan diterapkan secara berurutan pada setiap data.
- transforms.ToTensor(): mengubah gambar dalam bentuk PIL Image atau numpy array menjadi tensor PyTorch. Selain itu, juga mengubah skala piksel dari rentang [0, 255] ke [0.0, 1.0].
- transforms.Normalize((0.5,), (0.5,)): menormalisasi tensor gambar dengan rata-rata 0.5 dan standar deviasi 0.5 untuk setiap saluran.

Setelah semuanya siap mari kita buat komponen utama dari model GAN, tentunya Anda masih ingat bukan? Yup, Generator dan Discriminator. Karena materi ini bertujuan untuk melatih materi dasar, mari kita buat kedua komponen tersebut dari awall 

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(True),
            nn.Linear(hidden_dim, image_dim),
            nn.Tanh()
        )
 
    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), 1, 28, 28)
        return img

Kode di atas membuat sebuah kelas bernama Generator yang bertugas untuk membuat gambar baru berdasarkan data noise yang sudah dipelajari. Mari kita bahas kode tersebut dengan detail.
- class Generator(nn.Module): kelas Generator ini merupakan subclass dari nn.Module, yang merupakan kelas dasar untuk semua jaringan saraf dalam PyTorch. Dengan mewarisi sifat dari nn.Module, kelas ini dapat memanfaatkan semua fungsionalitas yang ada dalam PyTorch untuk membangun dan melatih model jaringan saraf.

- def __init__(self): metode inisialisasi atau konstruktor untuk kelas Generator.
- super(Generator, self).__init__(): memanggil konstruktor dari kelas induk (nn.Module) untuk memastikan semua properti dan metode dari nn.Module diinisialisasi dengan benar.
- self.model = nn.Sequential(...): mendefinisikan jaringan saraf sebagai lapisan sequential yang bertugas untuk membangun jaringan lapis demi lapis.
- nn.Linear(latent_dim, hidden_dim): lapisan linear (fully connected) pertama yang mengambil input dari ruang laten (latent_dim) dan memetakan ke ruang berdimensi hidden_dim. latent_dim biasanya adalah dimensi dari input noise.
- nn.ReLU(True): fungsi aktivasi ReLU (Rectified Linear Unit) yang diterapkan setelah lapisan linear pertama. True menunjukkan bahwa aktivasi dilakukan secara in-place untuk menghemat memori.
nn.
- Linear(hidden_dim, hidden_dim): lapisan linear kedua yang tetap memiliki dimensi hidden_dim. Ini menambahkan kompleksitas ke jaringan dengan lebih banyak parameter untuk dipelajari.
- def forward(self, z): metode forward ini mendefinisikan aliran data melalui jaringan ketika input diberikan. Metode ini harus diimplementasikan kepada semua subclass nn.Module dan menentukan bagaimana input diteruskan melalui fully connected layer.
- img = self.model(z): input z, yang merupakan tensor noise acak dari ruang laten dengan dimensi latent_dim, diteruskan melalui jaringan yang didefinisikan dalam self.model. Hasilnya adalah tensor dengan dimensi image_dim.

Kelas Generator ini bertanggung jawab untuk mengambil input noise acak dari ruang laten dan menghasilkan gambar yang realistis. Struktur jaringan yang dibangun menggunakan beberapa lapisan linear dan fungsi aktivasi ReLU, diakhiri dengan fungsi aktivasi Tanh untuk memastikan output berada dalam rentang yang diinginkan. Metode forward mendefinisikan bagaimana input diteruskan melalui jaringan neuron dan mengubah bentuk output agar sesuai dengan dimensi gambar yang diinginkan.

Ingatkah kalian bahwa GAN terdiri dari dua buah komponen utama? Yup, kita belum mendefinisikan Discriminator sehingga kelas Generator di atas belum dapat digunakan secara fungsional.

Untuk melengkapi komponen GAN mari kita buat sebuah kelas Discriminator seperti berikut.

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(image_dim, hidden_dim),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
 
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity

Perhatikan kode Discriminator di atas, apakah Anda menemukan perbedaan kode dengan Generator? Secara singkat memang terdapat sedikit perbedaan layer, tetapi tidak hanya layer bahkan fungsi forward pada kelas ini memiliki tugas yang berbeda. Mari kita bahas satu per satu

Kelas Discriminator di atas bertanggung jawab untuk mengambil gambar sebagai input dan menghasilkan skor yang menunjukkan seberapa asli gambar tersebut. Struktur jaringan menggunakan beberapa lapisan linear dan fungsi aktivasi Leaky ReLU, diakhiri dengan fungsi aktivasi Sigmoid untuk membatasi output ke rentang [0, 1].

Metode forward mendefinisikan bagaimana input akan diteruskan kepada jaringan neural network dan mengubah bentuk input agar sesuai dengan dimensi yang diinginkan. Dengan cara ini, Discriminator dilatih untuk membedakan antara gambar asli dan gambar palsu yang dihasilkan oleh Generator.

Lalu, bagaimana cara melatih kedua kelas tersebut agar dapat bekerja sesuai tugasnya? Pertama-tama kita perlu melakukan inisialisasi kelas tersebut dengan kode berikut.

generator = Generator()
discriminator = Discriminator()
Kedua, tentukan optimizers dan loss function seperti model deep learning pada umumnya. 

# Optimizers
optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)
optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)
 
# Loss function
adversarial_loss = nn.BCELoss()

Terakhir, kita perlu melatih model agar dapat mempelajari pola dari data yang sudah ada. Jika kita menggunakan TensorFlow, tentu akan sangat mudah karena tinggal memanggil fungsi model.fit(). Untuk kasus ini, kita akan membuat sebuah fungsi yang dapat melatih model GAN dari awal dengan tujuan untuk memberikan pengetahuan bagaimana fungsi training berjalan.

for epoch in range(num_epochs):
    for i, (imgs, _) in enumerate(train_loader):
        valid = torch.ones(imgs.size(0), 1)
        fake = torch.zeros(imgs.size(0), 1)
 
        real_imgs = imgs
 
        # Train Generator
        optimizer_G.zero_grad()
        z = torch.randn(imgs.size(0), latent_dim)
        gen_imgs = generator(z)
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)
        g_loss.backward()
        optimizer_G.step()
 
        # Train Discriminator
        optimizer_D.zero_grad()
        real_loss = adversarial_loss(discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_D.step()
 
    print(f'Epoch {epoch + 1}/{num_epochs}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')

Perulangan di atas akan menjalankan pelatihan sebanyak nilai epochs yang sudah ditentukan. Lalu bagaimana dengan yang lainnya? 

for i, (imgs, _) in enumerate(train_loader): perulangan data sejumlah satu batch. train_loader adalah DataLoader yang memuat data pelatihan. Dan imgs adalah batch gambar, dan _ mengabaikan label yang tidak diperlukan untuk pelatihan GAN.
valid = torch.ones(imgs.size(0), 1): membuat tensor valid dengan nilai 1 untuk menunjukkan gambar asli. Ukuran batch adalah imgs.size(0) dengan satu kolom (1).
fake = torch.zeros(imgs.size(0), 1): membuat tensor fake dengan nilai 0 untuk menunjukkan gambar palsu. Ukuran batch adalah imgs.size(0) dengan satu kolom (1).
real_imgs = imgs: menyimpan gambar asli ke variabel real_imgs.

Kode di atas melatih GAN sejumlah epoch yang sudah ditentukan, di mana pada setiap epoch generator dan discriminator dilatih secara bergantian menggunakan data dari setiap batch train_loader. Generator dilatih untuk menghasilkan gambar yang terlihat asli, sementara discriminator dilatih untuk membedakan antara gambar asli dan gambar palsu. Proses ini berulang hingga semua epoch selesai. Kurang lebih gambaran kedua komponen utama GAN akan bekerja seperti berikut.

Tentunya sampai di sini mungkin tebersit di benak Anda sebuah pertanyaan, “Lalu bagaimana membuat gambar baru dari model tersebut?” Tenang, kita akan mempelajari sampai akarnya. Kita akan menghasilkan gambar baru menggunakan fungsi yang dapat menjalankan model GAN dengan mudah melalui kode berikut.


# Generate new images
def generate_images(generator, num_images, latent_dim):
    # Menentukan status generator menjadi evaluation mode
    generator.eval()
 
    # Generate random noise
    z = torch.randn(num_images, latent_dim)
 
    # Generate images from noise
    gen_imgs = generator(z)
 
    # Rescale images menjadi rentang [0, 1]
    gen_imgs = 0.5 * gen_imgs + 0.5
 
    # Plot the generated images
    fig, axs = plt.subplots(1, num_images, figsize=(num_images, 1))
    for i in range(num_images):
        axs[i].imshow(gen_imgs[i].detach().numpy().squeeze(), cmap='gray')
        axs[i].axis('off')
    plt.show()
 
# Generate dan menampilkan 10 gambar baru
generate_images(generator, 10, latent_dim)

Sampai di sini Anda sudah memahami dasar GAN hingga menghasilkan gambar baru. Jika masih memiliki rasa penasaran yang sangat tinggi, Anda dapat melakukan eksplorasi terkait deep convolutional GAN (DCGAN), Wasserstein GAN with Gradient Penalty (WGAN-GP), ataupun the conditional GAN (CGAN). 

Selanjutnya, kita akan mempelajari satu pendekatan Generative AI yang dapat menghasilkan gambar yaitu Diffusion Models, hmmm menarik ya? Oleh karena itu, jangan ke mana-mana, mari kita lanjutkan proses pembelajaran bersama-sama


Diffusion Models:
model difusi (diffusion models) merupakan salah satu teknik generatif yang paling populer dan memiliki peran penting untuk model pembuatan gambar. Pada beberapa aspek model difusi kini mengungguli GAN yang sebelumnya terkenal canggih dan menjadi pilihan utama bagi para praktisi machine learning, terutama untuk domain visual (misalnya, DALL.E 2 dari OpenAI dan ImageGen dari Google untuk pembuatan teks ke gambar). 

Difusi mengacu pada proses mengubah sinyal terstruktur (gambar) menjadi noise acak sedikit demi sedikit. Dengan menggunakan metode difusi, kita dapat menghasilkan noise dari gambar yang sudah ada. Lalu, kita juga dapat melatih jaringan saraf untuk mencoba menghilangkan noise tersebut. Dengan menggunakan jaringan yang telah dilatih, kita dapat menyimulasikan kebalikan dari difusi, yaitu reverse diffusion yang merupakan proses menghasilkan gambar dari noise.

Sama halnya seperti GAN dan VAE, diffusion models adalah jenis model generatif yang digunakan untuk menghasilkan gambar menyerupai data asli. Secara singkat model ini memanfaatkan proses difusi dan denoising untuk belajar dan menghasilkan data.

Salah satu platform penyedia model diffusion adalah Stable Diffusion Online. Stable Diffusion adalah model difusi teks ke gambar laten yang mampu menghasilkan gambar realistis dengan input teks apa pun dengan gratis secara online. Versi terbaru dari model ini adalah Stable Diffusion XL yang lebih besar dari pada UNet backbone dan dapat menghasilkan gambar berkualitas lebih tinggi. Silakan Anda eksplorasi prompt atau perintah untuk menghasilkan gambar pada platform tersebut untuk mengetahui secara singkat hasil dan proses dari diffusion model. Sebagai informasi Anda dapat mencari lebih dari 9 juta contoh prompt di Prompt Database Stable Diffusion.

apakah Anda tidak merasa terpanggil dan penasaran bagaimana proses dibalik platform Stable Diffusion Online? Tentunya sebagai seorang machine learning engineer, kita memiliki kelebihan untuk tidak sekadar menggunakan services yang sudah ada, alangkah baiknya kita bisa menggunakan atau mengoptimalkan pre-trained model yang sudah ada atau bahkan membuat model dari awal.

Walaupun pada akhirnya Anda bisa menggunakan services yang sudah ada dengan sangat mudah atau low code,  tentunya diperlukan pemahaman terkait konfigurasi parameter dari model yang digunakan. Dengan memahami konsepnya dari dasar, Anda dapat mengatur dan membuat model dengan lebih leluasa sesuai dengan kasus yang akan diselesaikan. Jadi, semakin banyak tahu, semakin menyenangkan, ‘kan?

Stable Diffusion adalah model generatif berbasis difusi yang digunakan untuk menghasilkan gambar. Model ini dirancang untuk mengubah teks menjadi gambar dan melakukan modifikasi atau menciptakan gambar berdasarkan input teks. Model ini dikembangkan oleh Stability AI, dengan kolaborasi dari berbagai organisasi seperti EleutherAI dan LAION. Stable Diffusion menjadi populer karena kualitasnya yang tinggi dan efisiensi komputasinya, serta model ini tersedia secara open-source sehingga memungkinkan penggunaan dan modifikasi luas oleh semua orang.

Saat ini, Stable Diffusion sudah memiliki beberapa versi, mulai dari stable-diffusion-v1-1 hingga stable-diffusion-v1-4. Untuk mengetahui perbedaannya, Anda bisa melihat detail dari masing-masing versi di tautan berikut: CompVis. Pada contoh kasus ini, kita akan menggunakan versi terbaru dari Stable Diffusion Models yaitu versi v1-4 karena setiap versi dari Stable Diffusion membawa peningkatan signifikan dalam hal kualitas gambar, efisiensi komputasi, dan akurasi dalam mengikuti deskripsi teks.

Hal pertama yang perlu Anda lakukan adalah memastikan bahwa kode yang akan dijalankan akan menggunakan GPU sebagai tenaga utamanya. 

# Menggunakan local environment
nvidia-msi
# Menggunakan Google Colab
!nvidia-msi 

Seperti biasa langkah awal yang perlu Anda lakukan adalah instalasi libraries yang akan digunakan. Jika Anda menggunakan Google Colab, cukup lakukan instalasi beberapa library berikut karena by default library lainnya sudah terpasang di Google Colab.

!pip install diffusers==0.11.1
!pip install -q accelerate transformers bitsandbytes==0.35.0 safetensors xformers
!pip install jax==0.4.23 jaxlib==0.4.23

Setelah semuanya berhasil terpasang, tentunya Anda penasaran dengan fungsi dari libraries sebanyak itu. Tenang, mari kita bahas satu per satu
- diffusers: library dari Hugging Face yang fokus pada model difusi, termasuk Stable Diffusion, untuk menghasilkan gambar atau data lainnya.
- accelerate: library ini dikembangkan oleh Hugging Face untuk memudahkan akselerasi dan pengaturan pelatihan model pada berbagai perangkat keras (CPU, GPU, TPU) dengan minimal perubahan kode.
- transformers: library utama dari Hugging Face yang menyediakan berbagai model transformer pra-latih untuk tugas-tugas NLP seperti teks klasifikasi, teks generasi, dan penerjemahan.
- bitsandbytes: library yang mengoptimalkan penggunaan memori dan komputasi dalam pelatihan model deep learning
- safetensors: digunakan untuk menyimpan dan memuat tensor dalam proyek machine learning dengan keamanan tambahan karena akan menyimpan data tensor (data multidimensi yang digunakan dalam machine learning).
- xformers: bertugas untuk meningkatkan performa model transformer dalam pelatihan dan inferensi.
- jax: library untuk melakukan komputasi numerik yang memungkinkan penulisan kode Python yang berjalan dengan cepat pada CPU dan GPU.
- jaxlib: library pendukung untuk jax yang menyertakan implementasi low-level dari operasi yang dipercepat oleh Accelerated Linear Algebra (XLA) dan menyediakan binding ke perangkat keras untuk komputasi dengan lebih efisien

Selanjutnya, panggillah libraries tersebut agar dapat digunakan menggunakan kode berikut.

import torch 
from diffusers import StableDiffusionPipeline
from PIL import Image
import matplotlib.pyplot as plt

Seperti pada kasus deep learning lainnya, kita perlu mengunduh dataset atau pada kasus ini adalah pre-trained model agar dapat dikonsumsi oleh program yang sedang kita bangun.

pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16)

Kode di atas digunakan untuk memuat model Stable Diffusion yang telah dilatih sebelumnya (pre-trained) menggunakan library diffusers dari Hugging Face. Berikut merupakan langkah yang akan ditempuh oleh kode tersebut
- StableDiffusionPipeline.from_pretrained akan mengunduh model CompVis/stable-diffusion-v1-4 dari repositori Hugging Face Model Hub termasuk model difusi, tokenizer, dan konfigurasi.
- Model dimuat dengan precision 16-bit floating point (torch.float16), yang efisien dalam hal memori dan kecepatan komputasi saat dijalankan di GPU. 
- Pipeline yang diinisialisasi akan disimpan pada variabel pipe yang dapat digunakan untuk menjalankan tugas-tugas generatif, seperti mengubah deskripsi teks menjadi gambar dengan menggunakan model Stable Diffusion.

Sebagai informasi Anda akan mengunduh beberapa file stable-diffusion-v1-4 mulai dari model_index.json hingga model safetensors. Oleh karena itu, jika Anda menggunakan local environment, mohon persiapkan jaringan internet yang stabil, ya.

Seperti yang sudah kita bahas sebelumnya, kode ini memerlukan GPU sebagai tenaga utamanya. Untuk memastikan penggunaan GPU, kita dapat menentukan cuda sebagai komponen utama untuk menjalankan pipeline di atas.

pipe = pipe.to('cuda')

Setelah itu, kita perlu mengoptimalkan penggunaan memori dan meningkatkan efisiensi komputasi ketika menggunakan model yang cukup besar, tetapi memiliki spesifikasi menengah.

pipe.enable_attention_slicing()
pipe.enable_xformers_memory_efficient_attention()

Dengan mengaktifkan kedua metode ini, Anda dapat menjalankan model Stable Diffusion dengan lebih efisien, terutama jika Anda bekerja dengan model yang besar atau gambar beresolusi tinggi pada perangkat keras yang memiliki keterbatasan memori. 

Langkah terakhir yang perlu kita perhatikan adalah safety checker. Fungsi ini bertugas untuk memastikan gambar yang dihasilkan tidak hitam sepenuhnya.

Setelah semuanya siap, Anda perlu mengetahui bahwa ada beberapa parameter krusial yang perlu diatur agar dapat menghasilkan gambar sesuai dengan ekspektasi. Parameter tersebut dapat Anda sesuaikan dengan target dan kualitas gambar yang dihasilkan mulai dari Prompt, Seed, Inference Step, Guidance Scale, Image Size dan Negative Prompt.
1. Prompt
Prompt adalah teks yang Anda berikan sebagai input ke model untuk menghasilkan gambar. Ini bisa berupa deskripsi, kalimat, atau kata kunci yang menggambarkan apa yang Anda ingin hasilkan dari model.
Model Stable Diffusion menggunakan prompt ini untuk memahami apa yang harus dihasilkan. Semakin jelas dan detail prompt-nya, semakin baik model dapat memahami dan menghasilkan gambar yang sesuai dengan deskripsi tersebut.

prompt = 'a dog using combat machete to war, full HD'
img = pipe(prompt).images[0]

2. Seed
Seed adalah angka yang digunakan untuk menginisialisasi generator angka acak. Ini memastikan bahwa hasil dari proses generatif dapat direproduksi mirip setiap dijalankan.
Dengan menetapkan seed yang sama, Anda dapat menghasilkan gambar yang sama (mirip) dari prompt yang sama. Ini sangat berguna untuk eksperimen dan debugging, di mana konsistensi hasil sangat penting.

seed = 2000generator = torch.Generator('cuda').manual_seed(seed)
img = pipe(prompt, generator=generator).images[0]
img

3. Guidance Scale
Guidance scale adalah parameter yang mengontrol seberapa kuat model harus mengikuti prompt yang diberikan.
Nilai guidance scale yang yang tinggi akan menghasilkan gambar yang memiliki gaya artistik, sedangkan semakin rendah nilai gambar akan semakin realistis atau mirip dengan data yang sudah dipelajari. Biasanya rentang guidance scale berada di 6 hingga 20.

plt.figure(figsize=(18,8))for i in range(1, 6):
 
  n_guidance = i + 3
  generator = torch.Generator("cuda").manual_seed(seed)
  img = pipe(prompt, guidance_scale=n_guidance, generator=generator).images[0]
 
  plt.subplot(1,5,i)
  plt.title('guidance_scale: {}'.format(n_guidance))
  plt.imshow(img)
  plt.axis('off')
 
plt.show()

4. Inference Step
Inference steps mengacu pada jumlah langkah yang diambil oleh model Stable Diffusion selama proses generasi gambar. Ini adalah jumlah iterasi yang digunakan oleh model untuk menghapus noise dan secara bertahap membentuk gambar akhir dari input acak berdasarkan prompt yang diberikan.

Jumlah inference steps yang lebih tinggi biasanya menghasilkan gambar dengan kualitas yang lebih baik karena model memiliki lebih banyak kesempatan untuk menghapus noise dan memperbaiki detail gambar. Namun, lebih banyak langkah juga berarti waktu komputasi yang lebih lama. By defaultangka dari inference step adalah 50 sehingga untuk menghasilkan gambar sesuai dengan keinginan, Anda dapat melakukan eksplorasi terhadap nilainya, ya.

prompt = "A man using baby shark costume at the moon"
generator = torch.Generator("cuda")
img = pipe(prompt, num_inference_steps=999, generator=generator).images[0]
img

5. Image Size
Image size adalah ukuran gambar yang dihasilkan oleh model. Ini biasanya ditentukan dalam piksel dan bisa berupa ukuran lebar dan tinggi (width x height).
Ukuran gambar memengaruhi jumlah komputasi yang diperlukan. Gambar yang lebih besar membutuhkan lebih banyak memori dan waktu komputasi. Biasanya, ukuran gambar ditentukan berdasarkan kebutuhan spesifik aplikasi atau keterbatasan perangkat keras.

6. Negative Prompt
Negative prompt adalah teks yang Anda tentukan untuk menentukan apa yang tidak ingin dihasilkan dalam gambar. Ini membantu model menghindari elemen-elemen tertentu yang melanggar etika ataupun perlu dihindari.
Dengan memberikan negative prompt, Anda dapat mengarahkan model untuk menghindari menghasilkan elemen atau fitur tertentu yang tidak diinginkan dalam gambar. Ini berguna untuk mendapatkan hasil yang lebih sesuai dengan preferensi atau kebutuhan tertentu.

num_images = 5 
prompt = 'a man enjoying sunset at bali beach with his girlfriend'
neg_prompt = 'coconut, knife, straw, building'
 
 
imgs = pipe(prompt, negative_prompt = neg_prompt, num_images_per_prompt=num_images).images
grid = grid_img(imgs, rows = 1, cols = 5, scale=0.75)
grid

Dengan pemahaman yang baik tentang masing-masing parameter di atas, Anda dapat mengarahkan model Stable Diffusion untuk menghasilkan gambar yang lebih sesuai dengan deskripsi teks, menjaga efisiensi komputasi, dan memenuhi kebutuhan spesifik dari aplikasi Anda. Ada tiga hal penting yang perlu Anda perhatikan dalam penggunaan parameter tersebut.

1. Kombinasi Parameter: pengaturan yang optimal dari parameter-parameter tersebut bergantung pada tujuan spesifik dan keterbatasan perangkat keras yang Anda miliki. Misalnya, untuk menghasilkan gambar berkualitas tinggi pada perangkat keras dengan memori terbatas, Anda mungkin perlu menyeimbangkan antara ukuran gambar dan jumlah inference steps.

2. Eksperimen: menggunakan berbagai kombinasi dari parameter ini dan mengevaluasi hasilnya adalah cara terbaik untuk menemukan pengaturan yang menghasilkan gambar terbaik sesuai kebutuhan Anda.

3. Reproduksi: menggunakan seed memungkinkan hasil yang dapat direproduksi sehingga berguna untuk eksperimen dan debugging.




Text Generation:
Text generation menggunakan Large Language Models (LLMs) adalah bidang dalam pemrosesan bahasa alami (NLP) yang berfokus pada menghasilkan teks yang koheren dan bermakna berdasarkan input tertentu. LLMs adalah model pembelajaran mesin yang besar dan kompleks, dilatih pada dataset teks yang sangat besar untuk memahami dan menghasilkan bahasa manusia.

Text generation yang menggunakan Large Language Models (LLMs) adalah bidang yang kompleks tetapi sangat kuat dalam pemrosesan bahasa alami. Dengan berbagai tipe LLM (public, private) dan pendekatan untuk membuat model dari scratch, pengguna memiliki fleksibilitas untuk memilih pendekatan yang paling sesuai dengan kebutuhan mereka, baik dalam hal kontrol, biaya, dan kompleksitas teknis.

Model seperti GPT-3, BERT, dan lainnya telah digunakan dalam berbagai aplikasi baru pada kasus NLP. Perkembangan ini tentunya akan terus berlanjut untuk membuat model yang lebih cerdas, lengkap dan efisien. Lalu, bagaimana cara kita memiliki pendekatan yang cocok dari berbagai macam tipe LLM? Mari kita cari tahu perbedaannya terlebih dahulu.
1. Building from Scratch
Membuat LLM dari scratch berarti mengembangkan model bahasa besar dari awal, termasuk pengumpulan data, preprocessing, pelatihan model, dan deployment.
Hal yang paling dipertimbangkan untuk membuat model dari nol adalah biaya, karena kita memerlukan resourceyang sangat banyak seperti waktu, sumber daya manusia, dan sumber daya komputasi. Namun, dengan membuat dari nol, kita memiliki kendali penuh terhadap model yang dibangun. Selain dapat menyesuaikan dengan kebutuhan. Anda juga dapat melakukan eksperimen dan inovasi baru dalam arsitektur model. Secara umum tahapan yang perlu Anda lakukan untuk membangun LLM sebagai berikut.

- Pengumpulan Data
Mengumpulkan data teks yang sangat besar dan beragam untuk melatih model. Data bisa berasal dari berbagai sumber seperti buku, artikel, website, dan lainnya. Dataset biasanya berukuran sangat besar (biasanya dalam ukuran terabyte) dan beragam untuk mencakup berbagai gaya dan topik bahasa.

- Preprocessing Data
Membersihkan dan memformat data agar sesuai untuk pelatihan. Ini melibatkan langkah-langkah seperti tokenisasi, lowercasing, menghapus karakter yang tidak diinginkan, dan lainnya.
Libraries yang perlu dipelajari adalah NLTK, SpaCy, atau fungsi-fungsi preprocessing dalam framework machine learning seperti TensorFlow dan PyTorch.

- Membangun Arsitektur Model
Menentukan arsitektur model. Model seperti Transformer (digunakan dalam GPT, BERT) adalah pilihan populer karena efektivitasnya dalam memahami konteks panjang. Framework yang biasa digunakan adalah TensorFlow, PyTorch, JAX.

- Pelatihan Model
Melatih model menggunakan dataset yang sudah dipreproses. Ini adalah tahap yang paling memakan waktu dan sumber daya. Berikut hal yang perlu diperhatikan.
Infrastructure: GPU/TPU cluster, distributed training.
Parameter: Learning rate, batch size, jumlah epochs.

- Evaluasi dan Fine-Tuning
Mengevaluasi model menggunakan dataset validasi dan menyesuaikan hyperparameter untuk mengoptimalkan kinerja. Fine-tuning dengan dataset spesifik untuk tugas tertentu, seperti teks klasifikasi atau teks generasi.

- Deployment
Menyimpan model ke lingkungan produksi untuk digunakan dalam aplikasi. Anda bisa menggunakan Docker, Kubernetes, cloud services seperti AWS, GCP, Azure, dan lain sebagainya

- Monitoring dan Pemeliharaan
Memantau kinerja model di lingkungan produksi dan melakukan pembaruan atau retraining jika diperlukan menggunakan Logging tools, dan performance monitoring tools.


2. Private Model/Commercial Model
Private LLMs adalah model bahasa besar yang dikembangkan dan digunakan secara eksklusif oleh organisasi atau individu tertentu. Model ini tidak tersedia untuk umum dan biasanya disesuaikan dengan kebutuhan spesifik organisasi atau proyek. Model bahasa yang digunakan oleh perusahaan besar seperti Google Search AI, Amazon Alexa, IBM Watson, Gemini, dan lain sebagainya.
Untuk menggunakan model ini, biasanya kita perlu membayar lisensi untuk mendapatkan API sehingga bisa digunakan pada aplikasi yang sedang dibangun.

3. Open Source Model
Public LLMs adalah model bahasa besar yang tersedia secara publik untuk digunakan oleh siapa saja. Model ini biasanya dikembangkan oleh organisasi riset, perusahaan teknologi besar, atau komunitas open-source dan dirilis untuk akses umum. Beberapa open source model seperti GEMMA, GPT-3, LLama, BERT, T5, Claude, Cohere, dan lain sebagainya.
Untuk menggunakan model publik ini, kita tidak perlu biaya besar karena model sudah dilatih dan siap digunakan. Namun, sebagai pengguna, kita tidak memiliki kontrol penuh atas data pelatihan dan parameternya. Selain itu, dengan menggunakan layanan pihak ketiga, dapat menimbulkan resiko privasi untuk data sensitif karena kita perlu menyetujui syarat dan ketentuan dari masing-masing penerbit.
Salah satu contoh penggunaan model publik GEMMA dapat menggunakan kode berikut dengan catatan Anda sudah mengisi form akses yang disediakan oleh Google.

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
 
tokenizer = AutoTokenizer.from_pretrained("google/gemma-1.1-2b-it")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-1.1-2b-it",
    torch_dtype=torch.bfloat16
)
 
input_text = "Write me a poem about Machine Learning."
input_ids = tokenizer(input_text, return_tensors="pt")
 
outputs = model.generate(**input_ids, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))

Nah, sampai di sini Anda sudah dapat menentukan model mana yang akan digunakan, tentunya kembali tergantung dengan kebutuhan dan resource yang dimiliki. 

Mungkin tebersit sebuah pertanyaan di benak Anda, “Jika menggunakan model yang sudah ada, bagaimana cara kita membuat model bisa menjawab sesuai keinginan?” Nah, jika pertanyaan itu sudah muncul berarti Anda berada di jalur yang tepat. Ada beberapa pendekatan agar kita dapat “memerintah” LLMs model menjawab pertanyaan sesuai dengan keinginan developer mulai dari fine-tuning, Retrieval Augmented Generation, dan Prompting. Tentunya ketiga pendekatan tersebut memiliki kelebihan dan kekurangannya. Secara garis besar, berikut sifat dari masing-masing metode.

Sebetulnya apa sih ketiga pendekatan di atas? Mari kita bahas secara saksama agar Anda lebih paham dan bisa menentukan pendekatan mana yang paling cocok dengan aplikasi yang sedang dibangun.

1. Fine-tuning
Fine-tuning adalah proses menyesuaikan model pra-latih (pre-trained model) pada dataset yang lebih spesifik dan lebih kecil untuk suatu tugas tertentu. Pendekatan ini melibatkan pelatihan tambahan pada model yang sudah terlatih sebelumnya untuk meningkatkan kinerja pada tugas tertentu.

Dengan menggunakan model yang sudah terlatih, kita dapat menghemat waktu dan sumber daya dibandingkan melatih model dari awal. Selain itu, model dapat disesuaikan dengan lebih baik untuk tugas spesifik. Contoh sederhananya adalah jika kita bertanya kepada pre-trained model apa itu Dicoding, model akan menjawab seperti berikut.

Model tidak mengetahui apa itu Dicoding Indonesia karena ia tidak memiliki data apa pun terkait Dicoding. Oleh karena itu, ia tidak akan bisa menjawab sesuai dengan ekspektasi pengguna. Dengan melakukan fine-tuning model yang telah dilatih pada dataset umum untuk diadaptasi ke domain spesifik seperti kesehatan, hukum, teknologi, atau bahkan perusahaan seperti Dicoding Indonesia.
Hal ini dapat meningkatkan akurasi dan relevansi hasil dalam konteks tertentu. Misalnya, fine-tuning model untuk teks Dicoding akan membuatnya lebih baik dalam memahami dan menjawab pertanyaan seputar Dicoding. Secara sederhana proses fine-tuning dapat Anda lakukan dengan kode berikut.


(kodenya sama ketika aku terapin DistilBERT untuk sentimen)

2. Retrieval Augmented Generation (RAG)
Retrieval-Augmented Generation (RAG) adalah teknik dalam pemrosesan bahasa alami (NLP) yang menggabungkan kemampuan model retrieval (pengambilan informasi) dan generative (pembuatan teks) untuk menghasilkan teks yang lebih informatif dan relevan.

RAG menggabungkan dua komponen utama yaitu Retriever dan Generator. Retriever bertugas mengambil informasi yang relevan dari basis data atau korpus teks berdasarkan query atau prompt. Lalu, Generator akan menggunakan informasi yang diambil oleh retriever untuk menghasilkan teks atau jawaban yang lebih terstruktur dan kontekstual.
Umumnya RAG akan membutuhkan sebuah database yang menyimpan berbagai informasi spesifik agar dapat menjawab pertanyaan pengguna sesuai dengan relevan. Salah satu database yang paling sering digunakan untuk melakukan RAG adalah vector database. Dengan menggunakan vector database teknik ini menjadi lebih efisien dan efektif dalam mencari dan mengambil informasi yang relevan dari korpus besar berdasarkan representasi vector dari teks.
Vector database adalah sistem basis data yang dioptimalkan untuk menyimpan dan mencari vector berdimensi tinggi. Dalam konteks NLP, teks atau dokumen diubah menjadi representasi vector menggunakan model embedding, seperti BERT atau Sentence Transformers. Vector database kemudian memungkinkan pencarian cepat dan efisien berdasarkan kedekatan atau kesamaan vector. Berikut adalah beberapa contoh vector database yang dapat Anda gunakan.

Dengan menggabungkan Retrieval-Augmented Generation (RAG) dengan vector database, efisiensi dan akurasi dalam mencari dan menghasilkan teks berbasis informasi dapat meningkat. Vector database memungkinkan pencarian cepat berdasarkan representasi vector, sementara model generatif menghasilkan teks yang lebih informatif dan relevan.

Kombinasi ini sangat powerfull untuk aplikasi tanya jawab, pembuatan konten, dan penelusuran informasi, memberikan jawaban yang lebih tepat dan berdasarkan data aktual. Implementasi dengan vector database seperti Milvus, Pinecone, atau FAISS memungkinkan pencarian dan pengambilan informasi dalam skala besar dengan kecepatan yang relatif tinggi.
Sama halnya dengan fine-tuning, Anda akan mempelajari langkah lengkap dari RAG pada kelas berikutnya karena tujuan dari modul ini adalah memberikan awareness supaya Anda lebih siap ketika menggunakan kedua pendekatan ini. Stay tune!


3. Prompting
Prompting adalah teknik merancang dan menyusun prompt (input teks) yang diberikan kepada model LLMs untuk memaksimalkan hasil yang diinginkan. Ini melibatkan eksperimen dengan berbagai format dan struktur prompt untuk mendapatkan hasil yang paling relevan dan berguna.
Prompt yang disusun dengan baik dapat secara signifikan meningkatkan performa model dalam menghasilkan teks yang relevan dan akurat sesuai dengan keinginan developer. Selain itu, Anda dapat meningkatkan efisiensi dengan prompt yang optimal, sehingga model dapat memberikan jawaban yang lebih tepat tanpa perlu pelatihan ulang atau fine-tuning yang mahal dan memakan waktu.

Ada beberapa teknik prompt tuning yang perlu Anda ketahui seperti instructional, contextual, few-shot, zero-shot, dan chain-of-thought prompting.

- Instructional Prompting yaitu memberikan instruksi eksplisit dan langsung kepada model mengenai tugas yang harus dilakukan.
- Contextual Prompting yaitu menyediakan konteks atau latar belakang yang membantu model memahami tugas yang diminta.
- Few-Shot Prompting yaitu memberikan beberapa contoh input-output dalam prompt untuk membantu model memahami pola atau format yang diinginkan.
- Zero-Shot Prompting yaitu memberikan prompt tanpa contoh sebelumnya, mengandalkan kemampuan model untuk memahami dan menyelesaikan tugas dari prompt itu sendiri.
- Chain-of-Thought Prompting yaitu memberikan serangkaian pertanyaan atau perintah yang memandu model melalui proses berpikir bertahap untuk mencapai jawaban yang lebih kompleks.

Dengan melakukan prompting, kita dapat membuat persona pada model LLMs yang digunakan sehingga model dapat menjawab pertanyaan pengguna sesuai dengan apa yang diharapkan. Salah satu contoh prompting, kita dapat membuat karakter dan kerangka jawaban dari model LLMs yang digunakan.

Dengan memberikan arahan tambahan kepada model, LLMs akan memberikan kemampuan model untuk menjawab sesuai dengan kebutuhan developer.

Ada beberapa hal yang perlu Anda perhatikan.

Eksperimen: cobalah berbagai format dan struktur prompt untuk melihat bagaimana model merespons.
Evaluasi: tinjau hasil yang dihasilkan oleh model untuk menentukan apakah prompt tersebut efektif.
Penyempurnaan: modifikasi dan sesuaikan prompt berdasarkan hasil evaluasi untuk meningkatkan kinerja model.
Pengulangan: ulangi proses ini hingga menemukan prompt yang menghasilkan output terbaik.
Kejelasan dan Spesifik: pastikan prompt Anda jelas dan spesifik tentang apa yang diharapkan dari model.
Memberikan Konteks yang Relevan: tambahkan informasi latar belakang yang relevan untuk membantu model memahami tugas.

Dengan melakukan hal tersebut, Anda dapat membuat sebuah model yang secara khusus akan memberikan response sesuai dengan kehendak developer. Kenapa hal ini diperlukan? Karena pada dasarnya model LLMs yang digunakan memiliki informasi umum sehingga dapat menjawab pertanyaan pengguna tidak spesifik. Kelebihan ini dapat Anda maksimalkan jika memiliki keterbatasan resource.Anda dapat membuat model memiliki nama Dico dan hanya menjawab pertanyaan seputar AI pada salah satu prompting di atas.

Langkah selanjutnya, Anda perlu mempersiapkan model yang akan digunakan. Pada kasus ini, kita akan menggunakan model Llama 2 7B yang dapat dijalankan hanya dengan menggunakan CPU.

from langchain.prompts import PromptTemplate
from langchain.llms import LlamaCpp
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import (
    StreamingStdOutCallbackHandler
)

your_model_path = "./models/llama-2-7b-chat.ggmlv3.q2_K.bin"

Tahapan utama pada proses prompting adalah menentukan karakter sehingga model dapat menjawab pertanyaan pengguna dengan lebih baik. Dengan membuat prompt yang baik, pengguna dapat menghasilkan prompt yang sesuai dengan konteks pertanyaan mereka sehingga dapat memberikan instruksi yang jelas kepada model terkait cara menjawab pertanyaan tersebut. Untuk melakukan hal tersebut, tulis kode di bawah ini.

chat_topic = "Machine Learning"
user_question = str(input("Enter your question : ")) 
template = """
Your name is Dico, you are machine learning expert and personal assistant with polite and wise. 
explain this question with complete and clear: '{question}' the topic is about {topic} and remember your name is Dico.
"""
 
prompt = PromptTemplate.from_template(template)
final_prompt = prompt.format(
    topic=chat_topic,
    question=user_question
)

Fungsi dari kode di atas adalah memberikan informasi topik dan template pertanyaan yang akan diproses. Kode tersebut menentukan format pertanyaan dengan menggunakan prompt.format(...) yang berfungsi untuk mengisi placeholder {question} dan {topic} dalam template dengan nilai yang sesuai dari user_question dan chat_topic. Hasilnya adalah string final_prompt yang telah diisi dengan pertanyaan pengguna dan topik pembicaraan, serta karakteristik asisten yang disebutkan dalam template.Tahap terakhir, kita perlu menentukan kemampuan model LLMs menggunakan kode berikut.

CallbackManager = CallbackManager([StreamingStdOutCallbackHandler()])
 
llm = LlamaCpp(
    model_path=your_model_path,
    n_ctx=6000,
    n_gpu_layers=512,
    n_batch=30,
    callback_manager=CallbackManager,
    temperature=0.9,
    max_tokens=4095,
    n_parts=1,
    verbose=0
)
llm(final_prompt)

Kode di atas mengilustrasikan cara menginisialisasi dan menjalankan model LLM menggunakan library LlamaCpp. Dengan mengkonfigurasi berbagai parameter, pengguna dapat mengontrol berbagai aspek pemrosesan model, termasuk penggunaan hardware, ukuran batch, panjang konteks, kreativitas output, dan lainnya. Lalu, apa fungsi dari masing-masing parameter tersebut? Mari kita bahas satu per satu.

- CallbackManager merupakan objek yang mengelola callback, memungkinkan berbagai tindakan diambil pada berbagai titik dalam proses pemrosesan model. Callback digunakan untuk memonitor atau memodifikasi perilaku model saat berjalan.

- StreamingStdOutCallbackHandler() adalah handler callback yang digunakan untuk streaming output langsung ke standar output (misalnya, mencetak hasil ke konsol saat dihasilkan).

- LlamaCpp,kelas yang menginisialisasi dan menjalankan model LLM berbasis library LlamaCpp, yang mengindikasikan penggunaan Llama.
- model_path=your_model_path: path ke fail model yang akan digunakan. your_model_path harus diganti dengan path yang benar ke model yang ingin digunakan.
- n_ctx=6000: menentukan panjang konteks maksimum yang digunakan oleh model. Ini adalah jumlah token yang dapat diproses oleh model dalam satu waktu.
- n_gpu_layers=512: menentukan jumlah layer dalam model yang akan diproses menggunakan GPU. Ini dapat mempercepat inferensi jika menggunakan model dengan ukuran yang relatif besar.
- n_batch=30: ukuran batch untuk pemrosesan, menentukan berapa banyak contoh yang diproses.
- temperature=0.9: parameter ini mengontrol kreativitas model selama generasi teks. Nilai yang lebih tinggi (lebih dekat ke 1) menghasilkan output yang lebih kreatif dan variatif, dan nilai yang lebih kecil akan menghasilkan output yang lebih sesuai dengan data yang dipelajari.
- max_tokens=4095: batas jumlah token yang akan dihasilkan dalam satu output. Ini memastikan bahwa output tidak terlalu panjang sehingga tidak memberikan beban berlebihan kepada komputer. Tentunya parameter ini bisa disesuaikan dengan kemampuan komputer kalian ya.
- n_parts=1: menentukan pembagian model atau data dalam bagian-bagian yang lebih kecil untuk pemrosesan
- verbose=0: mengatur tingkat verbosity. Nilai 0 biasanya berarti tidak ada log detail yang ditampilkan.




Multimodal Model:
Multimodal model adalah jenis model pembelajaran mesin yang dapat memproses dan memahami berbagai jenis data atau modalitas, seperti teks, gambar, audio, dan video secara bersamaan. Tujuannya adalah untuk menggabungkan informasi dari berbagai sumber ini untuk mencapai pemahaman yang lebih komprehensif dan menghasilkan keluaran yang lebih kaya dan akurat.

Setelah perkembangan yang sangat cepat dari Generative AI, lalu mengapa multimodal memiliki peran penting dan seringkali menjadi pilihan utama? Hal ini karena multimodal model memiliki kekayaan informasi, fleksibilitas aplikasi, hingga kemampuan pemahaman kontekstual yang lebih baik. 

Modalitas yang berbeda memberikan berbagai jenis informasi. Menggabungkannya dapat memberikan pemahaman yang lebih baik tentang konteks dan makna yang diberikan. Sehingga, Multimodal model dapat digunakan dalam berbagai aplikasi seperti pencarian multimedia, analisis video, sistem tanya jawab berbasis video, dan banyak lagi. Dengan memadukan informasi dari berbagai sumber, model dapat menangkap nuansa dan konteks yang mungkin terlewatkan jika hanya menggunakan satu modalitas (contoh teks).

Multimodal model umumnya menggunakan teknik fusion/fusi (penggabungan) untuk membangun model yang dapat berinteraksi dengan berbagai tipe data. Fusi dalam multimodal model adalah proses menggabungkan informasi dari berbagai modalitas untuk mencapai pemahaman yang lebih lengkap dan menghasilkan keluaran yang lebih kaya dan akurat. Ada beberapa teknik fusi yang umum digunakan dalam multimodal model, seperti early fusion, late fusion, hybrid fusion, dan attention mechanism.

1. Early Fusion
Early fusion digunakan untuk menggabungkan data mentah dari berbagai modalitas sebelum diberikan ke model untuk dilatih.

Data dari berbagai modalitas diambil dan digabungkan pada tahap awal kemudian kombinasi ini diberikan ke model untuk pelatihan atau inferensi. Kelebihan teknik ini dapat menangkap interaksi awal antara berbagai modalitas dan memungkinkan model untuk mempelajari representasi dari berbagai modalitas. Namun, teknik ini memiliki kekurangan juga karena bisa menyebabkan kesulitan untuk menangani modalitas dengan dimensi yang sangat berbeda.

2. Late Fusion
Sesuai namanya late fusion menggabungkan output dari model yang memproses setiap modalitas secara terpisah.

Setiap modalitas diproses secara independen oleh model khusus yang sudah dibuat sebelumnya. Output dari model-model ini kemudian digabungkan menggunakan metode layaknya menghitung rata-rata, penjumlahan, atau perhitungan lainnya. Dengan menggunakan teknik ini, model akan memiliki sifat fleksibel dalam menangani modalitas dengan dimensi yang berbeda. Hal itu karena setiap modalitas dapat diproses menggunakan model yang paling sesuai untuk tipe modalitas tersebut. Kekurangan dari model ini bergantung pada performa model individu untuk setiap modalitas.

3. Hybrid Fusion
Hybrid fusion adalah kombinasi dari early fusion dan late fusion, konsepnya beberapa fitur yang memiliki kemiripan akan digabungkan lebih awal dan sisanya akan digabungkan di tahap akhir seperti late fusion.

Dengan menggunakan teknik ini, tentu kita mendapatkan kelebihan dari teknik early fusion dan late fusion. Namun teknik ini juga memiliki kekurangan yaitu memiliki sifat lebih kompleks dalam implementasi dan tuning. Anda perlu melakukan pemilihan dengan lebih hati-hati terkait fitur mana yang harus digabungkan pada tahap awal dan akhir.


Sebelum membangun alangkah lebih baik kita bisa menggunakan multimodal model tersebut, ‘kan? Agar dapat dilakukan oleh semua orang pada local environment, mari kita gunakan satu tools bernama Ollama.

Ollama adalah perusahaan yang berfokus pada pengembangan Generative AI yang memungkinkan pengguna untuk menjalankan dan mengelola model secara lokal pada perangkat mereka tanpa memerlukan koneksi internet. Ollama bertujuan untuk menyediakan alat dan infrastruktur yang memungkinkan penggunaan model generative dengan efisien di berbagai platform.

Penggunaan Ollama cenderung sangat mudah bahkan ada opsi low-code yang mana Anda hanya perlu mengunduh Ollama beserta model yang ingin digunakan. Namun, Anda juga dapat melakukan fine-tuning dan mengunggah model yang dibuat ke database Ollama sehingga tidak menghilangkan programming sense. 

Pada contoh penggunaan multimodal model ini, kita akan menggunakan Ollama sebagai tools utama dan LLaVa sebagai model multimodal.

LLaVa (Large Language and Vision Assistant) adalah contoh model multimodal yang menggabungkan kemampuan pemrosesan NLP dan computer vision (CV) sehingga memiliki kemampuan interaksi yang lebih kaya dan kontekstual. LLaVa bertujuan untuk memproses dan memahami informasi dari berbagai modalitas, seperti teks dan gambar, untuk menghasilkan output yang lebih informatif dan relevan. 

Untuk menggunakan LLaVa melalui Ollama sangatlah mudah. Hal pertama yang perlu Anda lakukan adalah menginstall Ollama baik melalui CLI ataupun mengunduh melalui fail Ollama melalui platform Ollama.

Setelah berhasil menginstal Ollama, selanjutnya Anda perlu mengunduh model yang akan digunakan pada library models yang sudah disediakan oleh Ollama. Karena contoh kasus ini akan menggunakan LLaVa, Anda dapat menggunakan kode berikut untuk memanggil model LLaVa.

ollama run llava

CLI akan dengan sendirinya mencari dan mengunduh model dengan versi terbaru jika tidak terdapat typo dalam penulisannya. Anyway, LLaVa juga memiliki beberapa versi layaknya Llama. Oleh karena itu, Anda dapat menyesuaikan parameter yang akan digunakan berdasarkan nama model yang dipanggil.

# Pilih salah satu jika ingin menggunakan versi lainnya.
ollama run llava:7b
ollama run llava:13b
ollama run llava:34b


Anda diminta untuk memilih model AI untuk sebuah tugas di mana Anda perlu memahami bagaimana keputusan dibuat oleh model. Model mana yang lebih cocok untuk tugas ini dan mengapa?
a. Discriminative AI, karena cenderung lebi transparan dan lebih mudah diinterpretasikan dalam konteks pengambilan keputusan
b. Discriminative AI, karena lebih efisien dalam menggunakan data komputasi.
d. Generative AI, karena dapat menghasilkan data baru yang mirip dengan data pelatihan.


Generative AI memiliki berbagai aplikasi dalam berbagai bidang. Sebutkan salah satu aplikasi utama dari Generative AI dalam bidang kesehatan dan jelaskan manfaatnya.
c. Menghasilkan gambar medis sintetis untuk pelatihan model AI lainnya dan memperluas dataset pelatihan.


Dalam konteks periklanan digital, bagaimana Generative AI dapat digunakan untuk meningkatkan efektivitas kampanye iklan?
a. Dengan menghasilkan konten iklan yang dipersonalisasi untuk audiens target berdasarkan data demografis dan perilaku.

Salah satu aplikasi LLM adalah dalam pengembangan chatbot. Apa tantangan utama dalam mengembangkan chatbot menggunakan LLM agar interaksi dengan pengguna tetap alami dan bermanfaat?
Memastikan..



Bagaimana lapisan konvolusi dalam CNN menghasilkan fitur-fitur yang semakin kompleks?
a. Dengan menggunakan filter yang lebih kecil
b. Dengan menambahkan lebih banyak lapisan konvolusi (pilih)
c. Dengan menggunakan fungsi aktivasi linear
d. Dengan melakukan konvolusi pada gambar secara berulang

Bagaimana komputasi yang dilakukan oleh sebuah neuron dalam Artificial Neural Network? 
b. Sebuah neuron menghitung fungsi linear (z = Wx + b) diikuti oleh fungsi aktivasi.

Apa keuntungan dari batch loading dalam pelatihan neural network dibandingkan dengan memuat seluruh dataset sekaligus?
B. Batch loading memungkinkan penggunaan dataset yang lebih besar tanpa memori yang besar.
C. Batch loading mempercepat waktu pelatihan karena hanya memuat sebagian kecil dari dataset pada setiap iterasi. (betul)
D. Batch loading mengurangi risiko overfitting karena model tidak melihat seluruh dataset pada saat yang sama.

Apa yang dimaksud dengan weight dalam konteks neural network?
a. Weight adalah nilai yang menentukan seberapa cepat model neural network belajar dari data
b. Weight adalah parameter yang mengatur kekuatan dan arah koneksi antara neuron dalam jaringan neutral (pilih)
c. Weight adalah nilai yang menentukan seberapa pentingnya setiap fitur dalam membuat prediksi model
d. Weight adalah nilai yang menentukan probabilitas dari output model

Apa yang dimaksud dengan model probabilitas sederhana yang digunakan untuk mengklasifikasikan teks berdasarkan kemungkinan kemunculan kata-kata dalam Natural Language Processing (NLP)?
a. Naive Bayes (pilih)
b. 
c. 
d. 

Apa nama kelas stemmer NLTK yang umumnya dipilih untuk melakukan stemming dalam bahasa selain Bahasa Inggris?
a. Snowballstemmer (pilih)
b. BaseStemmer
c. LinguistikStemmer
d. RootStemmer

Apa peran utama dari algoritma TF-IDF dalam NLP?
a. 
b. 
c. 
d. Menentukan kepentingan relatif dari sebuah kata dalam sebuah dokumen (pilih)

Kenapa penting untuk menormalkan data sebelum melatih model time series forecasting menggunakan TensorFlow?
a. Untuk membuat model lebih sulit diinterpretasikan
b. Untuk mengurangi waktu pelatihan model
c. Untuk menghindari masalah numerik dan mempercepat konvergensi model (pilih)
d. Untuk membuat data lebih sulit diolah oleh model

Anda sedang mengevaluasi model time series forecasting untuk memprediksi produksi pabrik dalam satu tahun ke depan. Setelah melihat hasil evaluasi, Anda menemukan bahwa nilai MSE sebesar 10000 dan nilai RMSE sebesar 100. Bagaimana Anda akan menafsirkan kinerja model ini?
a. Model memiliki kesalahan yang rendah dan dapat diandalkan untuk memprediksi produksi pabrik
b. Model memiliki kesalahan yang tinggi dan tidak dapat diandalkan untuk untuk memprediksi produksi pabrik (pilih)
c. Model memiliki kesalahan yang tinggi tetapi masih dapat diterima karena nilai RMSE yang rendah
d. Model memiliki kesalahan yang rendah tapi masih perlu diperbaiki karena nilai MSE yang tinggi

Anda menggabungkan dua dataset gambar yang berbeda untuk melatih model deep learning. Apa yang harus diperhatikan untuk memastikan model dapat generalisasi dengan baik?
a. Mengurangi jumlah epoch pelatihan
b. Menggunakan lebih banyak layer pada model
c. Melakukan normalisasi data dan memastikan distribusi data yang seimbang (pilih)
d. Menggunakan learning rate yang lebih tinggi

Anda ingin mengklasifikasikan gambar makanan menggunakan deep learning. Dataset Anda tidak seimbang (lebih banyak gambar pizza daripada gambar sushi). Metode apa yang paling tepat untuk mengatasi masalah ini?
a. 
b. 
c. 
d. Augmentasi data untuk kategori sushi (pilih)

Anda sedang mengevaluasi performa model menggunakan metrik F1-score. Dalam konteks klasifikasi gambar yang memiliki ketidakseimbangan kelas yang signifikan, mengapa F1-score lebih cocok dibandingkan akurasi?
C. Karena F1-score memperhitungkan precision dan recall secara seimbang

Bayangkan sebuah sistem rekomendasi film menggunakan algoritma content-based filtering. Seorang pengguna menonton banyak film bergenre science fiction dan memiliki rating tinggi pada film-film tersebut. Apa kelemahan utama dari pendekatan ini dalam hal diversifikasi rekomendasi?
a. Sistem akan merekomendasikan film dengan rating rendah
b. Sistem mungkin tidak akan merekomendasikan film-film dari genre lain yang mungkin juga disukai pengguna (pilih)
c. Sistem akan selalu merekomendasikan film-film baru tanpa mempertimbangkan preferensi pengguna
d. Sistem tidak dapat memanfaatkan data pengguna lain untuk meningkatkan rekomendasi

Mengapa embedding digunakan dalam sistem rekomendasi?
a. 
b. 
c. 
d. Untuk menangkap relasi kompleks antara pengguna dan item dengan merepresentasikan mereka dalam ruang vektor berukuran lebih rendah.

Seorang peneliti ingin mengimplementasikan algoritma reinforcement learning untuk mengontrol sistem yang sangat kompleks dan dinamis. Di antara pilihan algoritma berikut, yang mana yang paling sesuai untuk digunakan
a. SARSA
b. Policy Gradient Methods
c. Deep Q-Networks (pilih)

Pada Markov Decision Process, pemilihan ruas jalan dalam permasalahan shortest path merupakan bagian dari ...
a. Action (salah)
b. State (salah)
c. Action
d. Reward 

Apa keuntungan utama menggunakan TensorFlow.js?
a. 
b. 
c. Mengizinkan pelatihan dan inferensi model machine learning di browser
d. 

Mengapa monitoring penting dalam proses deployment model machine learning?
d. Untuk memastikan model tetap bekerja dengan baik dalam produksi dan mendeteksi masalah lebih awal

Dalam penerapan Generative AI, sering kali digunakan model yang membutuhkan dataset yang sangat besar untuk pelatihan. Apa dampak etis yang perlu dipertimbangkan dalam pengumpulan data skala besar ini?
a. 
b. 
c. 
d. 

Dalam penerapan Generative AI, sering kali digunakan model yang membutuhkan dataset yang sangat besar untuk pelatihan. Apa dampak etis yang perlu dipertimbangkan dalam pengumpulan data skala besar ini?
a. Mengurangi kebutuhan tenaga kerja manusia
b. Mempercepat proses inovasi teknologi
c. Efisiensi Komputassi meningkat
d. Risiko privasi dan pelanggaran hak cipta (pilih)

Generative Adversarial Networks (GANs) adalah salah satu metode yang populer dalam Generative AI. Bagaimana mekanisme kerja dari GANs ini?
a. Satu jaringan saraf berfungsi sebagai generator untuk menghasilkan data baru, sementara jaringan lain (discriminator) mengevaluasi keaslian data tersebut. (pilih) 

Contoh Linkedin Post untuk Beasiswa DBS Foundation Intermediate

https://www.linkedin.com/posts/mashumabduljabbar_belajar-pengembangan-machine-learning-ugcPost-7221012029187223552-QWi6?utm_source=share&utm_medium=member_desktop
---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------


---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------

---------------------------------------------------------------------------
                        Materi Materi Materi Materi
---------------------------------------------------------------------------


? = pertanyaan
* / ** = penting